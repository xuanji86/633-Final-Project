{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import numpy as np\n",
    "import ast\n",
    "from PIL import Image\n",
    "from datetime import datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "cgm_test = pd.read_csv('633FinalData/cgm_test.csv')\n",
    "cgm_train = pd.read_csv('633FinalData/cgm_train.csv')\n",
    "img_train = pd.read_csv('633FinalData/img_train.csv')\n",
    "img_test = pd.read_csv('633FinalData/img_test.csv')\n",
    "cgm_test['cgm'] = cgm_test['CGM Data'].apply(lambda x: ast.literal_eval(x))\n",
    "cgm_train['cgm'] = cgm_train['CGM Data'].apply(lambda x: ast.literal_eval(x))\n",
    "def get_img(img):\n",
    "    # Convert the string representation of the image to a list\n",
    "    img = ast.literal_eval(img)\n",
    "    img = np.array(img)\n",
    "    \n",
    "    # Convert the NumPy array to a PIL Image\n",
    "    img = Image.fromarray(np.uint8(img))\n",
    "    img = img.resize((64,64))\n",
    "    img = np.array(img)\n",
    "    \n",
    "    if len(img.shape) == 2:\n",
    "        img = np.stack((img,)*3, axis=-1)\n",
    "    return img\n",
    "\n",
    "img_train['img_b'] = img_train['Image Before Breakfast'].apply(get_img)\n",
    "img_train['img_l'] = img_train['Image Before Lunch'].apply(get_img)\n",
    "img_test['img_b'] = img_test['Image Before Breakfast'].apply(get_img)\n",
    "img_test['img_l'] = img_test['Image Before Lunch'].apply(get_img)\n",
    "\n",
    "def to_step(t):\n",
    "    date_obj = datetime.strptime(t, '%Y-%m-%d %H:%M:%S')\n",
    "    return (date_obj.hour*60 + date_obj.minute)//5\n",
    "\n",
    "def cgm_to_steps(cgm):\n",
    "    steps = [0 for _ in range(288)]\n",
    "    for t,value in cgm:\n",
    "        steps[to_step(t)] = value\n",
    "    return steps\n",
    "\n",
    "def time_to_step(t1,t2):\n",
    "    if t1 == '{}' or t2 == '{}':\n",
    "        return [0 for _ in range(288)]\n",
    "    steps = [0 for _ in range(288)]\n",
    "    steps[to_step(t1)] = 1\n",
    "    steps[to_step(t2)] = 1\n",
    "    return steps\n",
    "\n",
    "cgm_train = cgm_train.dropna()\n",
    "\n",
    "cgm_train = cgm_train[~cgm_train.apply(lambda row: row.astype(str).str.contains('{}').any(), axis=1)]\n",
    "\n",
    "cgm_test['cgm_sequential'] = cgm_test['cgm'].apply(cgm_to_steps)\n",
    "cgm_train['cgm_sequential'] = cgm_train['cgm'].apply(cgm_to_steps)\n",
    "cgm_train['when_to_eat'] = cgm_train[['Breakfast Time', 'Lunch Time']].apply(lambda x: time_to_step(x['Breakfast Time'], x['Lunch Time']), axis=1)\n",
    "cgm_test['when_to_eat'] = cgm_test[['Breakfast Time', 'Lunch Time']].apply(lambda x: time_to_step(x['Breakfast Time'], x['Lunch Time']), axis=1)\n",
    "viome_test = pd.read_csv('633FinalData/demo_viome_test.csv')\n",
    "viome_train = pd.read_csv('633FinalData/demo_viome_train.csv')\n",
    "viome_test= pd.get_dummies(viome_test, columns=['Race'])\n",
    "viome_train= pd.get_dummies(viome_train, columns=['Race'])\n",
    "viome_test['viome_sequential'] = viome_test['Viome'].apply(lambda x :[float(x) for x in x.split(',')])\n",
    "viome_train['viome_sequential'] = viome_train['Viome'].apply(lambda x :[float(x) for x in x.split(',')])\n",
    "combined_train = pd.merge(cgm_train, img_train, on=['Subject ID', 'Day'])\n",
    "combined_train = pd.merge(combined_train, viome_train, on=['Subject ID'])\n",
    "combined_test = pd.merge(cgm_test, img_test, on=['Subject ID', 'Day'])\n",
    "combined_test = pd.merge(combined_test, viome_test, on=['Subject ID'])\n",
    "to_drop_train = ['Subject ID','Day','Breakfast Time','Lunch Time','CGM Data','Image Before Breakfast','Image Before Lunch','Viome']\n",
    "combined_train = combined_train.drop(to_drop_train, axis=1)\n",
    "to_drop_test = ['Subject ID','Day','Breakfast Time','Lunch Time','CGM Data','Image Before Breakfast','Image Before Lunch','Viome']\n",
    "to_drop_test = ['Subject ID','Day','Breakfast Time','Lunch Time','CGM Data','Image Before Breakfast','Image Before Lunch','Viome']\n",
    "combined_test = combined_test.drop(to_drop_train, axis=1)\n",
    "label_train = pd.read_csv('633FinalData/label_train.csv')\n",
    "label_train = label_train['Lunch Calories']\n",
    "combined_train['label'] = label_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomDataset(Dataset):\n",
    "    def __init__(self, dataframe):\n",
    "        self.dataframe = dataframe\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.dataframe)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        row = self.dataframe.iloc[idx]\n",
    "\n",
    "        epsilon = 1e-6\n",
    "\n",
    "        # Extract sequential data\n",
    "        cgm_sequential = np.array(row['cgm_sequential'], dtype=float)\n",
    "        when_to_eat = np.array(row['when_to_eat'], dtype=float)\n",
    "\n",
    "        # Normalize sequential data\n",
    "        cgm_sequential = (cgm_sequential - np.mean(cgm_sequential)) / (np.std(cgm_sequential) + epsilon)\n",
    "        when_to_eat = (when_to_eat - np.mean(when_to_eat)) / (np.std(when_to_eat) + epsilon)\n",
    "        sequential_data = np.stack((cgm_sequential, when_to_eat), axis=-1)\n",
    "\n",
    "\n",
    "        img_b = np.array(row['img_b'], dtype=float)\n",
    "        img_l = np.array(row['img_l'], dtype=float)\n",
    "        img_b = (img_b - np.mean(img_b)) / (np.std(img_b) + epsilon)\n",
    "        img_l = (img_l - np.mean(img_l)) / (np.std(img_l) + epsilon)\n",
    "\n",
    "        if img_b.ndim == 2:\n",
    "            img_b = np.stack((img_b, img_b, img_b), axis=-1)\n",
    "        if img_l.ndim == 2:\n",
    "            img_l = np.stack((img_l, img_l, img_l), axis=-1)\n",
    "\n",
    "        # Convert image data to PIL Image and then to tensor\n",
    "        img_b = Image.fromarray(np.uint8(img_b))\n",
    "        img_l = Image.fromarray(np.uint8(img_l))\n",
    "        img_b = torch.tensor(np.array(img_b), dtype=torch.float32).permute(2, 0, 1)  \n",
    "        img_l = torch.tensor(np.array(img_l), dtype=torch.float32).permute(2, 0, 1)  \n",
    "\n",
    "        # Extract numeric data\n",
    "        numeric_data = row[['Age', 'Gender', 'Weight', 'Height', 'Diabetes Status', 'A1C',\n",
    "                            'Baseline Fasting Glucose', 'Insulin', 'Triglycerides', 'Cholesterol',\n",
    "                            'HDL', 'Non-HDL', 'LDL', 'VLDL', 'CHO/HDL Ratio', 'HOMA-IR', 'BMI',\n",
    "                            'Race_African American', 'Race_Hispanic/Latino', 'Race_White']].values.astype(np.float32)\n",
    "\n",
    "        # Normalize numeric data\n",
    "        numeric_data = (numeric_data - np.mean(numeric_data)) / (np.std(numeric_data) + epsilon)\n",
    "        numeric_data = torch.tensor(numeric_data, dtype=torch.float32)\n",
    "\n",
    "        # Extract and reshape viome_sequential\n",
    "        viome_sequential = np.array(row['viome_sequential'], dtype=float).reshape(27, 1)\n",
    "        viome_sequential = (viome_sequential - np.mean(viome_sequential)) / (np.std(viome_sequential) + epsilon)\n",
    "        label_train = row['label']\n",
    "\n",
    "        return {\n",
    "            'sequential_data': torch.tensor(sequential_data, dtype=torch.float32),\n",
    "            'viome_sequential': torch.tensor(viome_sequential, dtype=torch.float32),\n",
    "            'img_b': img_b,\n",
    "            'img_l': img_l,\n",
    "            'numeric_data': numeric_data,\n",
    "            'label': label_train\n",
    "        }\n",
    "        \n",
    "class CustomTestDataset(Dataset):\n",
    "    def __init__(self, dataframe):\n",
    "        self.dataframe = dataframe\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.dataframe)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        row = self.dataframe.iloc[idx]\n",
    "\n",
    "        epsilon = 1e-6\n",
    "\n",
    "        cgm_sequential = np.array(row['cgm_sequential'], dtype=float)\n",
    "        when_to_eat = np.array(row['when_to_eat'], dtype=float)\n",
    "\n",
    "        # Normalize sequential data\n",
    "        cgm_sequential = (cgm_sequential - np.mean(cgm_sequential)) / (np.std(cgm_sequential) + epsilon)\n",
    "        when_to_eat = (when_to_eat - np.mean(when_to_eat)) / (np.std(when_to_eat) + epsilon)\n",
    "        sequential_data = np.stack((cgm_sequential, when_to_eat), axis=-1)\n",
    "\n",
    "        # Extract image data\n",
    "        img_b = np.array(row['img_b'], dtype=float)\n",
    "        img_l = np.array(row['img_l'], dtype=float)\n",
    "        img_b = (img_b - np.mean(img_b)) / (np.std(img_b) + epsilon)\n",
    "        img_l = (img_l - np.mean(img_l)) / (np.std(img_l) + epsilon)\n",
    "\n",
    "        if img_b.ndim == 2:\n",
    "            img_b = np.stack((img_b, img_b, img_b), axis=-1)\n",
    "        if img_l.ndim == 2: \n",
    "            img_l = np.stack((img_l, img_l, img_l), axis=-1)\n",
    "\n",
    "        img_b = Image.fromarray(np.uint8(img_b))\n",
    "        img_l = Image.fromarray(np.uint8(img_l))\n",
    "        img_b = torch.tensor(np.array(img_b), dtype=torch.float32).permute(2, 0, 1) \n",
    "        img_l = torch.tensor(np.array(img_l), dtype=torch.float32).permute(2, 0, 1) \n",
    "\n",
    "        # Extract numeric data\n",
    "        numeric_data = row[['Age', 'Gender', 'Weight', 'Height', 'Diabetes Status', 'A1C',\n",
    "                            'Baseline Fasting Glucose', 'Insulin', 'Triglycerides', 'Cholesterol',\n",
    "                            'HDL', 'Non-HDL', 'LDL', 'VLDL', 'CHO/HDL Ratio', 'HOMA-IR', 'BMI',\n",
    "                            'Race_African American', 'Race_Hispanic/Latino', 'Race_White']].values.astype(np.float32)\n",
    "\n",
    "        # Normalize numeric data\n",
    "        numeric_data = (numeric_data - np.mean(numeric_data)) / (np.std(numeric_data) + epsilon)\n",
    "        numeric_data = torch.tensor(numeric_data, dtype=torch.float32)\n",
    "\n",
    "        # Extract and reshape viome_sequential\n",
    "        viome_sequential = np.array(row['viome_sequential'], dtype=float).reshape(27, 1)\n",
    "        viome_sequential = (viome_sequential - np.mean(viome_sequential)) / (np.std(viome_sequential) + epsilon)\n",
    "\n",
    "\n",
    "        return {\n",
    "            'sequential_data': torch.tensor(sequential_data, dtype=torch.float32),\n",
    "            'viome_sequential': torch.tensor(viome_sequential, dtype=torch.float32),\n",
    "            'img_b': img_b,\n",
    "            'img_l': img_l,\n",
    "            'numeric_data': numeric_data,\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample 0:\n",
      "  sequential_data shape: torch.Size([288, 2])\n",
      "  viome_sequential shape: torch.Size([27, 1])\n",
      "  img_b shape: torch.Size([3, 64, 64])\n",
      "  img_l shape: torch.Size([3, 64, 64])\n",
      "  numeric_data shape: torch.Size([20])\n",
      "  label: 830\n"
     ]
    }
   ],
   "source": [
    "dataset_train = CustomDataset(combined_train)\n",
    "dataset_test = CustomTestDataset(combined_test)\n",
    "combined_train['img_b'].apply(lambda x: x.shape).unique()\n",
    "\n",
    "for i in range(1):\n",
    "    sample = dataset_train[i]\n",
    "    print(f\"Sample {i}:\")\n",
    "    print(f\"  sequential_data shape: {sample['sequential_data'].shape}\")\n",
    "    print(f\"  viome_sequential shape: {sample['viome_sequential'].shape}\")\n",
    "    print(f\"  img_b shape: {sample['img_b'].shape}\")\n",
    "    print(f\"  img_l shape: {sample['img_l'].shape}\")\n",
    "    print(f\"  numeric_data shape: {sample['numeric_data'].shape}\")\n",
    "    print(f\"  label: {sample['label']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader\n",
    "import torchvision.models as models\n",
    "\n",
    "class CombinedModel(nn.Module):\n",
    "    def __init__(self, hidden_size=64):\n",
    "        super(CombinedModel, self).__init__()\n",
    "        \n",
    "        # Sequential data sub-network\n",
    "        self.seq_net = nn.LSTM(input_size=2, hidden_size=hidden_size, num_layers=1, batch_first=True)\n",
    "        self.seq_fc = nn.Linear(hidden_size, 32)\n",
    "        self.viome_net = nn.LSTM(input_size=1, hidden_size=hidden_size, num_layers=1, batch_first=True)\n",
    "        self.viome_fc = nn.Linear(hidden_size, 32)\n",
    "        self.img_net = models.resnet50(pretrained=False)\n",
    "        self.img_net.fc = nn.Linear(self.img_net.fc.in_features, 32) \n",
    "        \n",
    "        # Numeric data sub-network\n",
    "        self.num_net = nn.Sequential(\n",
    "            nn.Linear(20, 32),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(32, 16)\n",
    "        )\n",
    "        \n",
    "        # Combined fully connected layers\n",
    "        self.fc = nn.Sequential(\n",
    "            nn.Linear(32 + 32 + 32 + 32 + 16, 64),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(64, 1) \n",
    "        )\n",
    "    \n",
    "    def forward(self, sequential_data, img_b, img_l, numeric_data, viome_sequential):\n",
    "        seq_out, _ = self.seq_net(sequential_data)\n",
    "        seq_out = self.seq_fc(seq_out[:, -1, :])  \n",
    "        \n",
    "        # Process viome sequential data\n",
    "        viome_out, _ = self.viome_net(viome_sequential)\n",
    "        viome_out = self.viome_fc(viome_out[:, -1, :])  \n",
    "        img_b_out = self.img_net(img_b)\n",
    "        img_l_out = self.img_net(img_l)\n",
    "        num_out = self.num_net(numeric_data)\n",
    "        \n",
    "        # Concatenate all outputs\n",
    "        combined = torch.cat((seq_out, viome_out, img_b_out, img_l_out, num_out), dim=1)\n",
    "        out = self.fc(combined)\n",
    "        \n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\10374\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torchvision\\models\\_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "c:\\Users\\10374\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torchvision\\models\\_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=None`.\n",
      "  warnings.warn(msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/50], Loss: 0.976251482963562\n",
      "Epoch [2/50], Loss: 0.8092807398902045\n",
      "Epoch [3/50], Loss: 0.45263984468248153\n",
      "Epoch [4/50], Loss: 0.3556179520156648\n",
      "Epoch [5/50], Loss: 0.340121501021915\n",
      "Epoch [6/50], Loss: 0.3377652102046543\n",
      "Epoch [7/50], Loss: 0.3370044231414795\n",
      "Epoch [8/50], Loss: 0.3339032034079234\n",
      "Epoch [9/50], Loss: 0.3323914011319478\n",
      "Epoch [10/50], Loss: 0.33153705133332145\n",
      "Epoch [11/50], Loss: 0.33340845505396527\n",
      "Epoch [12/50], Loss: 0.3327429129017724\n",
      "Epoch [13/50], Loss: 0.3321627808941735\n",
      "Epoch [14/50], Loss: 0.3345035281446245\n",
      "Epoch [15/50], Loss: 0.33416008949279785\n",
      "Epoch [16/50], Loss: 0.3339487910270691\n",
      "Epoch [17/50], Loss: 0.33500489261415267\n",
      "Epoch [18/50], Loss: 0.33324697613716125\n",
      "Epoch [19/50], Loss: 0.3344210584958394\n",
      "Epoch [20/50], Loss: 0.33448794815275407\n",
      "Epoch [21/50], Loss: 0.33650610513157314\n",
      "Epoch [22/50], Loss: 0.33635568287637496\n",
      "Epoch [23/50], Loss: 0.33494392699665493\n",
      "Epoch [24/50], Loss: 0.3334389262729221\n",
      "Epoch [25/50], Loss: 0.33512018455399406\n",
      "Epoch [26/50], Loss: 0.33520153496000504\n",
      "Epoch [27/50], Loss: 0.3351101941532559\n",
      "Epoch [28/50], Loss: 0.3348269561926524\n",
      "Epoch [29/50], Loss: 0.33615710337956745\n",
      "Epoch [30/50], Loss: 0.33795387546221417\n",
      "Epoch [31/50], Loss: 0.3365474541982015\n",
      "Epoch [32/50], Loss: 0.33497118287616306\n",
      "Epoch [33/50], Loss: 0.3322124481201172\n",
      "Epoch [34/50], Loss: 0.3349565499358707\n",
      "Epoch [35/50], Loss: 0.33289426896307206\n",
      "Epoch [36/50], Loss: 0.3312380015850067\n",
      "Epoch [37/50], Loss: 0.33302074008517796\n",
      "Epoch [38/50], Loss: 0.335059255361557\n",
      "Epoch [39/50], Loss: 0.3348442680305905\n",
      "Epoch [40/50], Loss: 0.33458376593059963\n",
      "Epoch [41/50], Loss: 0.33491845594512093\n",
      "Epoch [42/50], Loss: 0.3356366091304355\n",
      "Epoch [43/50], Loss: 0.33239448732799953\n",
      "Epoch [44/50], Loss: 0.33482257193989223\n",
      "Epoch [45/50], Loss: 0.3347691165076362\n",
      "Epoch [46/50], Loss: 0.33426984151204425\n",
      "Epoch [47/50], Loss: 0.3355653054184384\n",
      "Epoch [48/50], Loss: 0.3346550663312276\n",
      "Epoch [49/50], Loss: 0.33601781725883484\n",
      "Epoch [50/50], Loss: 0.3356548547744751\n",
      "Training complete.\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "epoch_losses = []\n",
    "\n",
    "class RMSRELoss(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(RMSRELoss, self).__init__()\n",
    "\n",
    "    def forward(self, y_pred, y_true):\n",
    "        epsilon = 1e-6  \n",
    "        relative_error = (y_pred - y_true) / (y_true + epsilon)\n",
    "        return torch.sqrt(torch.mean(relative_error ** 2))\n",
    "    \n",
    "train_loader = DataLoader(dataset_train, batch_size=32, shuffle=True)\n",
    "test_loader = DataLoader(dataset_test, batch_size=32, shuffle=False)\n",
    "\n",
    "model = CombinedModel()\n",
    "criterion = RMSRELoss()  \n",
    "optimizer = optim.Adam(model.parameters(), lr=1e-3)  \n",
    "\n",
    "# Learning rate scheduler\n",
    "scheduler = optim.lr_scheduler.StepLR(optimizer, step_size=50, gamma=0.1)\n",
    "device = torch.device(\"cuda\")\n",
    "\n",
    "model = model.to(device)\n",
    "criterion = criterion.to(device)\n",
    "\n",
    "num_epochs = 50\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "    for batch in train_loader:\n",
    "        sequential_data = batch['sequential_data'].to(device)\n",
    "        img_b = batch['img_b'].to(device)\n",
    "        img_l = batch['img_l'].to(device)\n",
    "        numeric_data = batch['numeric_data'].to(device)\n",
    "        viome_sequential = batch['viome_sequential'].to(device)\n",
    "        label = batch['label'].to(device)\n",
    "        \n",
    "        outputs = model(sequential_data, img_b, img_l, numeric_data, viome_sequential)\n",
    "        loss = criterion(outputs, label)\n",
    "        \n",
    "        # Backward pass and optimization\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        running_loss += loss.item()\n",
    "    \n",
    "\n",
    "    epoch_loss = running_loss / len(train_loader)\n",
    "    epoch_losses.append(epoch_loss)\n",
    "    print(f\"Epoch [{epoch+1}/{num_epochs}], Loss: {epoch_loss}\")\n",
    "\n",
    "    scheduler.step()\n",
    "\n",
    "print(\"Training complete.\")\n",
    "\n",
    "plt.plot(range(1, num_epochs + 1), epoch_losses, marker='o')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.title('Training Loss over Epochs')\n",
    "plt.grid(True)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predictions saved to 'submission.csv' in Kaggle format.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "model.eval()  \n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = model.to(device)\n",
    "test_loss = 0.0\n",
    "all_outputs = []\n",
    "all_labels = []\n",
    "\n",
    "with torch.no_grad(): \n",
    "    for batch in test_loader:\n",
    "        sequential_data = batch['sequential_data'].to(device)\n",
    "        img_b = batch['img_b'].to(device)\n",
    "        img_l = batch['img_l'].to(device)\n",
    "        numeric_data = batch['numeric_data'].to(device)\n",
    "        viome_sequential = batch['viome_sequential'].to(device)\n",
    "        \n",
    "        outputs = model(sequential_data, img_b, img_l, numeric_data, viome_sequential)\n",
    "        \n",
    "        all_outputs += ([i[0] for i in outputs.tolist()])\n",
    "        \n",
    "test_ids = np.arange(len(all_outputs))  \n",
    "submission = pd.DataFrame({\"row_id\": test_ids, \"label\": all_outputs})\n",
    "submission.to_csv(\"submission.csv\", index=False)\n",
    "print(\"Predictions saved to 'submission.csv' in Kaggle format.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\10374\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torchvision\\models\\_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "c:\\Users\\10374\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torchvision\\models\\_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=None`.\n",
      "  warnings.warn(msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/50], Hidden Size: 32, LR: 0.001, Batch Size: 16, Loss: 0.9222\n",
      "Epoch [2/50], Hidden Size: 32, LR: 0.001, Batch Size: 16, Loss: 0.4521\n",
      "Epoch [3/50], Hidden Size: 32, LR: 0.001, Batch Size: 16, Loss: 0.3379\n",
      "Epoch [4/50], Hidden Size: 32, LR: 0.001, Batch Size: 16, Loss: 0.3320\n",
      "Epoch [5/50], Hidden Size: 32, LR: 0.001, Batch Size: 16, Loss: 0.3355\n",
      "Epoch [6/50], Hidden Size: 32, LR: 0.001, Batch Size: 16, Loss: 0.3353\n",
      "Epoch [7/50], Hidden Size: 32, LR: 0.001, Batch Size: 16, Loss: 0.3343\n",
      "Epoch [8/50], Hidden Size: 32, LR: 0.001, Batch Size: 16, Loss: 0.3314\n",
      "Epoch [9/50], Hidden Size: 32, LR: 0.001, Batch Size: 16, Loss: 0.3312\n",
      "Epoch [10/50], Hidden Size: 32, LR: 0.001, Batch Size: 16, Loss: 0.3356\n",
      "Epoch [11/50], Hidden Size: 32, LR: 0.001, Batch Size: 16, Loss: 0.3353\n",
      "Epoch [12/50], Hidden Size: 32, LR: 0.001, Batch Size: 16, Loss: 0.3386\n",
      "Epoch [13/50], Hidden Size: 32, LR: 0.001, Batch Size: 16, Loss: 0.3473\n",
      "Epoch [14/50], Hidden Size: 32, LR: 0.001, Batch Size: 16, Loss: 0.3319\n",
      "Epoch [15/50], Hidden Size: 32, LR: 0.001, Batch Size: 16, Loss: 0.3395\n",
      "Epoch [16/50], Hidden Size: 32, LR: 0.001, Batch Size: 16, Loss: 0.3420\n",
      "Epoch [17/50], Hidden Size: 32, LR: 0.001, Batch Size: 16, Loss: 0.3348\n",
      "Epoch [18/50], Hidden Size: 32, LR: 0.001, Batch Size: 16, Loss: 0.3328\n",
      "Epoch [19/50], Hidden Size: 32, LR: 0.001, Batch Size: 16, Loss: 0.3379\n",
      "Epoch [20/50], Hidden Size: 32, LR: 0.001, Batch Size: 16, Loss: 0.3323\n",
      "Epoch [21/50], Hidden Size: 32, LR: 0.001, Batch Size: 16, Loss: 0.3326\n",
      "Epoch [22/50], Hidden Size: 32, LR: 0.001, Batch Size: 16, Loss: 0.3536\n",
      "Epoch [23/50], Hidden Size: 32, LR: 0.001, Batch Size: 16, Loss: 0.3424\n",
      "Epoch [24/50], Hidden Size: 32, LR: 0.001, Batch Size: 16, Loss: 0.3360\n",
      "Epoch [25/50], Hidden Size: 32, LR: 0.001, Batch Size: 16, Loss: 0.3314\n",
      "Epoch [26/50], Hidden Size: 32, LR: 0.001, Batch Size: 16, Loss: 0.3336\n",
      "Epoch [27/50], Hidden Size: 32, LR: 0.001, Batch Size: 16, Loss: 0.3345\n",
      "Epoch [28/50], Hidden Size: 32, LR: 0.001, Batch Size: 16, Loss: 0.3319\n",
      "Epoch [29/50], Hidden Size: 32, LR: 0.001, Batch Size: 16, Loss: 0.3354\n",
      "Epoch [30/50], Hidden Size: 32, LR: 0.001, Batch Size: 16, Loss: 0.3331\n",
      "Epoch [31/50], Hidden Size: 32, LR: 0.001, Batch Size: 16, Loss: 0.3353\n",
      "Epoch [32/50], Hidden Size: 32, LR: 0.001, Batch Size: 16, Loss: 0.3384\n",
      "Epoch [33/50], Hidden Size: 32, LR: 0.001, Batch Size: 16, Loss: 0.3355\n",
      "Epoch [34/50], Hidden Size: 32, LR: 0.001, Batch Size: 16, Loss: 0.3370\n",
      "Epoch [35/50], Hidden Size: 32, LR: 0.001, Batch Size: 16, Loss: 0.3320\n",
      "Epoch [36/50], Hidden Size: 32, LR: 0.001, Batch Size: 16, Loss: 0.3337\n",
      "Epoch [37/50], Hidden Size: 32, LR: 0.001, Batch Size: 16, Loss: 0.3351\n",
      "Epoch [38/50], Hidden Size: 32, LR: 0.001, Batch Size: 16, Loss: 0.3285\n",
      "Epoch [39/50], Hidden Size: 32, LR: 0.001, Batch Size: 16, Loss: 0.3386\n",
      "Epoch [40/50], Hidden Size: 32, LR: 0.001, Batch Size: 16, Loss: 0.3333\n",
      "Epoch [41/50], Hidden Size: 32, LR: 0.001, Batch Size: 16, Loss: 0.3336\n",
      "Epoch [42/50], Hidden Size: 32, LR: 0.001, Batch Size: 16, Loss: 0.3347\n",
      "Epoch [43/50], Hidden Size: 32, LR: 0.001, Batch Size: 16, Loss: 0.3393\n",
      "Epoch [44/50], Hidden Size: 32, LR: 0.001, Batch Size: 16, Loss: 0.3305\n",
      "Epoch [45/50], Hidden Size: 32, LR: 0.001, Batch Size: 16, Loss: 0.3364\n",
      "Epoch [46/50], Hidden Size: 32, LR: 0.001, Batch Size: 16, Loss: 0.3332\n",
      "Epoch [47/50], Hidden Size: 32, LR: 0.001, Batch Size: 16, Loss: 0.3391\n",
      "Epoch [48/50], Hidden Size: 32, LR: 0.001, Batch Size: 16, Loss: 0.3331\n",
      "Epoch [49/50], Hidden Size: 32, LR: 0.001, Batch Size: 16, Loss: 0.3308\n",
      "Epoch [50/50], Hidden Size: 32, LR: 0.001, Batch Size: 16, Loss: 0.3357\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\10374\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torchvision\\models\\_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "c:\\Users\\10374\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torchvision\\models\\_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=None`.\n",
      "  warnings.warn(msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/50], Hidden Size: 32, LR: 0.001, Batch Size: 32, Loss: 0.9606\n",
      "Epoch [2/50], Hidden Size: 32, LR: 0.001, Batch Size: 32, Loss: 0.7038\n",
      "Epoch [3/50], Hidden Size: 32, LR: 0.001, Batch Size: 32, Loss: 0.3803\n",
      "Epoch [4/50], Hidden Size: 32, LR: 0.001, Batch Size: 32, Loss: 0.3452\n",
      "Epoch [5/50], Hidden Size: 32, LR: 0.001, Batch Size: 32, Loss: 0.3354\n",
      "Epoch [6/50], Hidden Size: 32, LR: 0.001, Batch Size: 32, Loss: 0.3336\n",
      "Epoch [7/50], Hidden Size: 32, LR: 0.001, Batch Size: 32, Loss: 0.3340\n",
      "Epoch [8/50], Hidden Size: 32, LR: 0.001, Batch Size: 32, Loss: 0.3333\n",
      "Epoch [9/50], Hidden Size: 32, LR: 0.001, Batch Size: 32, Loss: 0.3387\n",
      "Epoch [10/50], Hidden Size: 32, LR: 0.001, Batch Size: 32, Loss: 0.3349\n",
      "Epoch [11/50], Hidden Size: 32, LR: 0.001, Batch Size: 32, Loss: 0.3347\n",
      "Epoch [12/50], Hidden Size: 32, LR: 0.001, Batch Size: 32, Loss: 0.3358\n",
      "Epoch [13/50], Hidden Size: 32, LR: 0.001, Batch Size: 32, Loss: 0.3355\n",
      "Epoch [14/50], Hidden Size: 32, LR: 0.001, Batch Size: 32, Loss: 0.3344\n",
      "Epoch [15/50], Hidden Size: 32, LR: 0.001, Batch Size: 32, Loss: 0.3355\n",
      "Epoch [16/50], Hidden Size: 32, LR: 0.001, Batch Size: 32, Loss: 0.3347\n",
      "Epoch [17/50], Hidden Size: 32, LR: 0.001, Batch Size: 32, Loss: 0.3329\n",
      "Epoch [18/50], Hidden Size: 32, LR: 0.001, Batch Size: 32, Loss: 0.3368\n",
      "Epoch [19/50], Hidden Size: 32, LR: 0.001, Batch Size: 32, Loss: 0.3362\n",
      "Epoch [20/50], Hidden Size: 32, LR: 0.001, Batch Size: 32, Loss: 0.3342\n",
      "Epoch [21/50], Hidden Size: 32, LR: 0.001, Batch Size: 32, Loss: 0.3345\n",
      "Epoch [22/50], Hidden Size: 32, LR: 0.001, Batch Size: 32, Loss: 0.3357\n",
      "Epoch [23/50], Hidden Size: 32, LR: 0.001, Batch Size: 32, Loss: 0.3323\n",
      "Epoch [24/50], Hidden Size: 32, LR: 0.001, Batch Size: 32, Loss: 0.3347\n",
      "Epoch [25/50], Hidden Size: 32, LR: 0.001, Batch Size: 32, Loss: 0.3352\n",
      "Epoch [26/50], Hidden Size: 32, LR: 0.001, Batch Size: 32, Loss: 0.3320\n",
      "Epoch [27/50], Hidden Size: 32, LR: 0.001, Batch Size: 32, Loss: 0.3399\n",
      "Epoch [28/50], Hidden Size: 32, LR: 0.001, Batch Size: 32, Loss: 0.3358\n",
      "Epoch [29/50], Hidden Size: 32, LR: 0.001, Batch Size: 32, Loss: 0.3358\n",
      "Epoch [30/50], Hidden Size: 32, LR: 0.001, Batch Size: 32, Loss: 0.3355\n",
      "Epoch [31/50], Hidden Size: 32, LR: 0.001, Batch Size: 32, Loss: 0.3355\n",
      "Epoch [32/50], Hidden Size: 32, LR: 0.001, Batch Size: 32, Loss: 0.3365\n",
      "Epoch [33/50], Hidden Size: 32, LR: 0.001, Batch Size: 32, Loss: 0.3341\n",
      "Epoch [34/50], Hidden Size: 32, LR: 0.001, Batch Size: 32, Loss: 0.3360\n",
      "Epoch [35/50], Hidden Size: 32, LR: 0.001, Batch Size: 32, Loss: 0.3329\n",
      "Epoch [36/50], Hidden Size: 32, LR: 0.001, Batch Size: 32, Loss: 0.3341\n",
      "Epoch [37/50], Hidden Size: 32, LR: 0.001, Batch Size: 32, Loss: 0.3352\n",
      "Epoch [38/50], Hidden Size: 32, LR: 0.001, Batch Size: 32, Loss: 0.3368\n",
      "Epoch [39/50], Hidden Size: 32, LR: 0.001, Batch Size: 32, Loss: 0.3355\n",
      "Epoch [40/50], Hidden Size: 32, LR: 0.001, Batch Size: 32, Loss: 0.3350\n",
      "Epoch [41/50], Hidden Size: 32, LR: 0.001, Batch Size: 32, Loss: 0.3351\n",
      "Epoch [42/50], Hidden Size: 32, LR: 0.001, Batch Size: 32, Loss: 0.3344\n",
      "Epoch [43/50], Hidden Size: 32, LR: 0.001, Batch Size: 32, Loss: 0.3359\n",
      "Epoch [44/50], Hidden Size: 32, LR: 0.001, Batch Size: 32, Loss: 0.3335\n",
      "Epoch [45/50], Hidden Size: 32, LR: 0.001, Batch Size: 32, Loss: 0.3344\n",
      "Epoch [46/50], Hidden Size: 32, LR: 0.001, Batch Size: 32, Loss: 0.3383\n",
      "Epoch [47/50], Hidden Size: 32, LR: 0.001, Batch Size: 32, Loss: 0.3371\n",
      "Epoch [48/50], Hidden Size: 32, LR: 0.001, Batch Size: 32, Loss: 0.3343\n",
      "Epoch [49/50], Hidden Size: 32, LR: 0.001, Batch Size: 32, Loss: 0.3344\n",
      "Epoch [50/50], Hidden Size: 32, LR: 0.001, Batch Size: 32, Loss: 0.3374\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\10374\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torchvision\\models\\_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "c:\\Users\\10374\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torchvision\\models\\_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=None`.\n",
      "  warnings.warn(msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/50], Hidden Size: 32, LR: 0.001, Batch Size: 64, Loss: 0.9941\n",
      "Epoch [2/50], Hidden Size: 32, LR: 0.001, Batch Size: 64, Loss: 0.9525\n",
      "Epoch [3/50], Hidden Size: 32, LR: 0.001, Batch Size: 64, Loss: 0.8561\n",
      "Epoch [4/50], Hidden Size: 32, LR: 0.001, Batch Size: 64, Loss: 0.6962\n",
      "Epoch [5/50], Hidden Size: 32, LR: 0.001, Batch Size: 64, Loss: 0.4681\n",
      "Epoch [6/50], Hidden Size: 32, LR: 0.001, Batch Size: 64, Loss: 0.3543\n",
      "Epoch [7/50], Hidden Size: 32, LR: 0.001, Batch Size: 64, Loss: 0.3626\n",
      "Epoch [8/50], Hidden Size: 32, LR: 0.001, Batch Size: 64, Loss: 0.3457\n",
      "Epoch [9/50], Hidden Size: 32, LR: 0.001, Batch Size: 64, Loss: 0.3332\n",
      "Epoch [10/50], Hidden Size: 32, LR: 0.001, Batch Size: 64, Loss: 0.3326\n",
      "Epoch [11/50], Hidden Size: 32, LR: 0.001, Batch Size: 64, Loss: 0.3329\n",
      "Epoch [12/50], Hidden Size: 32, LR: 0.001, Batch Size: 64, Loss: 0.3283\n",
      "Epoch [13/50], Hidden Size: 32, LR: 0.001, Batch Size: 64, Loss: 0.3295\n",
      "Epoch [14/50], Hidden Size: 32, LR: 0.001, Batch Size: 64, Loss: 0.3376\n",
      "Epoch [15/50], Hidden Size: 32, LR: 0.001, Batch Size: 64, Loss: 0.3413\n",
      "Epoch [16/50], Hidden Size: 32, LR: 0.001, Batch Size: 64, Loss: 0.3341\n",
      "Epoch [17/50], Hidden Size: 32, LR: 0.001, Batch Size: 64, Loss: 0.3323\n",
      "Epoch [18/50], Hidden Size: 32, LR: 0.001, Batch Size: 64, Loss: 0.3292\n",
      "Epoch [19/50], Hidden Size: 32, LR: 0.001, Batch Size: 64, Loss: 0.3305\n",
      "Epoch [20/50], Hidden Size: 32, LR: 0.001, Batch Size: 64, Loss: 0.3368\n",
      "Epoch [21/50], Hidden Size: 32, LR: 0.001, Batch Size: 64, Loss: 0.3320\n",
      "Epoch [22/50], Hidden Size: 32, LR: 0.001, Batch Size: 64, Loss: 0.3265\n",
      "Epoch [23/50], Hidden Size: 32, LR: 0.001, Batch Size: 64, Loss: 0.3422\n",
      "Epoch [24/50], Hidden Size: 32, LR: 0.001, Batch Size: 64, Loss: 0.3371\n",
      "Epoch [25/50], Hidden Size: 32, LR: 0.001, Batch Size: 64, Loss: 0.3366\n",
      "Epoch [26/50], Hidden Size: 32, LR: 0.001, Batch Size: 64, Loss: 0.3360\n",
      "Epoch [27/50], Hidden Size: 32, LR: 0.001, Batch Size: 64, Loss: 0.3357\n",
      "Epoch [28/50], Hidden Size: 32, LR: 0.001, Batch Size: 64, Loss: 0.3393\n",
      "Epoch [29/50], Hidden Size: 32, LR: 0.001, Batch Size: 64, Loss: 0.3292\n",
      "Epoch [30/50], Hidden Size: 32, LR: 0.001, Batch Size: 64, Loss: 0.3334\n",
      "Epoch [31/50], Hidden Size: 32, LR: 0.001, Batch Size: 64, Loss: 0.3292\n",
      "Epoch [32/50], Hidden Size: 32, LR: 0.001, Batch Size: 64, Loss: 0.3377\n",
      "Epoch [33/50], Hidden Size: 32, LR: 0.001, Batch Size: 64, Loss: 0.3275\n",
      "Epoch [34/50], Hidden Size: 32, LR: 0.001, Batch Size: 64, Loss: 0.3258\n",
      "Epoch [35/50], Hidden Size: 32, LR: 0.001, Batch Size: 64, Loss: 0.3324\n",
      "Epoch [36/50], Hidden Size: 32, LR: 0.001, Batch Size: 64, Loss: 0.3393\n",
      "Epoch [37/50], Hidden Size: 32, LR: 0.001, Batch Size: 64, Loss: 0.3418\n",
      "Epoch [38/50], Hidden Size: 32, LR: 0.001, Batch Size: 64, Loss: 0.3280\n",
      "Epoch [39/50], Hidden Size: 32, LR: 0.001, Batch Size: 64, Loss: 0.3350\n",
      "Epoch [40/50], Hidden Size: 32, LR: 0.001, Batch Size: 64, Loss: 0.3372\n",
      "Epoch [41/50], Hidden Size: 32, LR: 0.001, Batch Size: 64, Loss: 0.3364\n",
      "Epoch [42/50], Hidden Size: 32, LR: 0.001, Batch Size: 64, Loss: 0.3291\n",
      "Epoch [43/50], Hidden Size: 32, LR: 0.001, Batch Size: 64, Loss: 0.3365\n",
      "Epoch [44/50], Hidden Size: 32, LR: 0.001, Batch Size: 64, Loss: 0.3394\n",
      "Epoch [45/50], Hidden Size: 32, LR: 0.001, Batch Size: 64, Loss: 0.3305\n",
      "Epoch [46/50], Hidden Size: 32, LR: 0.001, Batch Size: 64, Loss: 0.3337\n",
      "Epoch [47/50], Hidden Size: 32, LR: 0.001, Batch Size: 64, Loss: 0.3331\n",
      "Epoch [48/50], Hidden Size: 32, LR: 0.001, Batch Size: 64, Loss: 0.3376\n",
      "Epoch [49/50], Hidden Size: 32, LR: 0.001, Batch Size: 64, Loss: 0.3395\n",
      "Epoch [50/50], Hidden Size: 32, LR: 0.001, Batch Size: 64, Loss: 0.3345\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\10374\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torchvision\\models\\_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "c:\\Users\\10374\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torchvision\\models\\_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=None`.\n",
      "  warnings.warn(msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/50], Hidden Size: 32, LR: 0.0001, Batch Size: 16, Loss: 0.9978\n",
      "Epoch [2/50], Hidden Size: 32, LR: 0.0001, Batch Size: 16, Loss: 0.9885\n",
      "Epoch [3/50], Hidden Size: 32, LR: 0.0001, Batch Size: 16, Loss: 0.9674\n",
      "Epoch [4/50], Hidden Size: 32, LR: 0.0001, Batch Size: 16, Loss: 0.9276\n",
      "Epoch [5/50], Hidden Size: 32, LR: 0.0001, Batch Size: 16, Loss: 0.8711\n",
      "Epoch [6/50], Hidden Size: 32, LR: 0.0001, Batch Size: 16, Loss: 0.7948\n",
      "Epoch [7/50], Hidden Size: 32, LR: 0.0001, Batch Size: 16, Loss: 0.7010\n",
      "Epoch [8/50], Hidden Size: 32, LR: 0.0001, Batch Size: 16, Loss: 0.5892\n",
      "Epoch [9/50], Hidden Size: 32, LR: 0.0001, Batch Size: 16, Loss: 0.4734\n",
      "Epoch [10/50], Hidden Size: 32, LR: 0.0001, Batch Size: 16, Loss: 0.3713\n",
      "Epoch [11/50], Hidden Size: 32, LR: 0.0001, Batch Size: 16, Loss: 0.3352\n",
      "Epoch [12/50], Hidden Size: 32, LR: 0.0001, Batch Size: 16, Loss: 0.3304\n",
      "Epoch [13/50], Hidden Size: 32, LR: 0.0001, Batch Size: 16, Loss: 0.3317\n",
      "Epoch [14/50], Hidden Size: 32, LR: 0.0001, Batch Size: 16, Loss: 0.3310\n",
      "Epoch [15/50], Hidden Size: 32, LR: 0.0001, Batch Size: 16, Loss: 0.3334\n",
      "Epoch [16/50], Hidden Size: 32, LR: 0.0001, Batch Size: 16, Loss: 0.3318\n",
      "Epoch [17/50], Hidden Size: 32, LR: 0.0001, Batch Size: 16, Loss: 0.3321\n",
      "Epoch [18/50], Hidden Size: 32, LR: 0.0001, Batch Size: 16, Loss: 0.3345\n",
      "Epoch [19/50], Hidden Size: 32, LR: 0.0001, Batch Size: 16, Loss: 0.3341\n",
      "Epoch [20/50], Hidden Size: 32, LR: 0.0001, Batch Size: 16, Loss: 0.3321\n",
      "Epoch [21/50], Hidden Size: 32, LR: 0.0001, Batch Size: 16, Loss: 0.3305\n",
      "Epoch [22/50], Hidden Size: 32, LR: 0.0001, Batch Size: 16, Loss: 0.3322\n",
      "Epoch [23/50], Hidden Size: 32, LR: 0.0001, Batch Size: 16, Loss: 0.3330\n",
      "Epoch [24/50], Hidden Size: 32, LR: 0.0001, Batch Size: 16, Loss: 0.3275\n",
      "Epoch [25/50], Hidden Size: 32, LR: 0.0001, Batch Size: 16, Loss: 0.3323\n",
      "Epoch [26/50], Hidden Size: 32, LR: 0.0001, Batch Size: 16, Loss: 0.3328\n",
      "Epoch [27/50], Hidden Size: 32, LR: 0.0001, Batch Size: 16, Loss: 0.3315\n",
      "Epoch [28/50], Hidden Size: 32, LR: 0.0001, Batch Size: 16, Loss: 0.3303\n",
      "Epoch [29/50], Hidden Size: 32, LR: 0.0001, Batch Size: 16, Loss: 0.3313\n",
      "Epoch [30/50], Hidden Size: 32, LR: 0.0001, Batch Size: 16, Loss: 0.3294\n",
      "Epoch [31/50], Hidden Size: 32, LR: 0.0001, Batch Size: 16, Loss: 0.3317\n",
      "Epoch [32/50], Hidden Size: 32, LR: 0.0001, Batch Size: 16, Loss: 0.3329\n",
      "Epoch [33/50], Hidden Size: 32, LR: 0.0001, Batch Size: 16, Loss: 0.3304\n",
      "Epoch [34/50], Hidden Size: 32, LR: 0.0001, Batch Size: 16, Loss: 0.3301\n",
      "Epoch [35/50], Hidden Size: 32, LR: 0.0001, Batch Size: 16, Loss: 0.3317\n",
      "Epoch [36/50], Hidden Size: 32, LR: 0.0001, Batch Size: 16, Loss: 0.3315\n",
      "Epoch [37/50], Hidden Size: 32, LR: 0.0001, Batch Size: 16, Loss: 0.3332\n",
      "Epoch [38/50], Hidden Size: 32, LR: 0.0001, Batch Size: 16, Loss: 0.3309\n",
      "Epoch [39/50], Hidden Size: 32, LR: 0.0001, Batch Size: 16, Loss: 0.3292\n",
      "Epoch [40/50], Hidden Size: 32, LR: 0.0001, Batch Size: 16, Loss: 0.3313\n",
      "Epoch [41/50], Hidden Size: 32, LR: 0.0001, Batch Size: 16, Loss: 0.3322\n",
      "Epoch [42/50], Hidden Size: 32, LR: 0.0001, Batch Size: 16, Loss: 0.3325\n",
      "Epoch [43/50], Hidden Size: 32, LR: 0.0001, Batch Size: 16, Loss: 0.3343\n",
      "Epoch [44/50], Hidden Size: 32, LR: 0.0001, Batch Size: 16, Loss: 0.3354\n",
      "Epoch [45/50], Hidden Size: 32, LR: 0.0001, Batch Size: 16, Loss: 0.3340\n",
      "Epoch [46/50], Hidden Size: 32, LR: 0.0001, Batch Size: 16, Loss: 0.3288\n",
      "Epoch [47/50], Hidden Size: 32, LR: 0.0001, Batch Size: 16, Loss: 0.3288\n",
      "Epoch [48/50], Hidden Size: 32, LR: 0.0001, Batch Size: 16, Loss: 0.3311\n",
      "Epoch [49/50], Hidden Size: 32, LR: 0.0001, Batch Size: 16, Loss: 0.3321\n",
      "Epoch [50/50], Hidden Size: 32, LR: 0.0001, Batch Size: 16, Loss: 0.3334\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\10374\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torchvision\\models\\_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "c:\\Users\\10374\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torchvision\\models\\_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=None`.\n",
      "  warnings.warn(msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/50], Hidden Size: 32, LR: 0.0001, Batch Size: 32, Loss: 0.9989\n",
      "Epoch [2/50], Hidden Size: 32, LR: 0.0001, Batch Size: 32, Loss: 0.9959\n",
      "Epoch [3/50], Hidden Size: 32, LR: 0.0001, Batch Size: 32, Loss: 0.9910\n",
      "Epoch [4/50], Hidden Size: 32, LR: 0.0001, Batch Size: 32, Loss: 0.9829\n",
      "Epoch [5/50], Hidden Size: 32, LR: 0.0001, Batch Size: 32, Loss: 0.9698\n",
      "Epoch [6/50], Hidden Size: 32, LR: 0.0001, Batch Size: 32, Loss: 0.9510\n",
      "Epoch [7/50], Hidden Size: 32, LR: 0.0001, Batch Size: 32, Loss: 0.9269\n",
      "Epoch [8/50], Hidden Size: 32, LR: 0.0001, Batch Size: 32, Loss: 0.8982\n",
      "Epoch [9/50], Hidden Size: 32, LR: 0.0001, Batch Size: 32, Loss: 0.8629\n",
      "Epoch [10/50], Hidden Size: 32, LR: 0.0001, Batch Size: 32, Loss: 0.8230\n",
      "Epoch [11/50], Hidden Size: 32, LR: 0.0001, Batch Size: 32, Loss: 0.7770\n",
      "Epoch [12/50], Hidden Size: 32, LR: 0.0001, Batch Size: 32, Loss: 0.7239\n",
      "Epoch [13/50], Hidden Size: 32, LR: 0.0001, Batch Size: 32, Loss: 0.6633\n",
      "Epoch [14/50], Hidden Size: 32, LR: 0.0001, Batch Size: 32, Loss: 0.6020\n",
      "Epoch [15/50], Hidden Size: 32, LR: 0.0001, Batch Size: 32, Loss: 0.5361\n",
      "Epoch [16/50], Hidden Size: 32, LR: 0.0001, Batch Size: 32, Loss: 0.4688\n",
      "Epoch [17/50], Hidden Size: 32, LR: 0.0001, Batch Size: 32, Loss: 0.4104\n",
      "Epoch [18/50], Hidden Size: 32, LR: 0.0001, Batch Size: 32, Loss: 0.3641\n",
      "Epoch [19/50], Hidden Size: 32, LR: 0.0001, Batch Size: 32, Loss: 0.3415\n",
      "Epoch [20/50], Hidden Size: 32, LR: 0.0001, Batch Size: 32, Loss: 0.3351\n",
      "Epoch [21/50], Hidden Size: 32, LR: 0.0001, Batch Size: 32, Loss: 0.3339\n",
      "Epoch [22/50], Hidden Size: 32, LR: 0.0001, Batch Size: 32, Loss: 0.3320\n",
      "Epoch [23/50], Hidden Size: 32, LR: 0.0001, Batch Size: 32, Loss: 0.3353\n",
      "Epoch [24/50], Hidden Size: 32, LR: 0.0001, Batch Size: 32, Loss: 0.3338\n",
      "Epoch [25/50], Hidden Size: 32, LR: 0.0001, Batch Size: 32, Loss: 0.3337\n",
      "Epoch [26/50], Hidden Size: 32, LR: 0.0001, Batch Size: 32, Loss: 0.3344\n",
      "Epoch [27/50], Hidden Size: 32, LR: 0.0001, Batch Size: 32, Loss: 0.3344\n",
      "Epoch [28/50], Hidden Size: 32, LR: 0.0001, Batch Size: 32, Loss: 0.3334\n",
      "Epoch [29/50], Hidden Size: 32, LR: 0.0001, Batch Size: 32, Loss: 0.3336\n",
      "Epoch [30/50], Hidden Size: 32, LR: 0.0001, Batch Size: 32, Loss: 0.3330\n",
      "Epoch [31/50], Hidden Size: 32, LR: 0.0001, Batch Size: 32, Loss: 0.3334\n",
      "Epoch [32/50], Hidden Size: 32, LR: 0.0001, Batch Size: 32, Loss: 0.3342\n",
      "Epoch [33/50], Hidden Size: 32, LR: 0.0001, Batch Size: 32, Loss: 0.3337\n",
      "Epoch [34/50], Hidden Size: 32, LR: 0.0001, Batch Size: 32, Loss: 0.3344\n",
      "Epoch [35/50], Hidden Size: 32, LR: 0.0001, Batch Size: 32, Loss: 0.3348\n",
      "Epoch [36/50], Hidden Size: 32, LR: 0.0001, Batch Size: 32, Loss: 0.3335\n",
      "Epoch [37/50], Hidden Size: 32, LR: 0.0001, Batch Size: 32, Loss: 0.3339\n",
      "Epoch [38/50], Hidden Size: 32, LR: 0.0001, Batch Size: 32, Loss: 0.3335\n",
      "Epoch [39/50], Hidden Size: 32, LR: 0.0001, Batch Size: 32, Loss: 0.3335\n",
      "Epoch [40/50], Hidden Size: 32, LR: 0.0001, Batch Size: 32, Loss: 0.3336\n",
      "Epoch [41/50], Hidden Size: 32, LR: 0.0001, Batch Size: 32, Loss: 0.3331\n",
      "Epoch [42/50], Hidden Size: 32, LR: 0.0001, Batch Size: 32, Loss: 0.3345\n",
      "Epoch [43/50], Hidden Size: 32, LR: 0.0001, Batch Size: 32, Loss: 0.3345\n",
      "Epoch [44/50], Hidden Size: 32, LR: 0.0001, Batch Size: 32, Loss: 0.3353\n",
      "Epoch [45/50], Hidden Size: 32, LR: 0.0001, Batch Size: 32, Loss: 0.3335\n",
      "Epoch [46/50], Hidden Size: 32, LR: 0.0001, Batch Size: 32, Loss: 0.3318\n",
      "Epoch [47/50], Hidden Size: 32, LR: 0.0001, Batch Size: 32, Loss: 0.3339\n",
      "Epoch [48/50], Hidden Size: 32, LR: 0.0001, Batch Size: 32, Loss: 0.3345\n",
      "Epoch [49/50], Hidden Size: 32, LR: 0.0001, Batch Size: 32, Loss: 0.3348\n",
      "Epoch [50/50], Hidden Size: 32, LR: 0.0001, Batch Size: 32, Loss: 0.3338\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\10374\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torchvision\\models\\_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "c:\\Users\\10374\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torchvision\\models\\_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=None`.\n",
      "  warnings.warn(msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/50], Hidden Size: 32, LR: 0.0001, Batch Size: 64, Loss: 0.9996\n",
      "Epoch [2/50], Hidden Size: 32, LR: 0.0001, Batch Size: 64, Loss: 0.9984\n",
      "Epoch [3/50], Hidden Size: 32, LR: 0.0001, Batch Size: 64, Loss: 0.9967\n",
      "Epoch [4/50], Hidden Size: 32, LR: 0.0001, Batch Size: 64, Loss: 0.9947\n",
      "Epoch [5/50], Hidden Size: 32, LR: 0.0001, Batch Size: 64, Loss: 0.9922\n",
      "Epoch [6/50], Hidden Size: 32, LR: 0.0001, Batch Size: 64, Loss: 0.9883\n",
      "Epoch [7/50], Hidden Size: 32, LR: 0.0001, Batch Size: 64, Loss: 0.9835\n",
      "Epoch [8/50], Hidden Size: 32, LR: 0.0001, Batch Size: 64, Loss: 0.9775\n",
      "Epoch [9/50], Hidden Size: 32, LR: 0.0001, Batch Size: 64, Loss: 0.9700\n",
      "Epoch [10/50], Hidden Size: 32, LR: 0.0001, Batch Size: 64, Loss: 0.9622\n",
      "Epoch [11/50], Hidden Size: 32, LR: 0.0001, Batch Size: 64, Loss: 0.9522\n",
      "Epoch [12/50], Hidden Size: 32, LR: 0.0001, Batch Size: 64, Loss: 0.9425\n",
      "Epoch [13/50], Hidden Size: 32, LR: 0.0001, Batch Size: 64, Loss: 0.9293\n",
      "Epoch [14/50], Hidden Size: 32, LR: 0.0001, Batch Size: 64, Loss: 0.9172\n",
      "Epoch [15/50], Hidden Size: 32, LR: 0.0001, Batch Size: 64, Loss: 0.9033\n",
      "Epoch [16/50], Hidden Size: 32, LR: 0.0001, Batch Size: 64, Loss: 0.8867\n",
      "Epoch [17/50], Hidden Size: 32, LR: 0.0001, Batch Size: 64, Loss: 0.8707\n",
      "Epoch [18/50], Hidden Size: 32, LR: 0.0001, Batch Size: 64, Loss: 0.8536\n",
      "Epoch [19/50], Hidden Size: 32, LR: 0.0001, Batch Size: 64, Loss: 0.8324\n",
      "Epoch [20/50], Hidden Size: 32, LR: 0.0001, Batch Size: 64, Loss: 0.8087\n",
      "Epoch [21/50], Hidden Size: 32, LR: 0.0001, Batch Size: 64, Loss: 0.7892\n",
      "Epoch [22/50], Hidden Size: 32, LR: 0.0001, Batch Size: 64, Loss: 0.7627\n",
      "Epoch [23/50], Hidden Size: 32, LR: 0.0001, Batch Size: 64, Loss: 0.7345\n",
      "Epoch [24/50], Hidden Size: 32, LR: 0.0001, Batch Size: 64, Loss: 0.7034\n",
      "Epoch [25/50], Hidden Size: 32, LR: 0.0001, Batch Size: 64, Loss: 0.6734\n",
      "Epoch [26/50], Hidden Size: 32, LR: 0.0001, Batch Size: 64, Loss: 0.6410\n",
      "Epoch [27/50], Hidden Size: 32, LR: 0.0001, Batch Size: 64, Loss: 0.6092\n",
      "Epoch [28/50], Hidden Size: 32, LR: 0.0001, Batch Size: 64, Loss: 0.5705\n",
      "Epoch [29/50], Hidden Size: 32, LR: 0.0001, Batch Size: 64, Loss: 0.5417\n",
      "Epoch [30/50], Hidden Size: 32, LR: 0.0001, Batch Size: 64, Loss: 0.5047\n",
      "Epoch [31/50], Hidden Size: 32, LR: 0.0001, Batch Size: 64, Loss: 0.4739\n",
      "Epoch [32/50], Hidden Size: 32, LR: 0.0001, Batch Size: 64, Loss: 0.4467\n",
      "Epoch [33/50], Hidden Size: 32, LR: 0.0001, Batch Size: 64, Loss: 0.4097\n",
      "Epoch [34/50], Hidden Size: 32, LR: 0.0001, Batch Size: 64, Loss: 0.3937\n",
      "Epoch [35/50], Hidden Size: 32, LR: 0.0001, Batch Size: 64, Loss: 0.3575\n",
      "Epoch [36/50], Hidden Size: 32, LR: 0.0001, Batch Size: 64, Loss: 0.3450\n",
      "Epoch [37/50], Hidden Size: 32, LR: 0.0001, Batch Size: 64, Loss: 0.3343\n",
      "Epoch [38/50], Hidden Size: 32, LR: 0.0001, Batch Size: 64, Loss: 0.3371\n",
      "Epoch [39/50], Hidden Size: 32, LR: 0.0001, Batch Size: 64, Loss: 0.3371\n",
      "Epoch [40/50], Hidden Size: 32, LR: 0.0001, Batch Size: 64, Loss: 0.3376\n",
      "Epoch [41/50], Hidden Size: 32, LR: 0.0001, Batch Size: 64, Loss: 0.3401\n",
      "Epoch [42/50], Hidden Size: 32, LR: 0.0001, Batch Size: 64, Loss: 0.3332\n",
      "Epoch [43/50], Hidden Size: 32, LR: 0.0001, Batch Size: 64, Loss: 0.3312\n",
      "Epoch [44/50], Hidden Size: 32, LR: 0.0001, Batch Size: 64, Loss: 0.3353\n",
      "Epoch [45/50], Hidden Size: 32, LR: 0.0001, Batch Size: 64, Loss: 0.3274\n",
      "Epoch [46/50], Hidden Size: 32, LR: 0.0001, Batch Size: 64, Loss: 0.3356\n",
      "Epoch [47/50], Hidden Size: 32, LR: 0.0001, Batch Size: 64, Loss: 0.3419\n",
      "Epoch [48/50], Hidden Size: 32, LR: 0.0001, Batch Size: 64, Loss: 0.3310\n",
      "Epoch [49/50], Hidden Size: 32, LR: 0.0001, Batch Size: 64, Loss: 0.3316\n",
      "Epoch [50/50], Hidden Size: 32, LR: 0.0001, Batch Size: 64, Loss: 0.3376\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\10374\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torchvision\\models\\_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "c:\\Users\\10374\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torchvision\\models\\_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=None`.\n",
      "  warnings.warn(msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/50], Hidden Size: 64, LR: 0.001, Batch Size: 16, Loss: 0.9390\n",
      "Epoch [2/50], Hidden Size: 64, LR: 0.001, Batch Size: 16, Loss: 0.4934\n",
      "Epoch [3/50], Hidden Size: 64, LR: 0.001, Batch Size: 16, Loss: 0.3472\n",
      "Epoch [4/50], Hidden Size: 64, LR: 0.001, Batch Size: 16, Loss: 0.3361\n",
      "Epoch [5/50], Hidden Size: 64, LR: 0.001, Batch Size: 16, Loss: 0.3359\n",
      "Epoch [6/50], Hidden Size: 64, LR: 0.001, Batch Size: 16, Loss: 0.3303\n",
      "Epoch [7/50], Hidden Size: 64, LR: 0.001, Batch Size: 16, Loss: 0.3345\n",
      "Epoch [8/50], Hidden Size: 64, LR: 0.001, Batch Size: 16, Loss: 0.3361\n",
      "Epoch [9/50], Hidden Size: 64, LR: 0.001, Batch Size: 16, Loss: 0.3379\n",
      "Epoch [10/50], Hidden Size: 64, LR: 0.001, Batch Size: 16, Loss: 0.3355\n",
      "Epoch [11/50], Hidden Size: 64, LR: 0.001, Batch Size: 16, Loss: 0.3352\n",
      "Epoch [12/50], Hidden Size: 64, LR: 0.001, Batch Size: 16, Loss: 0.3319\n",
      "Epoch [13/50], Hidden Size: 64, LR: 0.001, Batch Size: 16, Loss: 0.3336\n",
      "Epoch [14/50], Hidden Size: 64, LR: 0.001, Batch Size: 16, Loss: 0.3379\n",
      "Epoch [15/50], Hidden Size: 64, LR: 0.001, Batch Size: 16, Loss: 0.3329\n",
      "Epoch [16/50], Hidden Size: 64, LR: 0.001, Batch Size: 16, Loss: 0.3319\n",
      "Epoch [17/50], Hidden Size: 64, LR: 0.001, Batch Size: 16, Loss: 0.3376\n",
      "Epoch [18/50], Hidden Size: 64, LR: 0.001, Batch Size: 16, Loss: 0.3336\n",
      "Epoch [19/50], Hidden Size: 64, LR: 0.001, Batch Size: 16, Loss: 0.3333\n",
      "Epoch [20/50], Hidden Size: 64, LR: 0.001, Batch Size: 16, Loss: 0.3348\n",
      "Epoch [21/50], Hidden Size: 64, LR: 0.001, Batch Size: 16, Loss: 0.3366\n",
      "Epoch [22/50], Hidden Size: 64, LR: 0.001, Batch Size: 16, Loss: 0.3342\n",
      "Epoch [23/50], Hidden Size: 64, LR: 0.001, Batch Size: 16, Loss: 0.3336\n",
      "Epoch [24/50], Hidden Size: 64, LR: 0.001, Batch Size: 16, Loss: 0.3362\n",
      "Epoch [25/50], Hidden Size: 64, LR: 0.001, Batch Size: 16, Loss: 0.3366\n",
      "Epoch [26/50], Hidden Size: 64, LR: 0.001, Batch Size: 16, Loss: 0.3336\n",
      "Epoch [27/50], Hidden Size: 64, LR: 0.001, Batch Size: 16, Loss: 0.3357\n",
      "Epoch [28/50], Hidden Size: 64, LR: 0.001, Batch Size: 16, Loss: 0.3415\n",
      "Epoch [29/50], Hidden Size: 64, LR: 0.001, Batch Size: 16, Loss: 0.3387\n",
      "Epoch [30/50], Hidden Size: 64, LR: 0.001, Batch Size: 16, Loss: 0.3326\n",
      "Epoch [31/50], Hidden Size: 64, LR: 0.001, Batch Size: 16, Loss: 0.3325\n",
      "Epoch [32/50], Hidden Size: 64, LR: 0.001, Batch Size: 16, Loss: 0.3318\n",
      "Epoch [33/50], Hidden Size: 64, LR: 0.001, Batch Size: 16, Loss: 0.3425\n",
      "Epoch [34/50], Hidden Size: 64, LR: 0.001, Batch Size: 16, Loss: 0.3383\n",
      "Epoch [35/50], Hidden Size: 64, LR: 0.001, Batch Size: 16, Loss: 0.3340\n",
      "Epoch [36/50], Hidden Size: 64, LR: 0.001, Batch Size: 16, Loss: 0.3343\n",
      "Epoch [37/50], Hidden Size: 64, LR: 0.001, Batch Size: 16, Loss: 0.3338\n",
      "Epoch [38/50], Hidden Size: 64, LR: 0.001, Batch Size: 16, Loss: 0.3342\n",
      "Epoch [39/50], Hidden Size: 64, LR: 0.001, Batch Size: 16, Loss: 0.3340\n",
      "Epoch [40/50], Hidden Size: 64, LR: 0.001, Batch Size: 16, Loss: 0.3347\n",
      "Epoch [41/50], Hidden Size: 64, LR: 0.001, Batch Size: 16, Loss: 0.3360\n",
      "Epoch [42/50], Hidden Size: 64, LR: 0.001, Batch Size: 16, Loss: 0.3319\n",
      "Epoch [43/50], Hidden Size: 64, LR: 0.001, Batch Size: 16, Loss: 0.3356\n",
      "Epoch [44/50], Hidden Size: 64, LR: 0.001, Batch Size: 16, Loss: 0.3332\n",
      "Epoch [45/50], Hidden Size: 64, LR: 0.001, Batch Size: 16, Loss: 0.3358\n",
      "Epoch [46/50], Hidden Size: 64, LR: 0.001, Batch Size: 16, Loss: 0.3375\n",
      "Epoch [47/50], Hidden Size: 64, LR: 0.001, Batch Size: 16, Loss: 0.3341\n",
      "Epoch [48/50], Hidden Size: 64, LR: 0.001, Batch Size: 16, Loss: 0.3387\n",
      "Epoch [49/50], Hidden Size: 64, LR: 0.001, Batch Size: 16, Loss: 0.3340\n",
      "Epoch [50/50], Hidden Size: 64, LR: 0.001, Batch Size: 16, Loss: 0.3350\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\10374\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torchvision\\models\\_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "c:\\Users\\10374\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torchvision\\models\\_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=None`.\n",
      "  warnings.warn(msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/50], Hidden Size: 64, LR: 0.001, Batch Size: 32, Loss: 0.9739\n",
      "Epoch [2/50], Hidden Size: 64, LR: 0.001, Batch Size: 32, Loss: 0.7999\n",
      "Epoch [3/50], Hidden Size: 64, LR: 0.001, Batch Size: 32, Loss: 0.4388\n",
      "Epoch [4/50], Hidden Size: 64, LR: 0.001, Batch Size: 32, Loss: 0.3716\n",
      "Epoch [5/50], Hidden Size: 64, LR: 0.001, Batch Size: 32, Loss: 0.3346\n",
      "Epoch [6/50], Hidden Size: 64, LR: 0.001, Batch Size: 32, Loss: 0.3337\n",
      "Epoch [7/50], Hidden Size: 64, LR: 0.001, Batch Size: 32, Loss: 0.3337\n",
      "Epoch [8/50], Hidden Size: 64, LR: 0.001, Batch Size: 32, Loss: 0.3351\n",
      "Epoch [9/50], Hidden Size: 64, LR: 0.001, Batch Size: 32, Loss: 0.3353\n",
      "Epoch [10/50], Hidden Size: 64, LR: 0.001, Batch Size: 32, Loss: 0.3346\n",
      "Epoch [11/50], Hidden Size: 64, LR: 0.001, Batch Size: 32, Loss: 0.3348\n",
      "Epoch [12/50], Hidden Size: 64, LR: 0.001, Batch Size: 32, Loss: 0.3329\n",
      "Epoch [13/50], Hidden Size: 64, LR: 0.001, Batch Size: 32, Loss: 0.3343\n",
      "Epoch [14/50], Hidden Size: 64, LR: 0.001, Batch Size: 32, Loss: 0.3342\n",
      "Epoch [15/50], Hidden Size: 64, LR: 0.001, Batch Size: 32, Loss: 0.3347\n",
      "Epoch [16/50], Hidden Size: 64, LR: 0.001, Batch Size: 32, Loss: 0.3315\n",
      "Epoch [17/50], Hidden Size: 64, LR: 0.001, Batch Size: 32, Loss: 0.3341\n",
      "Epoch [18/50], Hidden Size: 64, LR: 0.001, Batch Size: 32, Loss: 0.3336\n",
      "Epoch [19/50], Hidden Size: 64, LR: 0.001, Batch Size: 32, Loss: 0.3351\n",
      "Epoch [20/50], Hidden Size: 64, LR: 0.001, Batch Size: 32, Loss: 0.3339\n",
      "Epoch [21/50], Hidden Size: 64, LR: 0.001, Batch Size: 32, Loss: 0.3347\n",
      "Epoch [22/50], Hidden Size: 64, LR: 0.001, Batch Size: 32, Loss: 0.3316\n",
      "Epoch [23/50], Hidden Size: 64, LR: 0.001, Batch Size: 32, Loss: 0.3340\n",
      "Epoch [24/50], Hidden Size: 64, LR: 0.001, Batch Size: 32, Loss: 0.3334\n",
      "Epoch [25/50], Hidden Size: 64, LR: 0.001, Batch Size: 32, Loss: 0.3346\n",
      "Epoch [26/50], Hidden Size: 64, LR: 0.001, Batch Size: 32, Loss: 0.3348\n",
      "Epoch [27/50], Hidden Size: 64, LR: 0.001, Batch Size: 32, Loss: 0.3352\n",
      "Epoch [28/50], Hidden Size: 64, LR: 0.001, Batch Size: 32, Loss: 0.3385\n",
      "Epoch [29/50], Hidden Size: 64, LR: 0.001, Batch Size: 32, Loss: 0.3352\n",
      "Epoch [30/50], Hidden Size: 64, LR: 0.001, Batch Size: 32, Loss: 0.3354\n",
      "Epoch [31/50], Hidden Size: 64, LR: 0.001, Batch Size: 32, Loss: 0.3348\n",
      "Epoch [32/50], Hidden Size: 64, LR: 0.001, Batch Size: 32, Loss: 0.3344\n",
      "Epoch [33/50], Hidden Size: 64, LR: 0.001, Batch Size: 32, Loss: 0.3339\n",
      "Epoch [34/50], Hidden Size: 64, LR: 0.001, Batch Size: 32, Loss: 0.3347\n",
      "Epoch [35/50], Hidden Size: 64, LR: 0.001, Batch Size: 32, Loss: 0.3356\n",
      "Epoch [36/50], Hidden Size: 64, LR: 0.001, Batch Size: 32, Loss: 0.3336\n",
      "Epoch [37/50], Hidden Size: 64, LR: 0.001, Batch Size: 32, Loss: 0.3359\n",
      "Epoch [38/50], Hidden Size: 64, LR: 0.001, Batch Size: 32, Loss: 0.3334\n",
      "Epoch [39/50], Hidden Size: 64, LR: 0.001, Batch Size: 32, Loss: 0.3343\n",
      "Epoch [40/50], Hidden Size: 64, LR: 0.001, Batch Size: 32, Loss: 0.3367\n",
      "Epoch [41/50], Hidden Size: 64, LR: 0.001, Batch Size: 32, Loss: 0.3382\n",
      "Epoch [42/50], Hidden Size: 64, LR: 0.001, Batch Size: 32, Loss: 0.3383\n",
      "Epoch [43/50], Hidden Size: 64, LR: 0.001, Batch Size: 32, Loss: 0.3334\n",
      "Epoch [44/50], Hidden Size: 64, LR: 0.001, Batch Size: 32, Loss: 0.3329\n",
      "Epoch [45/50], Hidden Size: 64, LR: 0.001, Batch Size: 32, Loss: 0.3337\n",
      "Epoch [46/50], Hidden Size: 64, LR: 0.001, Batch Size: 32, Loss: 0.3360\n",
      "Epoch [47/50], Hidden Size: 64, LR: 0.001, Batch Size: 32, Loss: 0.3358\n",
      "Epoch [48/50], Hidden Size: 64, LR: 0.001, Batch Size: 32, Loss: 0.3370\n",
      "Epoch [49/50], Hidden Size: 64, LR: 0.001, Batch Size: 32, Loss: 0.3339\n",
      "Epoch [50/50], Hidden Size: 64, LR: 0.001, Batch Size: 32, Loss: 0.3360\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\10374\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torchvision\\models\\_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "c:\\Users\\10374\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torchvision\\models\\_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=None`.\n",
      "  warnings.warn(msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/50], Hidden Size: 64, LR: 0.001, Batch Size: 64, Loss: 0.9931\n",
      "Epoch [2/50], Hidden Size: 64, LR: 0.001, Batch Size: 64, Loss: 0.9493\n",
      "Epoch [3/50], Hidden Size: 64, LR: 0.001, Batch Size: 64, Loss: 0.8533\n",
      "Epoch [4/50], Hidden Size: 64, LR: 0.001, Batch Size: 64, Loss: 0.6944\n",
      "Epoch [5/50], Hidden Size: 64, LR: 0.001, Batch Size: 64, Loss: 0.4712\n",
      "Epoch [6/50], Hidden Size: 64, LR: 0.001, Batch Size: 64, Loss: 0.3578\n",
      "Epoch [7/50], Hidden Size: 64, LR: 0.001, Batch Size: 64, Loss: 0.3724\n",
      "Epoch [8/50], Hidden Size: 64, LR: 0.001, Batch Size: 64, Loss: 0.3385\n",
      "Epoch [9/50], Hidden Size: 64, LR: 0.001, Batch Size: 64, Loss: 0.3372\n",
      "Epoch [10/50], Hidden Size: 64, LR: 0.001, Batch Size: 64, Loss: 0.3371\n",
      "Epoch [11/50], Hidden Size: 64, LR: 0.001, Batch Size: 64, Loss: 0.3374\n",
      "Epoch [12/50], Hidden Size: 64, LR: 0.001, Batch Size: 64, Loss: 0.3393\n",
      "Epoch [13/50], Hidden Size: 64, LR: 0.001, Batch Size: 64, Loss: 0.3280\n",
      "Epoch [14/50], Hidden Size: 64, LR: 0.001, Batch Size: 64, Loss: 0.3341\n",
      "Epoch [15/50], Hidden Size: 64, LR: 0.001, Batch Size: 64, Loss: 0.3371\n",
      "Epoch [16/50], Hidden Size: 64, LR: 0.001, Batch Size: 64, Loss: 0.3274\n",
      "Epoch [17/50], Hidden Size: 64, LR: 0.001, Batch Size: 64, Loss: 0.3319\n",
      "Epoch [18/50], Hidden Size: 64, LR: 0.001, Batch Size: 64, Loss: 0.3343\n",
      "Epoch [19/50], Hidden Size: 64, LR: 0.001, Batch Size: 64, Loss: 0.3343\n",
      "Epoch [20/50], Hidden Size: 64, LR: 0.001, Batch Size: 64, Loss: 0.3373\n",
      "Epoch [21/50], Hidden Size: 64, LR: 0.001, Batch Size: 64, Loss: 0.3307\n",
      "Epoch [22/50], Hidden Size: 64, LR: 0.001, Batch Size: 64, Loss: 0.3356\n",
      "Epoch [23/50], Hidden Size: 64, LR: 0.001, Batch Size: 64, Loss: 0.3364\n",
      "Epoch [24/50], Hidden Size: 64, LR: 0.001, Batch Size: 64, Loss: 0.3353\n",
      "Epoch [25/50], Hidden Size: 64, LR: 0.001, Batch Size: 64, Loss: 0.3393\n",
      "Epoch [26/50], Hidden Size: 64, LR: 0.001, Batch Size: 64, Loss: 0.3421\n",
      "Epoch [27/50], Hidden Size: 64, LR: 0.001, Batch Size: 64, Loss: 0.3317\n",
      "Epoch [28/50], Hidden Size: 64, LR: 0.001, Batch Size: 64, Loss: 0.3377\n",
      "Epoch [29/50], Hidden Size: 64, LR: 0.001, Batch Size: 64, Loss: 0.3365\n",
      "Epoch [30/50], Hidden Size: 64, LR: 0.001, Batch Size: 64, Loss: 0.3314\n",
      "Epoch [31/50], Hidden Size: 64, LR: 0.001, Batch Size: 64, Loss: 0.3426\n",
      "Epoch [32/50], Hidden Size: 64, LR: 0.001, Batch Size: 64, Loss: 0.3371\n",
      "Epoch [33/50], Hidden Size: 64, LR: 0.001, Batch Size: 64, Loss: 0.3385\n",
      "Epoch [34/50], Hidden Size: 64, LR: 0.001, Batch Size: 64, Loss: 0.3340\n",
      "Epoch [35/50], Hidden Size: 64, LR: 0.001, Batch Size: 64, Loss: 0.3360\n",
      "Epoch [36/50], Hidden Size: 64, LR: 0.001, Batch Size: 64, Loss: 0.3345\n",
      "Epoch [37/50], Hidden Size: 64, LR: 0.001, Batch Size: 64, Loss: 0.3432\n",
      "Epoch [38/50], Hidden Size: 64, LR: 0.001, Batch Size: 64, Loss: 0.3389\n",
      "Epoch [39/50], Hidden Size: 64, LR: 0.001, Batch Size: 64, Loss: 0.3313\n",
      "Epoch [40/50], Hidden Size: 64, LR: 0.001, Batch Size: 64, Loss: 0.3391\n",
      "Epoch [41/50], Hidden Size: 64, LR: 0.001, Batch Size: 64, Loss: 0.3344\n",
      "Epoch [42/50], Hidden Size: 64, LR: 0.001, Batch Size: 64, Loss: 0.3299\n",
      "Epoch [43/50], Hidden Size: 64, LR: 0.001, Batch Size: 64, Loss: 0.3291\n",
      "Epoch [44/50], Hidden Size: 64, LR: 0.001, Batch Size: 64, Loss: 0.3339\n",
      "Epoch [45/50], Hidden Size: 64, LR: 0.001, Batch Size: 64, Loss: 0.3356\n",
      "Epoch [46/50], Hidden Size: 64, LR: 0.001, Batch Size: 64, Loss: 0.3275\n",
      "Epoch [47/50], Hidden Size: 64, LR: 0.001, Batch Size: 64, Loss: 0.3325\n",
      "Epoch [48/50], Hidden Size: 64, LR: 0.001, Batch Size: 64, Loss: 0.3380\n",
      "Epoch [49/50], Hidden Size: 64, LR: 0.001, Batch Size: 64, Loss: 0.3347\n",
      "Epoch [50/50], Hidden Size: 64, LR: 0.001, Batch Size: 64, Loss: 0.3368\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\10374\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torchvision\\models\\_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "c:\\Users\\10374\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torchvision\\models\\_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=None`.\n",
      "  warnings.warn(msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/50], Hidden Size: 64, LR: 0.0001, Batch Size: 16, Loss: 0.9978\n",
      "Epoch [2/50], Hidden Size: 64, LR: 0.0001, Batch Size: 16, Loss: 0.9896\n",
      "Epoch [3/50], Hidden Size: 64, LR: 0.0001, Batch Size: 16, Loss: 0.9681\n",
      "Epoch [4/50], Hidden Size: 64, LR: 0.0001, Batch Size: 16, Loss: 0.9271\n",
      "Epoch [5/50], Hidden Size: 64, LR: 0.0001, Batch Size: 16, Loss: 0.8665\n",
      "Epoch [6/50], Hidden Size: 64, LR: 0.0001, Batch Size: 16, Loss: 0.7866\n",
      "Epoch [7/50], Hidden Size: 64, LR: 0.0001, Batch Size: 16, Loss: 0.6877\n",
      "Epoch [8/50], Hidden Size: 64, LR: 0.0001, Batch Size: 16, Loss: 0.5682\n",
      "Epoch [9/50], Hidden Size: 64, LR: 0.0001, Batch Size: 16, Loss: 0.4423\n",
      "Epoch [10/50], Hidden Size: 64, LR: 0.0001, Batch Size: 16, Loss: 0.3537\n",
      "Epoch [11/50], Hidden Size: 64, LR: 0.0001, Batch Size: 16, Loss: 0.3327\n",
      "Epoch [12/50], Hidden Size: 64, LR: 0.0001, Batch Size: 16, Loss: 0.3298\n",
      "Epoch [13/50], Hidden Size: 64, LR: 0.0001, Batch Size: 16, Loss: 0.3310\n",
      "Epoch [14/50], Hidden Size: 64, LR: 0.0001, Batch Size: 16, Loss: 0.3336\n",
      "Epoch [15/50], Hidden Size: 64, LR: 0.0001, Batch Size: 16, Loss: 0.3297\n",
      "Epoch [16/50], Hidden Size: 64, LR: 0.0001, Batch Size: 16, Loss: 0.3336\n",
      "Epoch [17/50], Hidden Size: 64, LR: 0.0001, Batch Size: 16, Loss: 0.3349\n",
      "Epoch [18/50], Hidden Size: 64, LR: 0.0001, Batch Size: 16, Loss: 0.3303\n",
      "Epoch [19/50], Hidden Size: 64, LR: 0.0001, Batch Size: 16, Loss: 0.3287\n",
      "Epoch [20/50], Hidden Size: 64, LR: 0.0001, Batch Size: 16, Loss: 0.3313\n",
      "Epoch [21/50], Hidden Size: 64, LR: 0.0001, Batch Size: 16, Loss: 0.3311\n",
      "Epoch [22/50], Hidden Size: 64, LR: 0.0001, Batch Size: 16, Loss: 0.3328\n",
      "Epoch [23/50], Hidden Size: 64, LR: 0.0001, Batch Size: 16, Loss: 0.3331\n",
      "Epoch [24/50], Hidden Size: 64, LR: 0.0001, Batch Size: 16, Loss: 0.3324\n",
      "Epoch [25/50], Hidden Size: 64, LR: 0.0001, Batch Size: 16, Loss: 0.3321\n",
      "Epoch [26/50], Hidden Size: 64, LR: 0.0001, Batch Size: 16, Loss: 0.3331\n",
      "Epoch [27/50], Hidden Size: 64, LR: 0.0001, Batch Size: 16, Loss: 0.3328\n",
      "Epoch [28/50], Hidden Size: 64, LR: 0.0001, Batch Size: 16, Loss: 0.3325\n",
      "Epoch [29/50], Hidden Size: 64, LR: 0.0001, Batch Size: 16, Loss: 0.3315\n",
      "Epoch [30/50], Hidden Size: 64, LR: 0.0001, Batch Size: 16, Loss: 0.3307\n",
      "Epoch [31/50], Hidden Size: 64, LR: 0.0001, Batch Size: 16, Loss: 0.3315\n",
      "Epoch [32/50], Hidden Size: 64, LR: 0.0001, Batch Size: 16, Loss: 0.3302\n",
      "Epoch [33/50], Hidden Size: 64, LR: 0.0001, Batch Size: 16, Loss: 0.3329\n",
      "Epoch [34/50], Hidden Size: 64, LR: 0.0001, Batch Size: 16, Loss: 0.3340\n",
      "Epoch [35/50], Hidden Size: 64, LR: 0.0001, Batch Size: 16, Loss: 0.3257\n",
      "Epoch [36/50], Hidden Size: 64, LR: 0.0001, Batch Size: 16, Loss: 0.3348\n",
      "Epoch [37/50], Hidden Size: 64, LR: 0.0001, Batch Size: 16, Loss: 0.3328\n",
      "Epoch [38/50], Hidden Size: 64, LR: 0.0001, Batch Size: 16, Loss: 0.3296\n",
      "Epoch [39/50], Hidden Size: 64, LR: 0.0001, Batch Size: 16, Loss: 0.3316\n",
      "Epoch [40/50], Hidden Size: 64, LR: 0.0001, Batch Size: 16, Loss: 0.3297\n",
      "Epoch [41/50], Hidden Size: 64, LR: 0.0001, Batch Size: 16, Loss: 0.3323\n",
      "Epoch [42/50], Hidden Size: 64, LR: 0.0001, Batch Size: 16, Loss: 0.3325\n",
      "Epoch [43/50], Hidden Size: 64, LR: 0.0001, Batch Size: 16, Loss: 0.3319\n",
      "Epoch [44/50], Hidden Size: 64, LR: 0.0001, Batch Size: 16, Loss: 0.3343\n",
      "Epoch [45/50], Hidden Size: 64, LR: 0.0001, Batch Size: 16, Loss: 0.3305\n",
      "Epoch [46/50], Hidden Size: 64, LR: 0.0001, Batch Size: 16, Loss: 0.3311\n",
      "Epoch [47/50], Hidden Size: 64, LR: 0.0001, Batch Size: 16, Loss: 0.3323\n",
      "Epoch [48/50], Hidden Size: 64, LR: 0.0001, Batch Size: 16, Loss: 0.3336\n",
      "Epoch [49/50], Hidden Size: 64, LR: 0.0001, Batch Size: 16, Loss: 0.3300\n",
      "Epoch [50/50], Hidden Size: 64, LR: 0.0001, Batch Size: 16, Loss: 0.3343\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\10374\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torchvision\\models\\_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "c:\\Users\\10374\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torchvision\\models\\_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=None`.\n",
      "  warnings.warn(msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/50], Hidden Size: 64, LR: 0.0001, Batch Size: 32, Loss: 0.9992\n",
      "Epoch [2/50], Hidden Size: 64, LR: 0.0001, Batch Size: 32, Loss: 0.9967\n",
      "Epoch [3/50], Hidden Size: 64, LR: 0.0001, Batch Size: 32, Loss: 0.9928\n",
      "Epoch [4/50], Hidden Size: 64, LR: 0.0001, Batch Size: 32, Loss: 0.9862\n",
      "Epoch [5/50], Hidden Size: 64, LR: 0.0001, Batch Size: 32, Loss: 0.9756\n",
      "Epoch [6/50], Hidden Size: 64, LR: 0.0001, Batch Size: 32, Loss: 0.9605\n",
      "Epoch [7/50], Hidden Size: 64, LR: 0.0001, Batch Size: 32, Loss: 0.9402\n",
      "Epoch [8/50], Hidden Size: 64, LR: 0.0001, Batch Size: 32, Loss: 0.9162\n",
      "Epoch [9/50], Hidden Size: 64, LR: 0.0001, Batch Size: 32, Loss: 0.8876\n",
      "Epoch [10/50], Hidden Size: 64, LR: 0.0001, Batch Size: 32, Loss: 0.8526\n",
      "Epoch [11/50], Hidden Size: 64, LR: 0.0001, Batch Size: 32, Loss: 0.8131\n",
      "Epoch [12/50], Hidden Size: 64, LR: 0.0001, Batch Size: 32, Loss: 0.7685\n",
      "Epoch [13/50], Hidden Size: 64, LR: 0.0001, Batch Size: 32, Loss: 0.7175\n",
      "Epoch [14/50], Hidden Size: 64, LR: 0.0001, Batch Size: 32, Loss: 0.6610\n",
      "Epoch [15/50], Hidden Size: 64, LR: 0.0001, Batch Size: 32, Loss: 0.5992\n",
      "Epoch [16/50], Hidden Size: 64, LR: 0.0001, Batch Size: 32, Loss: 0.5352\n",
      "Epoch [17/50], Hidden Size: 64, LR: 0.0001, Batch Size: 32, Loss: 0.4710\n",
      "Epoch [18/50], Hidden Size: 64, LR: 0.0001, Batch Size: 32, Loss: 0.4102\n",
      "Epoch [19/50], Hidden Size: 64, LR: 0.0001, Batch Size: 32, Loss: 0.3676\n",
      "Epoch [20/50], Hidden Size: 64, LR: 0.0001, Batch Size: 32, Loss: 0.3433\n",
      "Epoch [21/50], Hidden Size: 64, LR: 0.0001, Batch Size: 32, Loss: 0.3328\n",
      "Epoch [22/50], Hidden Size: 64, LR: 0.0001, Batch Size: 32, Loss: 0.3336\n",
      "Epoch [23/50], Hidden Size: 64, LR: 0.0001, Batch Size: 32, Loss: 0.3351\n",
      "Epoch [24/50], Hidden Size: 64, LR: 0.0001, Batch Size: 32, Loss: 0.3331\n",
      "Epoch [25/50], Hidden Size: 64, LR: 0.0001, Batch Size: 32, Loss: 0.3344\n",
      "Epoch [26/50], Hidden Size: 64, LR: 0.0001, Batch Size: 32, Loss: 0.3331\n",
      "Epoch [27/50], Hidden Size: 64, LR: 0.0001, Batch Size: 32, Loss: 0.3327\n",
      "Epoch [28/50], Hidden Size: 64, LR: 0.0001, Batch Size: 32, Loss: 0.3339\n",
      "Epoch [29/50], Hidden Size: 64, LR: 0.0001, Batch Size: 32, Loss: 0.3323\n",
      "Epoch [30/50], Hidden Size: 64, LR: 0.0001, Batch Size: 32, Loss: 0.3347\n",
      "Epoch [31/50], Hidden Size: 64, LR: 0.0001, Batch Size: 32, Loss: 0.3332\n",
      "Epoch [32/50], Hidden Size: 64, LR: 0.0001, Batch Size: 32, Loss: 0.3309\n",
      "Epoch [33/50], Hidden Size: 64, LR: 0.0001, Batch Size: 32, Loss: 0.3331\n",
      "Epoch [34/50], Hidden Size: 64, LR: 0.0001, Batch Size: 32, Loss: 0.3339\n",
      "Epoch [35/50], Hidden Size: 64, LR: 0.0001, Batch Size: 32, Loss: 0.3323\n",
      "Epoch [36/50], Hidden Size: 64, LR: 0.0001, Batch Size: 32, Loss: 0.3344\n",
      "Epoch [37/50], Hidden Size: 64, LR: 0.0001, Batch Size: 32, Loss: 0.3327\n",
      "Epoch [38/50], Hidden Size: 64, LR: 0.0001, Batch Size: 32, Loss: 0.3333\n",
      "Epoch [39/50], Hidden Size: 64, LR: 0.0001, Batch Size: 32, Loss: 0.3337\n",
      "Epoch [40/50], Hidden Size: 64, LR: 0.0001, Batch Size: 32, Loss: 0.3340\n",
      "Epoch [41/50], Hidden Size: 64, LR: 0.0001, Batch Size: 32, Loss: 0.3339\n",
      "Epoch [42/50], Hidden Size: 64, LR: 0.0001, Batch Size: 32, Loss: 0.3326\n",
      "Epoch [43/50], Hidden Size: 64, LR: 0.0001, Batch Size: 32, Loss: 0.3331\n",
      "Epoch [44/50], Hidden Size: 64, LR: 0.0001, Batch Size: 32, Loss: 0.3322\n",
      "Epoch [45/50], Hidden Size: 64, LR: 0.0001, Batch Size: 32, Loss: 0.3344\n",
      "Epoch [46/50], Hidden Size: 64, LR: 0.0001, Batch Size: 32, Loss: 0.3335\n",
      "Epoch [47/50], Hidden Size: 64, LR: 0.0001, Batch Size: 32, Loss: 0.3321\n",
      "Epoch [48/50], Hidden Size: 64, LR: 0.0001, Batch Size: 32, Loss: 0.3330\n",
      "Epoch [49/50], Hidden Size: 64, LR: 0.0001, Batch Size: 32, Loss: 0.3330\n",
      "Epoch [50/50], Hidden Size: 64, LR: 0.0001, Batch Size: 32, Loss: 0.3340\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\10374\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torchvision\\models\\_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "c:\\Users\\10374\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torchvision\\models\\_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=None`.\n",
      "  warnings.warn(msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/50], Hidden Size: 64, LR: 0.0001, Batch Size: 64, Loss: 0.9996\n",
      "Epoch [2/50], Hidden Size: 64, LR: 0.0001, Batch Size: 64, Loss: 0.9980\n",
      "Epoch [3/50], Hidden Size: 64, LR: 0.0001, Batch Size: 64, Loss: 0.9958\n",
      "Epoch [4/50], Hidden Size: 64, LR: 0.0001, Batch Size: 64, Loss: 0.9932\n",
      "Epoch [5/50], Hidden Size: 64, LR: 0.0001, Batch Size: 64, Loss: 0.9898\n",
      "Epoch [6/50], Hidden Size: 64, LR: 0.0001, Batch Size: 64, Loss: 0.9851\n",
      "Epoch [7/50], Hidden Size: 64, LR: 0.0001, Batch Size: 64, Loss: 0.9785\n",
      "Epoch [8/50], Hidden Size: 64, LR: 0.0001, Batch Size: 64, Loss: 0.9711\n",
      "Epoch [9/50], Hidden Size: 64, LR: 0.0001, Batch Size: 64, Loss: 0.9623\n",
      "Epoch [10/50], Hidden Size: 64, LR: 0.0001, Batch Size: 64, Loss: 0.9512\n",
      "Epoch [11/50], Hidden Size: 64, LR: 0.0001, Batch Size: 64, Loss: 0.9400\n",
      "Epoch [12/50], Hidden Size: 64, LR: 0.0001, Batch Size: 64, Loss: 0.9280\n",
      "Epoch [13/50], Hidden Size: 64, LR: 0.0001, Batch Size: 64, Loss: 0.9106\n",
      "Epoch [14/50], Hidden Size: 64, LR: 0.0001, Batch Size: 64, Loss: 0.8961\n",
      "Epoch [15/50], Hidden Size: 64, LR: 0.0001, Batch Size: 64, Loss: 0.8781\n",
      "Epoch [16/50], Hidden Size: 64, LR: 0.0001, Batch Size: 64, Loss: 0.8566\n",
      "Epoch [17/50], Hidden Size: 64, LR: 0.0001, Batch Size: 64, Loss: 0.8349\n",
      "Epoch [18/50], Hidden Size: 64, LR: 0.0001, Batch Size: 64, Loss: 0.8127\n",
      "Epoch [19/50], Hidden Size: 64, LR: 0.0001, Batch Size: 64, Loss: 0.7849\n",
      "Epoch [20/50], Hidden Size: 64, LR: 0.0001, Batch Size: 64, Loss: 0.7577\n",
      "Epoch [21/50], Hidden Size: 64, LR: 0.0001, Batch Size: 64, Loss: 0.7320\n",
      "Epoch [22/50], Hidden Size: 64, LR: 0.0001, Batch Size: 64, Loss: 0.7000\n",
      "Epoch [23/50], Hidden Size: 64, LR: 0.0001, Batch Size: 64, Loss: 0.6645\n",
      "Epoch [24/50], Hidden Size: 64, LR: 0.0001, Batch Size: 64, Loss: 0.6247\n",
      "Epoch [25/50], Hidden Size: 64, LR: 0.0001, Batch Size: 64, Loss: 0.5955\n",
      "Epoch [26/50], Hidden Size: 64, LR: 0.0001, Batch Size: 64, Loss: 0.5590\n",
      "Epoch [27/50], Hidden Size: 64, LR: 0.0001, Batch Size: 64, Loss: 0.5207\n",
      "Epoch [28/50], Hidden Size: 64, LR: 0.0001, Batch Size: 64, Loss: 0.4813\n",
      "Epoch [29/50], Hidden Size: 64, LR: 0.0001, Batch Size: 64, Loss: 0.4499\n",
      "Epoch [30/50], Hidden Size: 64, LR: 0.0001, Batch Size: 64, Loss: 0.4116\n",
      "Epoch [31/50], Hidden Size: 64, LR: 0.0001, Batch Size: 64, Loss: 0.3807\n",
      "Epoch [32/50], Hidden Size: 64, LR: 0.0001, Batch Size: 64, Loss: 0.3672\n",
      "Epoch [33/50], Hidden Size: 64, LR: 0.0001, Batch Size: 64, Loss: 0.3517\n",
      "Epoch [34/50], Hidden Size: 64, LR: 0.0001, Batch Size: 64, Loss: 0.3392\n",
      "Epoch [35/50], Hidden Size: 64, LR: 0.0001, Batch Size: 64, Loss: 0.3294\n",
      "Epoch [36/50], Hidden Size: 64, LR: 0.0001, Batch Size: 64, Loss: 0.3333\n",
      "Epoch [37/50], Hidden Size: 64, LR: 0.0001, Batch Size: 64, Loss: 0.3350\n",
      "Epoch [38/50], Hidden Size: 64, LR: 0.0001, Batch Size: 64, Loss: 0.3315\n",
      "Epoch [39/50], Hidden Size: 64, LR: 0.0001, Batch Size: 64, Loss: 0.3314\n",
      "Epoch [40/50], Hidden Size: 64, LR: 0.0001, Batch Size: 64, Loss: 0.3359\n",
      "Epoch [41/50], Hidden Size: 64, LR: 0.0001, Batch Size: 64, Loss: 0.3345\n",
      "Epoch [42/50], Hidden Size: 64, LR: 0.0001, Batch Size: 64, Loss: 0.3351\n",
      "Epoch [43/50], Hidden Size: 64, LR: 0.0001, Batch Size: 64, Loss: 0.3293\n",
      "Epoch [44/50], Hidden Size: 64, LR: 0.0001, Batch Size: 64, Loss: 0.3314\n",
      "Epoch [45/50], Hidden Size: 64, LR: 0.0001, Batch Size: 64, Loss: 0.3234\n",
      "Epoch [46/50], Hidden Size: 64, LR: 0.0001, Batch Size: 64, Loss: 0.3234\n",
      "Epoch [47/50], Hidden Size: 64, LR: 0.0001, Batch Size: 64, Loss: 0.3283\n",
      "Epoch [48/50], Hidden Size: 64, LR: 0.0001, Batch Size: 64, Loss: 0.3432\n",
      "Epoch [49/50], Hidden Size: 64, LR: 0.0001, Batch Size: 64, Loss: 0.3257\n",
      "Epoch [50/50], Hidden Size: 64, LR: 0.0001, Batch Size: 64, Loss: 0.3286\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\10374\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torchvision\\models\\_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "c:\\Users\\10374\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torchvision\\models\\_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=None`.\n",
      "  warnings.warn(msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/50], Hidden Size: 128, LR: 0.001, Batch Size: 16, Loss: 0.9020\n",
      "Epoch [2/50], Hidden Size: 128, LR: 0.001, Batch Size: 16, Loss: 0.4277\n",
      "Epoch [3/50], Hidden Size: 128, LR: 0.001, Batch Size: 16, Loss: 0.3361\n",
      "Epoch [4/50], Hidden Size: 128, LR: 0.001, Batch Size: 16, Loss: 0.3342\n",
      "Epoch [5/50], Hidden Size: 128, LR: 0.001, Batch Size: 16, Loss: 0.3357\n",
      "Epoch [6/50], Hidden Size: 128, LR: 0.001, Batch Size: 16, Loss: 0.3313\n",
      "Epoch [7/50], Hidden Size: 128, LR: 0.001, Batch Size: 16, Loss: 0.3368\n",
      "Epoch [8/50], Hidden Size: 128, LR: 0.001, Batch Size: 16, Loss: 0.3339\n",
      "Epoch [9/50], Hidden Size: 128, LR: 0.001, Batch Size: 16, Loss: 0.3381\n",
      "Epoch [10/50], Hidden Size: 128, LR: 0.001, Batch Size: 16, Loss: 0.3337\n",
      "Epoch [11/50], Hidden Size: 128, LR: 0.001, Batch Size: 16, Loss: 0.3332\n",
      "Epoch [12/50], Hidden Size: 128, LR: 0.001, Batch Size: 16, Loss: 0.3312\n",
      "Epoch [13/50], Hidden Size: 128, LR: 0.001, Batch Size: 16, Loss: 0.3345\n",
      "Epoch [14/50], Hidden Size: 128, LR: 0.001, Batch Size: 16, Loss: 0.3362\n",
      "Epoch [15/50], Hidden Size: 128, LR: 0.001, Batch Size: 16, Loss: 0.3309\n",
      "Epoch [16/50], Hidden Size: 128, LR: 0.001, Batch Size: 16, Loss: 0.3346\n",
      "Epoch [17/50], Hidden Size: 128, LR: 0.001, Batch Size: 16, Loss: 0.3374\n",
      "Epoch [18/50], Hidden Size: 128, LR: 0.001, Batch Size: 16, Loss: 0.3473\n",
      "Epoch [19/50], Hidden Size: 128, LR: 0.001, Batch Size: 16, Loss: 0.3323\n",
      "Epoch [20/50], Hidden Size: 128, LR: 0.001, Batch Size: 16, Loss: 0.3331\n",
      "Epoch [21/50], Hidden Size: 128, LR: 0.001, Batch Size: 16, Loss: 0.3311\n",
      "Epoch [22/50], Hidden Size: 128, LR: 0.001, Batch Size: 16, Loss: 0.3398\n",
      "Epoch [23/50], Hidden Size: 128, LR: 0.001, Batch Size: 16, Loss: 0.3355\n",
      "Epoch [24/50], Hidden Size: 128, LR: 0.001, Batch Size: 16, Loss: 0.3346\n",
      "Epoch [25/50], Hidden Size: 128, LR: 0.001, Batch Size: 16, Loss: 0.3353\n",
      "Epoch [26/50], Hidden Size: 128, LR: 0.001, Batch Size: 16, Loss: 0.3360\n",
      "Epoch [27/50], Hidden Size: 128, LR: 0.001, Batch Size: 16, Loss: 0.3403\n",
      "Epoch [28/50], Hidden Size: 128, LR: 0.001, Batch Size: 16, Loss: 0.3331\n",
      "Epoch [29/50], Hidden Size: 128, LR: 0.001, Batch Size: 16, Loss: 0.3338\n",
      "Epoch [30/50], Hidden Size: 128, LR: 0.001, Batch Size: 16, Loss: 0.3342\n",
      "Epoch [31/50], Hidden Size: 128, LR: 0.001, Batch Size: 16, Loss: 0.3339\n",
      "Epoch [32/50], Hidden Size: 128, LR: 0.001, Batch Size: 16, Loss: 0.3389\n",
      "Epoch [33/50], Hidden Size: 128, LR: 0.001, Batch Size: 16, Loss: 0.3407\n",
      "Epoch [34/50], Hidden Size: 128, LR: 0.001, Batch Size: 16, Loss: 0.3351\n",
      "Epoch [35/50], Hidden Size: 128, LR: 0.001, Batch Size: 16, Loss: 0.3317\n",
      "Epoch [36/50], Hidden Size: 128, LR: 0.001, Batch Size: 16, Loss: 0.3334\n",
      "Epoch [37/50], Hidden Size: 128, LR: 0.001, Batch Size: 16, Loss: 0.3336\n",
      "Epoch [38/50], Hidden Size: 128, LR: 0.001, Batch Size: 16, Loss: 0.3350\n",
      "Epoch [39/50], Hidden Size: 128, LR: 0.001, Batch Size: 16, Loss: 0.3407\n",
      "Epoch [40/50], Hidden Size: 128, LR: 0.001, Batch Size: 16, Loss: 0.3400\n",
      "Epoch [41/50], Hidden Size: 128, LR: 0.001, Batch Size: 16, Loss: 0.3339\n",
      "Epoch [42/50], Hidden Size: 128, LR: 0.001, Batch Size: 16, Loss: 0.3351\n",
      "Epoch [43/50], Hidden Size: 128, LR: 0.001, Batch Size: 16, Loss: 0.3328\n",
      "Epoch [44/50], Hidden Size: 128, LR: 0.001, Batch Size: 16, Loss: 0.3332\n",
      "Epoch [45/50], Hidden Size: 128, LR: 0.001, Batch Size: 16, Loss: 0.3346\n",
      "Epoch [46/50], Hidden Size: 128, LR: 0.001, Batch Size: 16, Loss: 0.3333\n",
      "Epoch [47/50], Hidden Size: 128, LR: 0.001, Batch Size: 16, Loss: 0.3342\n",
      "Epoch [48/50], Hidden Size: 128, LR: 0.001, Batch Size: 16, Loss: 0.3329\n",
      "Epoch [49/50], Hidden Size: 128, LR: 0.001, Batch Size: 16, Loss: 0.3407\n",
      "Epoch [50/50], Hidden Size: 128, LR: 0.001, Batch Size: 16, Loss: 0.3348\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\10374\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torchvision\\models\\_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "c:\\Users\\10374\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torchvision\\models\\_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=None`.\n",
      "  warnings.warn(msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/50], Hidden Size: 128, LR: 0.001, Batch Size: 32, Loss: 0.9773\n",
      "Epoch [2/50], Hidden Size: 128, LR: 0.001, Batch Size: 32, Loss: 0.8262\n",
      "Epoch [3/50], Hidden Size: 128, LR: 0.001, Batch Size: 32, Loss: 0.4931\n",
      "Epoch [4/50], Hidden Size: 128, LR: 0.001, Batch Size: 32, Loss: 0.3642\n",
      "Epoch [5/50], Hidden Size: 128, LR: 0.001, Batch Size: 32, Loss: 0.3356\n",
      "Epoch [6/50], Hidden Size: 128, LR: 0.001, Batch Size: 32, Loss: 0.3353\n",
      "Epoch [7/50], Hidden Size: 128, LR: 0.001, Batch Size: 32, Loss: 0.3344\n",
      "Epoch [8/50], Hidden Size: 128, LR: 0.001, Batch Size: 32, Loss: 0.3327\n",
      "Epoch [9/50], Hidden Size: 128, LR: 0.001, Batch Size: 32, Loss: 0.3354\n",
      "Epoch [10/50], Hidden Size: 128, LR: 0.001, Batch Size: 32, Loss: 0.3326\n",
      "Epoch [11/50], Hidden Size: 128, LR: 0.001, Batch Size: 32, Loss: 0.3349\n",
      "Epoch [12/50], Hidden Size: 128, LR: 0.001, Batch Size: 32, Loss: 0.3347\n",
      "Epoch [13/50], Hidden Size: 128, LR: 0.001, Batch Size: 32, Loss: 0.3363\n",
      "Epoch [14/50], Hidden Size: 128, LR: 0.001, Batch Size: 32, Loss: 0.3354\n",
      "Epoch [15/50], Hidden Size: 128, LR: 0.001, Batch Size: 32, Loss: 0.3346\n",
      "Epoch [16/50], Hidden Size: 128, LR: 0.001, Batch Size: 32, Loss: 0.3339\n",
      "Epoch [17/50], Hidden Size: 128, LR: 0.001, Batch Size: 32, Loss: 0.3345\n",
      "Epoch [18/50], Hidden Size: 128, LR: 0.001, Batch Size: 32, Loss: 0.3351\n",
      "Epoch [19/50], Hidden Size: 128, LR: 0.001, Batch Size: 32, Loss: 0.3341\n",
      "Epoch [20/50], Hidden Size: 128, LR: 0.001, Batch Size: 32, Loss: 0.3314\n",
      "Epoch [21/50], Hidden Size: 128, LR: 0.001, Batch Size: 32, Loss: 0.3343\n",
      "Epoch [22/50], Hidden Size: 128, LR: 0.001, Batch Size: 32, Loss: 0.3344\n",
      "Epoch [23/50], Hidden Size: 128, LR: 0.001, Batch Size: 32, Loss: 0.3346\n",
      "Epoch [24/50], Hidden Size: 128, LR: 0.001, Batch Size: 32, Loss: 0.3348\n",
      "Epoch [25/50], Hidden Size: 128, LR: 0.001, Batch Size: 32, Loss: 0.3319\n",
      "Epoch [26/50], Hidden Size: 128, LR: 0.001, Batch Size: 32, Loss: 0.3344\n",
      "Epoch [27/50], Hidden Size: 128, LR: 0.001, Batch Size: 32, Loss: 0.3326\n",
      "Epoch [28/50], Hidden Size: 128, LR: 0.001, Batch Size: 32, Loss: 0.3352\n",
      "Epoch [29/50], Hidden Size: 128, LR: 0.001, Batch Size: 32, Loss: 0.3357\n",
      "Epoch [30/50], Hidden Size: 128, LR: 0.001, Batch Size: 32, Loss: 0.3378\n",
      "Epoch [31/50], Hidden Size: 128, LR: 0.001, Batch Size: 32, Loss: 0.3326\n",
      "Epoch [32/50], Hidden Size: 128, LR: 0.001, Batch Size: 32, Loss: 0.3374\n",
      "Epoch [33/50], Hidden Size: 128, LR: 0.001, Batch Size: 32, Loss: 0.3359\n",
      "Epoch [34/50], Hidden Size: 128, LR: 0.001, Batch Size: 32, Loss: 0.3356\n",
      "Epoch [35/50], Hidden Size: 128, LR: 0.001, Batch Size: 32, Loss: 0.3348\n",
      "Epoch [36/50], Hidden Size: 128, LR: 0.001, Batch Size: 32, Loss: 0.3342\n",
      "Epoch [37/50], Hidden Size: 128, LR: 0.001, Batch Size: 32, Loss: 0.3348\n",
      "Epoch [38/50], Hidden Size: 128, LR: 0.001, Batch Size: 32, Loss: 0.3346\n",
      "Epoch [39/50], Hidden Size: 128, LR: 0.001, Batch Size: 32, Loss: 0.3344\n",
      "Epoch [40/50], Hidden Size: 128, LR: 0.001, Batch Size: 32, Loss: 0.3350\n",
      "Epoch [41/50], Hidden Size: 128, LR: 0.001, Batch Size: 32, Loss: 0.3353\n",
      "Epoch [42/50], Hidden Size: 128, LR: 0.001, Batch Size: 32, Loss: 0.3368\n",
      "Epoch [43/50], Hidden Size: 128, LR: 0.001, Batch Size: 32, Loss: 0.3354\n",
      "Epoch [44/50], Hidden Size: 128, LR: 0.001, Batch Size: 32, Loss: 0.3382\n",
      "Epoch [45/50], Hidden Size: 128, LR: 0.001, Batch Size: 32, Loss: 0.3355\n",
      "Epoch [46/50], Hidden Size: 128, LR: 0.001, Batch Size: 32, Loss: 0.3353\n",
      "Epoch [47/50], Hidden Size: 128, LR: 0.001, Batch Size: 32, Loss: 0.3355\n",
      "Epoch [48/50], Hidden Size: 128, LR: 0.001, Batch Size: 32, Loss: 0.3344\n",
      "Epoch [49/50], Hidden Size: 128, LR: 0.001, Batch Size: 32, Loss: 0.3344\n",
      "Epoch [50/50], Hidden Size: 128, LR: 0.001, Batch Size: 32, Loss: 0.3345\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\10374\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torchvision\\models\\_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "c:\\Users\\10374\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torchvision\\models\\_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=None`.\n",
      "  warnings.warn(msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/50], Hidden Size: 128, LR: 0.001, Batch Size: 64, Loss: 0.9936\n",
      "Epoch [2/50], Hidden Size: 128, LR: 0.001, Batch Size: 64, Loss: 0.9478\n",
      "Epoch [3/50], Hidden Size: 128, LR: 0.001, Batch Size: 64, Loss: 0.8378\n",
      "Epoch [4/50], Hidden Size: 128, LR: 0.001, Batch Size: 64, Loss: 0.6622\n",
      "Epoch [5/50], Hidden Size: 128, LR: 0.001, Batch Size: 64, Loss: 0.4256\n",
      "Epoch [6/50], Hidden Size: 128, LR: 0.001, Batch Size: 64, Loss: 0.3641\n",
      "Epoch [7/50], Hidden Size: 128, LR: 0.001, Batch Size: 64, Loss: 0.3702\n",
      "Epoch [8/50], Hidden Size: 128, LR: 0.001, Batch Size: 64, Loss: 0.3351\n",
      "Epoch [9/50], Hidden Size: 128, LR: 0.001, Batch Size: 64, Loss: 0.3292\n",
      "Epoch [10/50], Hidden Size: 128, LR: 0.001, Batch Size: 64, Loss: 0.3294\n",
      "Epoch [11/50], Hidden Size: 128, LR: 0.001, Batch Size: 64, Loss: 0.3384\n",
      "Epoch [12/50], Hidden Size: 128, LR: 0.001, Batch Size: 64, Loss: 0.3389\n",
      "Epoch [13/50], Hidden Size: 128, LR: 0.001, Batch Size: 64, Loss: 0.3397\n",
      "Epoch [14/50], Hidden Size: 128, LR: 0.001, Batch Size: 64, Loss: 0.3291\n",
      "Epoch [15/50], Hidden Size: 128, LR: 0.001, Batch Size: 64, Loss: 0.3260\n",
      "Epoch [16/50], Hidden Size: 128, LR: 0.001, Batch Size: 64, Loss: 0.3459\n",
      "Epoch [17/50], Hidden Size: 128, LR: 0.001, Batch Size: 64, Loss: 0.3365\n",
      "Epoch [18/50], Hidden Size: 128, LR: 0.001, Batch Size: 64, Loss: 0.3346\n",
      "Epoch [19/50], Hidden Size: 128, LR: 0.001, Batch Size: 64, Loss: 0.3297\n",
      "Epoch [20/50], Hidden Size: 128, LR: 0.001, Batch Size: 64, Loss: 0.3372\n",
      "Epoch [21/50], Hidden Size: 128, LR: 0.001, Batch Size: 64, Loss: 0.3314\n",
      "Epoch [22/50], Hidden Size: 128, LR: 0.001, Batch Size: 64, Loss: 0.3240\n",
      "Epoch [23/50], Hidden Size: 128, LR: 0.001, Batch Size: 64, Loss: 0.3376\n",
      "Epoch [24/50], Hidden Size: 128, LR: 0.001, Batch Size: 64, Loss: 0.3357\n",
      "Epoch [25/50], Hidden Size: 128, LR: 0.001, Batch Size: 64, Loss: 0.3322\n",
      "Epoch [26/50], Hidden Size: 128, LR: 0.001, Batch Size: 64, Loss: 0.3352\n",
      "Epoch [27/50], Hidden Size: 128, LR: 0.001, Batch Size: 64, Loss: 0.3333\n",
      "Epoch [28/50], Hidden Size: 128, LR: 0.001, Batch Size: 64, Loss: 0.3372\n",
      "Epoch [29/50], Hidden Size: 128, LR: 0.001, Batch Size: 64, Loss: 0.3317\n",
      "Epoch [30/50], Hidden Size: 128, LR: 0.001, Batch Size: 64, Loss: 0.3393\n",
      "Epoch [31/50], Hidden Size: 128, LR: 0.001, Batch Size: 64, Loss: 0.3386\n",
      "Epoch [32/50], Hidden Size: 128, LR: 0.001, Batch Size: 64, Loss: 0.3397\n",
      "Epoch [33/50], Hidden Size: 128, LR: 0.001, Batch Size: 64, Loss: 0.3334\n",
      "Epoch [34/50], Hidden Size: 128, LR: 0.001, Batch Size: 64, Loss: 0.3403\n",
      "Epoch [35/50], Hidden Size: 128, LR: 0.001, Batch Size: 64, Loss: 0.3363\n",
      "Epoch [36/50], Hidden Size: 128, LR: 0.001, Batch Size: 64, Loss: 0.3363\n",
      "Epoch [37/50], Hidden Size: 128, LR: 0.001, Batch Size: 64, Loss: 0.3352\n",
      "Epoch [38/50], Hidden Size: 128, LR: 0.001, Batch Size: 64, Loss: 0.3371\n",
      "Epoch [39/50], Hidden Size: 128, LR: 0.001, Batch Size: 64, Loss: 0.3306\n",
      "Epoch [40/50], Hidden Size: 128, LR: 0.001, Batch Size: 64, Loss: 0.3371\n",
      "Epoch [41/50], Hidden Size: 128, LR: 0.001, Batch Size: 64, Loss: 0.3336\n",
      "Epoch [42/50], Hidden Size: 128, LR: 0.001, Batch Size: 64, Loss: 0.3432\n",
      "Epoch [43/50], Hidden Size: 128, LR: 0.001, Batch Size: 64, Loss: 0.3259\n",
      "Epoch [44/50], Hidden Size: 128, LR: 0.001, Batch Size: 64, Loss: 0.3282\n",
      "Epoch [45/50], Hidden Size: 128, LR: 0.001, Batch Size: 64, Loss: 0.3354\n",
      "Epoch [46/50], Hidden Size: 128, LR: 0.001, Batch Size: 64, Loss: 0.3344\n",
      "Epoch [47/50], Hidden Size: 128, LR: 0.001, Batch Size: 64, Loss: 0.3339\n",
      "Epoch [48/50], Hidden Size: 128, LR: 0.001, Batch Size: 64, Loss: 0.3426\n",
      "Epoch [49/50], Hidden Size: 128, LR: 0.001, Batch Size: 64, Loss: 0.3326\n",
      "Epoch [50/50], Hidden Size: 128, LR: 0.001, Batch Size: 64, Loss: 0.3326\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\10374\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torchvision\\models\\_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "c:\\Users\\10374\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torchvision\\models\\_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=None`.\n",
      "  warnings.warn(msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/50], Hidden Size: 128, LR: 0.0001, Batch Size: 16, Loss: 0.9967\n",
      "Epoch [2/50], Hidden Size: 128, LR: 0.0001, Batch Size: 16, Loss: 0.9855\n",
      "Epoch [3/50], Hidden Size: 128, LR: 0.0001, Batch Size: 16, Loss: 0.9581\n",
      "Epoch [4/50], Hidden Size: 128, LR: 0.0001, Batch Size: 16, Loss: 0.9084\n",
      "Epoch [5/50], Hidden Size: 128, LR: 0.0001, Batch Size: 16, Loss: 0.8368\n",
      "Epoch [6/50], Hidden Size: 128, LR: 0.0001, Batch Size: 16, Loss: 0.7393\n",
      "Epoch [7/50], Hidden Size: 128, LR: 0.0001, Batch Size: 16, Loss: 0.6215\n",
      "Epoch [8/50], Hidden Size: 128, LR: 0.0001, Batch Size: 16, Loss: 0.4811\n",
      "Epoch [9/50], Hidden Size: 128, LR: 0.0001, Batch Size: 16, Loss: 0.3732\n",
      "Epoch [10/50], Hidden Size: 128, LR: 0.0001, Batch Size: 16, Loss: 0.3346\n",
      "Epoch [11/50], Hidden Size: 128, LR: 0.0001, Batch Size: 16, Loss: 0.3321\n",
      "Epoch [12/50], Hidden Size: 128, LR: 0.0001, Batch Size: 16, Loss: 0.3325\n",
      "Epoch [13/50], Hidden Size: 128, LR: 0.0001, Batch Size: 16, Loss: 0.3355\n",
      "Epoch [14/50], Hidden Size: 128, LR: 0.0001, Batch Size: 16, Loss: 0.3304\n",
      "Epoch [15/50], Hidden Size: 128, LR: 0.0001, Batch Size: 16, Loss: 0.3334\n",
      "Epoch [16/50], Hidden Size: 128, LR: 0.0001, Batch Size: 16, Loss: 0.3305\n",
      "Epoch [17/50], Hidden Size: 128, LR: 0.0001, Batch Size: 16, Loss: 0.3318\n",
      "Epoch [18/50], Hidden Size: 128, LR: 0.0001, Batch Size: 16, Loss: 0.3292\n",
      "Epoch [19/50], Hidden Size: 128, LR: 0.0001, Batch Size: 16, Loss: 0.3303\n",
      "Epoch [20/50], Hidden Size: 128, LR: 0.0001, Batch Size: 16, Loss: 0.3335\n",
      "Epoch [21/50], Hidden Size: 128, LR: 0.0001, Batch Size: 16, Loss: 0.3307\n",
      "Epoch [22/50], Hidden Size: 128, LR: 0.0001, Batch Size: 16, Loss: 0.3335\n",
      "Epoch [23/50], Hidden Size: 128, LR: 0.0001, Batch Size: 16, Loss: 0.3317\n",
      "Epoch [24/50], Hidden Size: 128, LR: 0.0001, Batch Size: 16, Loss: 0.3317\n",
      "Epoch [25/50], Hidden Size: 128, LR: 0.0001, Batch Size: 16, Loss: 0.3333\n",
      "Epoch [26/50], Hidden Size: 128, LR: 0.0001, Batch Size: 16, Loss: 0.3307\n",
      "Epoch [27/50], Hidden Size: 128, LR: 0.0001, Batch Size: 16, Loss: 0.3314\n",
      "Epoch [28/50], Hidden Size: 128, LR: 0.0001, Batch Size: 16, Loss: 0.3295\n",
      "Epoch [29/50], Hidden Size: 128, LR: 0.0001, Batch Size: 16, Loss: 0.3333\n",
      "Epoch [30/50], Hidden Size: 128, LR: 0.0001, Batch Size: 16, Loss: 0.3302\n",
      "Epoch [31/50], Hidden Size: 128, LR: 0.0001, Batch Size: 16, Loss: 0.3324\n",
      "Epoch [32/50], Hidden Size: 128, LR: 0.0001, Batch Size: 16, Loss: 0.3302\n",
      "Epoch [33/50], Hidden Size: 128, LR: 0.0001, Batch Size: 16, Loss: 0.3344\n",
      "Epoch [34/50], Hidden Size: 128, LR: 0.0001, Batch Size: 16, Loss: 0.3316\n",
      "Epoch [35/50], Hidden Size: 128, LR: 0.0001, Batch Size: 16, Loss: 0.3312\n",
      "Epoch [36/50], Hidden Size: 128, LR: 0.0001, Batch Size: 16, Loss: 0.3306\n",
      "Epoch [37/50], Hidden Size: 128, LR: 0.0001, Batch Size: 16, Loss: 0.3333\n",
      "Epoch [38/50], Hidden Size: 128, LR: 0.0001, Batch Size: 16, Loss: 0.3334\n",
      "Epoch [39/50], Hidden Size: 128, LR: 0.0001, Batch Size: 16, Loss: 0.3331\n",
      "Epoch [40/50], Hidden Size: 128, LR: 0.0001, Batch Size: 16, Loss: 0.3301\n",
      "Epoch [41/50], Hidden Size: 128, LR: 0.0001, Batch Size: 16, Loss: 0.3328\n",
      "Epoch [42/50], Hidden Size: 128, LR: 0.0001, Batch Size: 16, Loss: 0.3336\n",
      "Epoch [43/50], Hidden Size: 128, LR: 0.0001, Batch Size: 16, Loss: 0.3345\n",
      "Epoch [44/50], Hidden Size: 128, LR: 0.0001, Batch Size: 16, Loss: 0.3294\n",
      "Epoch [45/50], Hidden Size: 128, LR: 0.0001, Batch Size: 16, Loss: 0.3318\n",
      "Epoch [46/50], Hidden Size: 128, LR: 0.0001, Batch Size: 16, Loss: 0.3281\n",
      "Epoch [47/50], Hidden Size: 128, LR: 0.0001, Batch Size: 16, Loss: 0.3299\n",
      "Epoch [48/50], Hidden Size: 128, LR: 0.0001, Batch Size: 16, Loss: 0.3304\n",
      "Epoch [49/50], Hidden Size: 128, LR: 0.0001, Batch Size: 16, Loss: 0.3326\n",
      "Epoch [50/50], Hidden Size: 128, LR: 0.0001, Batch Size: 16, Loss: 0.3307\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\10374\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torchvision\\models\\_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "c:\\Users\\10374\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torchvision\\models\\_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=None`.\n",
      "  warnings.warn(msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/50], Hidden Size: 128, LR: 0.0001, Batch Size: 32, Loss: 0.9994\n",
      "Epoch [2/50], Hidden Size: 128, LR: 0.0001, Batch Size: 32, Loss: 0.9966\n",
      "Epoch [3/50], Hidden Size: 128, LR: 0.0001, Batch Size: 32, Loss: 0.9920\n",
      "Epoch [4/50], Hidden Size: 128, LR: 0.0001, Batch Size: 32, Loss: 0.9844\n",
      "Epoch [5/50], Hidden Size: 128, LR: 0.0001, Batch Size: 32, Loss: 0.9722\n",
      "Epoch [6/50], Hidden Size: 128, LR: 0.0001, Batch Size: 32, Loss: 0.9551\n",
      "Epoch [7/50], Hidden Size: 128, LR: 0.0001, Batch Size: 32, Loss: 0.9335\n",
      "Epoch [8/50], Hidden Size: 128, LR: 0.0001, Batch Size: 32, Loss: 0.9076\n",
      "Epoch [9/50], Hidden Size: 128, LR: 0.0001, Batch Size: 32, Loss: 0.8772\n",
      "Epoch [10/50], Hidden Size: 128, LR: 0.0001, Batch Size: 32, Loss: 0.8430\n",
      "Epoch [11/50], Hidden Size: 128, LR: 0.0001, Batch Size: 32, Loss: 0.8021\n",
      "Epoch [12/50], Hidden Size: 128, LR: 0.0001, Batch Size: 32, Loss: 0.7550\n",
      "Epoch [13/50], Hidden Size: 128, LR: 0.0001, Batch Size: 32, Loss: 0.7032\n",
      "Epoch [14/50], Hidden Size: 128, LR: 0.0001, Batch Size: 32, Loss: 0.6457\n",
      "Epoch [15/50], Hidden Size: 128, LR: 0.0001, Batch Size: 32, Loss: 0.5845\n",
      "Epoch [16/50], Hidden Size: 128, LR: 0.0001, Batch Size: 32, Loss: 0.5199\n",
      "Epoch [17/50], Hidden Size: 128, LR: 0.0001, Batch Size: 32, Loss: 0.4560\n",
      "Epoch [18/50], Hidden Size: 128, LR: 0.0001, Batch Size: 32, Loss: 0.4022\n",
      "Epoch [19/50], Hidden Size: 128, LR: 0.0001, Batch Size: 32, Loss: 0.3630\n",
      "Epoch [20/50], Hidden Size: 128, LR: 0.0001, Batch Size: 32, Loss: 0.3400\n",
      "Epoch [21/50], Hidden Size: 128, LR: 0.0001, Batch Size: 32, Loss: 0.3344\n",
      "Epoch [22/50], Hidden Size: 128, LR: 0.0001, Batch Size: 32, Loss: 0.3334\n",
      "Epoch [23/50], Hidden Size: 128, LR: 0.0001, Batch Size: 32, Loss: 0.3347\n",
      "Epoch [24/50], Hidden Size: 128, LR: 0.0001, Batch Size: 32, Loss: 0.3326\n",
      "Epoch [25/50], Hidden Size: 128, LR: 0.0001, Batch Size: 32, Loss: 0.3333\n",
      "Epoch [26/50], Hidden Size: 128, LR: 0.0001, Batch Size: 32, Loss: 0.3345\n",
      "Epoch [27/50], Hidden Size: 128, LR: 0.0001, Batch Size: 32, Loss: 0.3342\n",
      "Epoch [28/50], Hidden Size: 128, LR: 0.0001, Batch Size: 32, Loss: 0.3342\n",
      "Epoch [29/50], Hidden Size: 128, LR: 0.0001, Batch Size: 32, Loss: 0.3323\n",
      "Epoch [30/50], Hidden Size: 128, LR: 0.0001, Batch Size: 32, Loss: 0.3331\n",
      "Epoch [31/50], Hidden Size: 128, LR: 0.0001, Batch Size: 32, Loss: 0.3323\n",
      "Epoch [32/50], Hidden Size: 128, LR: 0.0001, Batch Size: 32, Loss: 0.3347\n",
      "Epoch [33/50], Hidden Size: 128, LR: 0.0001, Batch Size: 32, Loss: 0.3339\n",
      "Epoch [34/50], Hidden Size: 128, LR: 0.0001, Batch Size: 32, Loss: 0.3312\n",
      "Epoch [35/50], Hidden Size: 128, LR: 0.0001, Batch Size: 32, Loss: 0.3343\n",
      "Epoch [36/50], Hidden Size: 128, LR: 0.0001, Batch Size: 32, Loss: 0.3325\n",
      "Epoch [37/50], Hidden Size: 128, LR: 0.0001, Batch Size: 32, Loss: 0.3334\n",
      "Epoch [38/50], Hidden Size: 128, LR: 0.0001, Batch Size: 32, Loss: 0.3299\n",
      "Epoch [39/50], Hidden Size: 128, LR: 0.0001, Batch Size: 32, Loss: 0.3333\n",
      "Epoch [40/50], Hidden Size: 128, LR: 0.0001, Batch Size: 32, Loss: 0.3349\n",
      "Epoch [41/50], Hidden Size: 128, LR: 0.0001, Batch Size: 32, Loss: 0.3347\n",
      "Epoch [42/50], Hidden Size: 128, LR: 0.0001, Batch Size: 32, Loss: 0.3313\n",
      "Epoch [43/50], Hidden Size: 128, LR: 0.0001, Batch Size: 32, Loss: 0.3318\n",
      "Epoch [44/50], Hidden Size: 128, LR: 0.0001, Batch Size: 32, Loss: 0.3321\n",
      "Epoch [45/50], Hidden Size: 128, LR: 0.0001, Batch Size: 32, Loss: 0.3338\n",
      "Epoch [46/50], Hidden Size: 128, LR: 0.0001, Batch Size: 32, Loss: 0.3334\n",
      "Epoch [47/50], Hidden Size: 128, LR: 0.0001, Batch Size: 32, Loss: 0.3336\n",
      "Epoch [48/50], Hidden Size: 128, LR: 0.0001, Batch Size: 32, Loss: 0.3327\n",
      "Epoch [49/50], Hidden Size: 128, LR: 0.0001, Batch Size: 32, Loss: 0.3338\n",
      "Epoch [50/50], Hidden Size: 128, LR: 0.0001, Batch Size: 32, Loss: 0.3319\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\10374\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torchvision\\models\\_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "c:\\Users\\10374\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torchvision\\models\\_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=None`.\n",
      "  warnings.warn(msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/50], Hidden Size: 128, LR: 0.0001, Batch Size: 64, Loss: 0.9995\n",
      "Epoch [2/50], Hidden Size: 128, LR: 0.0001, Batch Size: 64, Loss: 0.9979\n",
      "Epoch [3/50], Hidden Size: 128, LR: 0.0001, Batch Size: 64, Loss: 0.9958\n",
      "Epoch [4/50], Hidden Size: 128, LR: 0.0001, Batch Size: 64, Loss: 0.9931\n",
      "Epoch [5/50], Hidden Size: 128, LR: 0.0001, Batch Size: 64, Loss: 0.9893\n",
      "Epoch [6/50], Hidden Size: 128, LR: 0.0001, Batch Size: 64, Loss: 0.9847\n",
      "Epoch [7/50], Hidden Size: 128, LR: 0.0001, Batch Size: 64, Loss: 0.9785\n",
      "Epoch [8/50], Hidden Size: 128, LR: 0.0001, Batch Size: 64, Loss: 0.9706\n",
      "Epoch [9/50], Hidden Size: 128, LR: 0.0001, Batch Size: 64, Loss: 0.9618\n",
      "Epoch [10/50], Hidden Size: 128, LR: 0.0001, Batch Size: 64, Loss: 0.9516\n",
      "Epoch [11/50], Hidden Size: 128, LR: 0.0001, Batch Size: 64, Loss: 0.9396\n",
      "Epoch [12/50], Hidden Size: 128, LR: 0.0001, Batch Size: 64, Loss: 0.9255\n",
      "Epoch [13/50], Hidden Size: 128, LR: 0.0001, Batch Size: 64, Loss: 0.9108\n",
      "Epoch [14/50], Hidden Size: 128, LR: 0.0001, Batch Size: 64, Loss: 0.8975\n",
      "Epoch [15/50], Hidden Size: 128, LR: 0.0001, Batch Size: 64, Loss: 0.8780\n",
      "Epoch [16/50], Hidden Size: 128, LR: 0.0001, Batch Size: 64, Loss: 0.8587\n",
      "Epoch [17/50], Hidden Size: 128, LR: 0.0001, Batch Size: 64, Loss: 0.8397\n",
      "Epoch [18/50], Hidden Size: 128, LR: 0.0001, Batch Size: 64, Loss: 0.8194\n",
      "Epoch [19/50], Hidden Size: 128, LR: 0.0001, Batch Size: 64, Loss: 0.7942\n",
      "Epoch [20/50], Hidden Size: 128, LR: 0.0001, Batch Size: 64, Loss: 0.7668\n",
      "Epoch [21/50], Hidden Size: 128, LR: 0.0001, Batch Size: 64, Loss: 0.7442\n",
      "Epoch [22/50], Hidden Size: 128, LR: 0.0001, Batch Size: 64, Loss: 0.7102\n",
      "Epoch [23/50], Hidden Size: 128, LR: 0.0001, Batch Size: 64, Loss: 0.6840\n",
      "Epoch [24/50], Hidden Size: 128, LR: 0.0001, Batch Size: 64, Loss: 0.6543\n",
      "Epoch [25/50], Hidden Size: 128, LR: 0.0001, Batch Size: 64, Loss: 0.6168\n",
      "Epoch [26/50], Hidden Size: 128, LR: 0.0001, Batch Size: 64, Loss: 0.5844\n",
      "Epoch [27/50], Hidden Size: 128, LR: 0.0001, Batch Size: 64, Loss: 0.5531\n",
      "Epoch [28/50], Hidden Size: 128, LR: 0.0001, Batch Size: 64, Loss: 0.5152\n",
      "Epoch [29/50], Hidden Size: 128, LR: 0.0001, Batch Size: 64, Loss: 0.4763\n",
      "Epoch [30/50], Hidden Size: 128, LR: 0.0001, Batch Size: 64, Loss: 0.4479\n",
      "Epoch [31/50], Hidden Size: 128, LR: 0.0001, Batch Size: 64, Loss: 0.4198\n",
      "Epoch [32/50], Hidden Size: 128, LR: 0.0001, Batch Size: 64, Loss: 0.3910\n",
      "Epoch [33/50], Hidden Size: 128, LR: 0.0001, Batch Size: 64, Loss: 0.3635\n",
      "Epoch [34/50], Hidden Size: 128, LR: 0.0001, Batch Size: 64, Loss: 0.3515\n",
      "Epoch [35/50], Hidden Size: 128, LR: 0.0001, Batch Size: 64, Loss: 0.3436\n",
      "Epoch [36/50], Hidden Size: 128, LR: 0.0001, Batch Size: 64, Loss: 0.3361\n",
      "Epoch [37/50], Hidden Size: 128, LR: 0.0001, Batch Size: 64, Loss: 0.3323\n",
      "Epoch [38/50], Hidden Size: 128, LR: 0.0001, Batch Size: 64, Loss: 0.3272\n",
      "Epoch [39/50], Hidden Size: 128, LR: 0.0001, Batch Size: 64, Loss: 0.3333\n",
      "Epoch [40/50], Hidden Size: 128, LR: 0.0001, Batch Size: 64, Loss: 0.3302\n",
      "Epoch [41/50], Hidden Size: 128, LR: 0.0001, Batch Size: 64, Loss: 0.3378\n",
      "Epoch [42/50], Hidden Size: 128, LR: 0.0001, Batch Size: 64, Loss: 0.3328\n",
      "Epoch [43/50], Hidden Size: 128, LR: 0.0001, Batch Size: 64, Loss: 0.3374\n",
      "Epoch [44/50], Hidden Size: 128, LR: 0.0001, Batch Size: 64, Loss: 0.3357\n",
      "Epoch [45/50], Hidden Size: 128, LR: 0.0001, Batch Size: 64, Loss: 0.3336\n",
      "Epoch [46/50], Hidden Size: 128, LR: 0.0001, Batch Size: 64, Loss: 0.3350\n",
      "Epoch [47/50], Hidden Size: 128, LR: 0.0001, Batch Size: 64, Loss: 0.3353\n",
      "Epoch [48/50], Hidden Size: 128, LR: 0.0001, Batch Size: 64, Loss: 0.3245\n",
      "Epoch [49/50], Hidden Size: 128, LR: 0.0001, Batch Size: 64, Loss: 0.3322\n",
      "Epoch [50/50], Hidden Size: 128, LR: 0.0001, Batch Size: 64, Loss: 0.3302\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "CombinedModel(\n",
       "  (seq_net): LSTM(2, 64, batch_first=True)\n",
       "  (seq_fc): Linear(in_features=64, out_features=32, bias=True)\n",
       "  (viome_net): LSTM(1, 64, batch_first=True)\n",
       "  (viome_fc): Linear(in_features=64, out_features=32, bias=True)\n",
       "  (img_net): ResNet(\n",
       "    (conv1): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n",
       "    (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (relu): ReLU(inplace=True)\n",
       "    (maxpool): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n",
       "    (layer1): Sequential(\n",
       "      (0): Bottleneck(\n",
       "        (conv1): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "        (downsample): Sequential(\n",
       "          (0): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        )\n",
       "      )\n",
       "      (1): Bottleneck(\n",
       "        (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "      )\n",
       "      (2): Bottleneck(\n",
       "        (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "      )\n",
       "    )\n",
       "    (layer2): Sequential(\n",
       "      (0): Bottleneck(\n",
       "        (conv1): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "        (downsample): Sequential(\n",
       "          (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "          (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        )\n",
       "      )\n",
       "      (1): Bottleneck(\n",
       "        (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "      )\n",
       "      (2): Bottleneck(\n",
       "        (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "      )\n",
       "      (3): Bottleneck(\n",
       "        (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "      )\n",
       "    )\n",
       "    (layer3): Sequential(\n",
       "      (0): Bottleneck(\n",
       "        (conv1): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "        (downsample): Sequential(\n",
       "          (0): Conv2d(512, 1024, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "          (1): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        )\n",
       "      )\n",
       "      (1): Bottleneck(\n",
       "        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "      )\n",
       "      (2): Bottleneck(\n",
       "        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "      )\n",
       "      (3): Bottleneck(\n",
       "        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "      )\n",
       "      (4): Bottleneck(\n",
       "        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "      )\n",
       "      (5): Bottleneck(\n",
       "        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "      )\n",
       "    )\n",
       "    (layer4): Sequential(\n",
       "      (0): Bottleneck(\n",
       "        (conv1): Conv2d(1024, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "        (downsample): Sequential(\n",
       "          (0): Conv2d(1024, 2048, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "          (1): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        )\n",
       "      )\n",
       "      (1): Bottleneck(\n",
       "        (conv1): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "      )\n",
       "      (2): Bottleneck(\n",
       "        (conv1): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "      )\n",
       "    )\n",
       "    (avgpool): AdaptiveAvgPool2d(output_size=(1, 1))\n",
       "    (fc): Linear(in_features=2048, out_features=32, bias=True)\n",
       "  )\n",
       "  (num_net): Sequential(\n",
       "    (0): Linear(in_features=20, out_features=32, bias=True)\n",
       "    (1): ReLU()\n",
       "    (2): Linear(in_features=32, out_features=16, bias=True)\n",
       "  )\n",
       "  (fc): Sequential(\n",
       "    (0): Linear(in_features=144, out_features=64, bias=True)\n",
       "    (1): ReLU()\n",
       "    (2): Linear(in_features=64, out_features=1, bias=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Hyperparameters\n",
    "hidden_sizes = [32, 64, 128]\n",
    "learning_rates = [1e-3, 1e-4]\n",
    "batch_sizes = [16, 32, 64]\n",
    "\n",
    "best_loss = float('inf')\n",
    "best_model = None\n",
    "\n",
    "# Iterate over different combinations of hyperparameters to find the best model\n",
    "for hidden_size in hidden_sizes:\n",
    "    for lr in learning_rates:\n",
    "        for batch_size in batch_sizes:\n",
    "            # Create the dataset loader for training and testing\n",
    "            train_loader = DataLoader(dataset_train, batch_size=batch_size, shuffle=True)\n",
    "            test_loader = DataLoader(dataset_test, batch_size=batch_size, shuffle=False)\n",
    "            \n",
    "            model = CombinedModel(hidden_size=hidden_size)\n",
    "            criterion = RMSRELoss()  \n",
    "            optimizer = optim.Adam(model.parameters(), lr=lr)\n",
    "            \n",
    "            # Learning rate scheduler\n",
    "            scheduler = optim.lr_scheduler.StepLR(optimizer, step_size=50, gamma=0.1)\n",
    "            \n",
    "            device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "            model = model.to(device)\n",
    "            criterion = criterion.to(device)\n",
    "            \n",
    "            # Training process\n",
    "            num_epochs = 50\n",
    "            for epoch in range(num_epochs):\n",
    "                model.train()\n",
    "                running_loss = 0.0\n",
    "                for batch in train_loader:\n",
    "                    sequential_data = batch['sequential_data'].to(device)\n",
    "                    img_b = batch['img_b'].to(device)\n",
    "                    img_l = batch['img_l'].to(device)\n",
    "                    numeric_data = batch['numeric_data'].to(device)\n",
    "                    viome_sequential = batch['viome_sequential'].to(device)\n",
    "                    label = batch['label'].to(device)\n",
    "                    \n",
    "                    outputs = model(sequential_data, img_b, img_l, numeric_data, viome_sequential)\n",
    "                    loss = criterion(outputs, label)\n",
    "                    \n",
    "                    # Backward pass and optimization\n",
    "                    optimizer.zero_grad()\n",
    "                    loss.backward()\n",
    "                    optimizer.step()\n",
    "                    \n",
    "                    running_loss += loss.item()\n",
    "                scheduler.step()\n",
    "                \n",
    "                avg_loss = running_loss / len(train_loader)\n",
    "                print(f\"Epoch [{epoch+1}/{num_epochs}], Hidden Size: {hidden_size}, LR: {lr}, Batch Size: {batch_size}, Loss: {avg_loss:.4f}\")\n",
    "            \n",
    "            # Save the best model\n",
    "            if avg_loss < best_loss:\n",
    "                best_loss = avg_loss\n",
    "                best_model = model\n",
    "\n",
    "# Use the best trained model for predictions\n",
    "best_model.eval()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predictions saved to 'submission.csv' in Kaggle format.\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "test_loss = 0.0\n",
    "all_outputs = []\n",
    "all_labels = []\n",
    "\n",
    "with torch.no_grad():\n",
    "    for batch in test_loader:\n",
    "        sequential_data = batch['sequential_data'].to(device)\n",
    "        img_b = batch['img_b'].to(device)\n",
    "        img_l = batch['img_l'].to(device)\n",
    "        numeric_data = batch['numeric_data'].to(device)\n",
    "        viome_sequential = batch['viome_sequential'].to(device)\n",
    "        \n",
    "        outputs = model(sequential_data, img_b, img_l, numeric_data, viome_sequential)\n",
    "        \n",
    "        all_outputs += ([i[0] for i in outputs.tolist()])\n",
    "        \n",
    "test_ids = np.arange(len(all_outputs)) \n",
    "submission = pd.DataFrame({\"row_id\": test_ids, \"label\": all_outputs})\n",
    "submission.to_csv(\"submission.csv\", index=False)\n",
    "print(\"Predictions saved to 'submission.csv' in Kaggle format.\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
