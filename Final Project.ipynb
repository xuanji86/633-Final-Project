{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import numpy as np\n",
    "import ast\n",
    "from PIL import Image\n",
    "from datetime import datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "cgm_test = pd.read_csv('633FinalData/cgm_test.csv')\n",
    "cgm_train = pd.read_csv('633FinalData/cgm_train.csv')\n",
    "img_train = pd.read_csv('633FinalData/img_train.csv')\n",
    "img_test = pd.read_csv('633FinalData/img_test.csv')\n",
    "cgm_test['cgm'] = cgm_test['CGM Data'].apply(lambda x: ast.literal_eval(x))\n",
    "cgm_train['cgm'] = cgm_train['CGM Data'].apply(lambda x: ast.literal_eval(x))\n",
    "def get_img(img):\n",
    "    # Convert the string representation of the image to a list\n",
    "    img = ast.literal_eval(img)\n",
    "    \n",
    "    # Convert the list to a NumPy array\n",
    "    img = np.array(img)\n",
    "    \n",
    "    # Convert the NumPy array to a PIL Image\n",
    "    img = Image.fromarray(np.uint8(img))\n",
    "    \n",
    "    # Resize the image\n",
    "    img = img.resize((64,64))\n",
    "    \n",
    "    # Convert the resized image back to a NumPy array\n",
    "    img = np.array(img)\n",
    "    \n",
    "    if len(img.shape) == 2:\n",
    "        img = np.stack((img,)*3, axis=-1)\n",
    "    return img\n",
    "\n",
    "img_train['img_b'] = img_train['Image Before Breakfast'].apply(get_img)\n",
    "img_train['img_l'] = img_train['Image Before Lunch'].apply(get_img)\n",
    "img_test['img_b'] = img_test['Image Before Breakfast'].apply(get_img)\n",
    "img_test['img_l'] = img_test['Image Before Lunch'].apply(get_img)\n",
    "\n",
    "def to_step(t):\n",
    "    date_obj = datetime.strptime(t, '%Y-%m-%d %H:%M:%S')\n",
    "    return (date_obj.hour*60 + date_obj.minute)//5\n",
    "\n",
    "def cgm_to_steps(cgm):\n",
    "    steps = [0 for _ in range(288)]\n",
    "    for t,value in cgm:\n",
    "        steps[to_step(t)] = value\n",
    "    return steps\n",
    "\n",
    "def time_to_step(t1,t2):\n",
    "    if t1 == '{}' or t2 == '{}':\n",
    "        return [0 for _ in range(288)]\n",
    "    steps = [0 for _ in range(288)]\n",
    "    steps[to_step(t1)] = 1\n",
    "    steps[to_step(t2)] = 1\n",
    "    return steps\n",
    "\n",
    "cgm_train = cgm_train.dropna()\n",
    "\n",
    "cgm_train = cgm_train[~cgm_train.apply(lambda row: row.astype(str).str.contains('{}').any(), axis=1)]\n",
    "\n",
    "cgm_test['cgm_sequential'] = cgm_test['cgm'].apply(cgm_to_steps)\n",
    "cgm_train['cgm_sequential'] = cgm_train['cgm'].apply(cgm_to_steps)\n",
    "cgm_train['when_to_eat'] = cgm_train[['Breakfast Time', 'Lunch Time']].apply(lambda x: time_to_step(x['Breakfast Time'], x['Lunch Time']), axis=1)\n",
    "cgm_test['when_to_eat'] = cgm_test[['Breakfast Time', 'Lunch Time']].apply(lambda x: time_to_step(x['Breakfast Time'], x['Lunch Time']), axis=1)\n",
    "viome_test = pd.read_csv('633FinalData/demo_viome_test.csv')\n",
    "viome_train = pd.read_csv('633FinalData/demo_viome_train.csv')\n",
    "viome_test= pd.get_dummies(viome_test, columns=['Race'])\n",
    "viome_train= pd.get_dummies(viome_train, columns=['Race'])\n",
    "viome_test['viome_sequential'] = viome_test['Viome'].apply(lambda x :[float(x) for x in x.split(',')])\n",
    "viome_train['viome_sequential'] = viome_train['Viome'].apply(lambda x :[float(x) for x in x.split(',')])\n",
    "combined_train = pd.merge(cgm_train, img_train, on=['Subject ID', 'Day'])\n",
    "combined_train = pd.merge(combined_train, viome_train, on=['Subject ID'])\n",
    "combined_test = pd.merge(cgm_test, img_test, on=['Subject ID', 'Day'])\n",
    "combined_test = pd.merge(combined_test, viome_test, on=['Subject ID'])\n",
    "to_drop_train = ['Subject ID','Day','Breakfast Time','Lunch Time','CGM Data','Image Before Breakfast','Image Before Lunch','Viome']\n",
    "combined_train = combined_train.drop(to_drop_train, axis=1)\n",
    "to_drop_test = ['Subject ID','Day','Breakfast Time','Lunch Time','CGM Data','Image Before Breakfast','Image Before Lunch','Viome']\n",
    "to_drop_test = ['Subject ID','Day','Breakfast Time','Lunch Time','CGM Data','Image Before Breakfast','Image Before Lunch','Viome']\n",
    "combined_test = combined_test.drop(to_drop_train, axis=1)\n",
    "label_train = pd.read_csv('633FinalData/label_train.csv')\n",
    "label_train = label_train['Lunch Calories']\n",
    "combined_train['label'] = label_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomDataset(Dataset):\n",
    "    def __init__(self, dataframe):\n",
    "        self.dataframe = dataframe\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.dataframe)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        row = self.dataframe.iloc[idx]\n",
    "\n",
    "        # Small epsilon value to avoid division by zero\n",
    "        epsilon = 1e-6\n",
    "\n",
    "        # Extract sequential data\n",
    "        cgm_sequential = np.array(row['cgm_sequential'], dtype=float)\n",
    "        when_to_eat = np.array(row['when_to_eat'], dtype=float)\n",
    "\n",
    "        # Normalize sequential data\n",
    "        cgm_sequential = (cgm_sequential - np.mean(cgm_sequential)) / (np.std(cgm_sequential) + epsilon)\n",
    "        when_to_eat = (when_to_eat - np.mean(when_to_eat)) / (np.std(when_to_eat) + epsilon)\n",
    "\n",
    "        # Stack cgm_sequential and when_to_eat to create a sequential data array with 2 features\n",
    "        sequential_data = np.stack((cgm_sequential, when_to_eat), axis=-1)\n",
    "\n",
    "        # Extract image data\n",
    "        img_b = np.array(row['img_b'], dtype=float)\n",
    "        img_l = np.array(row['img_l'], dtype=float)\n",
    "\n",
    "        # Normalize image data\n",
    "        img_b = (img_b - np.mean(img_b)) / (np.std(img_b) + epsilon)\n",
    "        img_l = (img_l - np.mean(img_l)) / (np.std(img_l) + epsilon)\n",
    "\n",
    "        # Ensure img_b has 3 channels\n",
    "        if img_b.ndim == 2:  # Grayscale image\n",
    "            img_b = np.stack((img_b, img_b, img_b), axis=-1)\n",
    "\n",
    "        # Ensure img_l has 3 channels\n",
    "        if img_l.ndim == 2:  # Grayscale image\n",
    "            img_l = np.stack((img_l, img_l, img_l), axis=-1)\n",
    "\n",
    "        # Convert image data to PIL Image and then to tensor\n",
    "        img_b = Image.fromarray(np.uint8(img_b))\n",
    "        img_l = Image.fromarray(np.uint8(img_l))\n",
    "        img_b = torch.tensor(np.array(img_b), dtype=torch.float32).permute(2, 0, 1)  # Convert to CxHxW\n",
    "        img_l = torch.tensor(np.array(img_l), dtype=torch.float32).permute(2, 0, 1)  # Convert to CxHxW\n",
    "\n",
    "        # Extract numeric data\n",
    "        numeric_data = row[['Age', 'Gender', 'Weight', 'Height', 'Diabetes Status', 'A1C',\n",
    "                            'Baseline Fasting Glucose', 'Insulin', 'Triglycerides', 'Cholesterol',\n",
    "                            'HDL', 'Non-HDL', 'LDL', 'VLDL', 'CHO/HDL Ratio', 'HOMA-IR', 'BMI',\n",
    "                            'Race_African American', 'Race_Hispanic/Latino', 'Race_White']].values.astype(np.float32)\n",
    "\n",
    "        # Normalize numeric data\n",
    "        numeric_data = (numeric_data - np.mean(numeric_data)) / (np.std(numeric_data) + epsilon)\n",
    "        numeric_data = torch.tensor(numeric_data, dtype=torch.float32)\n",
    "\n",
    "        # Extract and reshape viome_sequential\n",
    "        viome_sequential = np.array(row['viome_sequential'], dtype=float).reshape(27, 1)\n",
    "\n",
    "        # Normalize viome_sequential\n",
    "        viome_sequential = (viome_sequential - np.mean(viome_sequential)) / (np.std(viome_sequential) + epsilon)\n",
    "\n",
    "        label_train = row['label']\n",
    "\n",
    "        return {\n",
    "            'sequential_data': torch.tensor(sequential_data, dtype=torch.float32),\n",
    "            'viome_sequential': torch.tensor(viome_sequential, dtype=torch.float32),\n",
    "            'img_b': img_b,\n",
    "            'img_l': img_l,\n",
    "            'numeric_data': numeric_data,\n",
    "            'label': label_train\n",
    "        }\n",
    "        \n",
    "class CustomTestDataset(Dataset):\n",
    "    def __init__(self, dataframe):\n",
    "        self.dataframe = dataframe\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.dataframe)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        row = self.dataframe.iloc[idx]\n",
    "\n",
    "        # Small epsilon value to avoid division by zero\n",
    "        epsilon = 1e-6\n",
    "\n",
    "        # Extract sequential data\n",
    "        cgm_sequential = np.array(row['cgm_sequential'], dtype=float)\n",
    "        when_to_eat = np.array(row['when_to_eat'], dtype=float)\n",
    "\n",
    "        # Normalize sequential data\n",
    "        cgm_sequential = (cgm_sequential - np.mean(cgm_sequential)) / (np.std(cgm_sequential) + epsilon)\n",
    "        when_to_eat = (when_to_eat - np.mean(when_to_eat)) / (np.std(when_to_eat) + epsilon)\n",
    "\n",
    "        # Stack cgm_sequential and when_to_eat to create a sequential data array with 2 features\n",
    "        sequential_data = np.stack((cgm_sequential, when_to_eat), axis=-1)\n",
    "\n",
    "        # Extract image data\n",
    "        img_b = np.array(row['img_b'], dtype=float)\n",
    "        img_l = np.array(row['img_l'], dtype=float)\n",
    "\n",
    "        # Normalize image data\n",
    "        img_b = (img_b - np.mean(img_b)) / (np.std(img_b) + epsilon)\n",
    "        img_l = (img_l - np.mean(img_l)) / (np.std(img_l) + epsilon)\n",
    "\n",
    "        # Ensure img_b has 3 channels\n",
    "        if img_b.ndim == 2:  # Grayscale image\n",
    "            img_b = np.stack((img_b, img_b, img_b), axis=-1)\n",
    "\n",
    "        # Ensure img_l has 3 channels\n",
    "        if img_l.ndim == 2:  # Grayscale image\n",
    "            img_l = np.stack((img_l, img_l, img_l), axis=-1)\n",
    "\n",
    "        # Convert image data to PIL Image and then to tensor\n",
    "        img_b = Image.fromarray(np.uint8(img_b))\n",
    "        img_l = Image.fromarray(np.uint8(img_l))\n",
    "        img_b = torch.tensor(np.array(img_b), dtype=torch.float32).permute(2, 0, 1)  # Convert to CxHxW\n",
    "        img_l = torch.tensor(np.array(img_l), dtype=torch.float32).permute(2, 0, 1)  # Convert to CxHxW\n",
    "\n",
    "        # Extract numeric data\n",
    "        numeric_data = row[['Age', 'Gender', 'Weight', 'Height', 'Diabetes Status', 'A1C',\n",
    "                            'Baseline Fasting Glucose', 'Insulin', 'Triglycerides', 'Cholesterol',\n",
    "                            'HDL', 'Non-HDL', 'LDL', 'VLDL', 'CHO/HDL Ratio', 'HOMA-IR', 'BMI',\n",
    "                            'Race_African American', 'Race_Hispanic/Latino', 'Race_White']].values.astype(np.float32)\n",
    "\n",
    "        # Normalize numeric data\n",
    "        numeric_data = (numeric_data - np.mean(numeric_data)) / (np.std(numeric_data) + epsilon)\n",
    "        numeric_data = torch.tensor(numeric_data, dtype=torch.float32)\n",
    "\n",
    "        # Extract and reshape viome_sequential\n",
    "        viome_sequential = np.array(row['viome_sequential'], dtype=float).reshape(27, 1)\n",
    "\n",
    "        # Normalize viome_sequential\n",
    "        viome_sequential = (viome_sequential - np.mean(viome_sequential)) / (np.std(viome_sequential) + epsilon)\n",
    "\n",
    "\n",
    "        return {\n",
    "            'sequential_data': torch.tensor(sequential_data, dtype=torch.float32),\n",
    "            'viome_sequential': torch.tensor(viome_sequential, dtype=torch.float32),\n",
    "            'img_b': img_b,\n",
    "            'img_l': img_l,\n",
    "            'numeric_data': numeric_data,\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample 0:\n",
      "  sequential_data shape: torch.Size([288, 2])\n",
      "  viome_sequential shape: torch.Size([27, 1])\n",
      "  img_b shape: torch.Size([3, 64, 64])\n",
      "  img_l shape: torch.Size([3, 64, 64])\n",
      "  numeric_data shape: torch.Size([20])\n",
      "  label: 830\n"
     ]
    }
   ],
   "source": [
    "dataset_train = CustomDataset(combined_train)\n",
    "dataset_test = CustomTestDataset(combined_test)\n",
    "combined_train['img_b'].apply(lambda x: x.shape).unique()\n",
    "\n",
    "for i in range(1):\n",
    "    sample = dataset_train[i]\n",
    "    print(f\"Sample {i}:\")\n",
    "    print(f\"  sequential_data shape: {sample['sequential_data'].shape}\")\n",
    "    print(f\"  viome_sequential shape: {sample['viome_sequential'].shape}\")\n",
    "    print(f\"  img_b shape: {sample['img_b'].shape}\")\n",
    "    print(f\"  img_l shape: {sample['img_l'].shape}\")\n",
    "    print(f\"  numeric_data shape: {sample['numeric_data'].shape}\")\n",
    "    print(f\"  label: {sample['label']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader\n",
    "import torchvision.models as models\n",
    "\n",
    "class CombinedModel(nn.Module):\n",
    "    def __init__(self, hidden_size=64):\n",
    "        super(CombinedModel, self).__init__()\n",
    "        \n",
    "        # Sequential data sub-network\n",
    "        self.seq_net = nn.LSTM(input_size=2, hidden_size=hidden_size, num_layers=1, batch_first=True)\n",
    "        self.seq_fc = nn.Linear(hidden_size, 32)\n",
    "        \n",
    "        # Viome sequential data sub-network\n",
    "        self.viome_net = nn.LSTM(input_size=1, hidden_size=hidden_size, num_layers=1, batch_first=True)\n",
    "        self.viome_fc = nn.Linear(hidden_size, 32)\n",
    "        \n",
    "        # Image data sub-network using pre-trained ResNet\n",
    "        self.img_net = models.resnet50(pretrained=False)\n",
    "        self.img_net.fc = nn.Linear(self.img_net.fc.in_features, 32)  # Modify the final layer\n",
    "        \n",
    "        # Numeric data sub-network\n",
    "        self.num_net = nn.Sequential(\n",
    "            nn.Linear(20, 32),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(32, 16)\n",
    "        )\n",
    "        \n",
    "        # Combined fully connected layers\n",
    "        self.fc = nn.Sequential(\n",
    "            nn.Linear(32 + 32 + 32 + 32 + 16, 64),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(64, 1)  # Assuming a regression task\n",
    "        )\n",
    "    \n",
    "    def forward(self, sequential_data, img_b, img_l, numeric_data, viome_sequential):\n",
    "        # Process sequential data\n",
    "        seq_out, _ = self.seq_net(sequential_data)\n",
    "        seq_out = self.seq_fc(seq_out[:, -1, :])  # Take the last output of the LSTM and pass through a linear layer\n",
    "        \n",
    "        # Process viome sequential data\n",
    "        viome_out, _ = self.viome_net(viome_sequential)\n",
    "        viome_out = self.viome_fc(viome_out[:, -1, :])  # Take the last output of the LSTM and pass through a linear layer\n",
    "        \n",
    "        # Process image data\n",
    "        img_b_out = self.img_net(img_b)\n",
    "        img_l_out = self.img_net(img_l)\n",
    "        \n",
    "        # Process numeric data\n",
    "        num_out = self.num_net(numeric_data)\n",
    "        \n",
    "        # Concatenate all outputs\n",
    "        combined = torch.cat((seq_out, viome_out, img_b_out, img_l_out, num_out), dim=1)\n",
    "        \n",
    "        # Pass through fully connected layers\n",
    "        out = self.fc(combined)\n",
    "        \n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\10374\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torchvision\\models\\_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "c:\\Users\\10374\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torchvision\\models\\_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=None`.\n",
      "  warnings.warn(msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/50], Loss: 0.9797210428449843\n",
      "Epoch [2/50], Loss: 0.8379740118980408\n",
      "Epoch [3/50], Loss: 0.5090599126285977\n",
      "Epoch [4/50], Loss: 0.3614649242824978\n",
      "Epoch [5/50], Loss: 0.33659889300664264\n",
      "Epoch [6/50], Loss: 0.33484194344944423\n",
      "Epoch [7/50], Loss: 0.33350226614210343\n",
      "Epoch [8/50], Loss: 0.33361560106277466\n",
      "Epoch [9/50], Loss: 0.33550000853008694\n",
      "Epoch [10/50], Loss: 0.3327505952782101\n",
      "Epoch [11/50], Loss: 0.3334180878268348\n",
      "Epoch [12/50], Loss: 0.3349885510073768\n",
      "Epoch [13/50], Loss: 0.33583178785112167\n",
      "Epoch [14/50], Loss: 0.3374780664841334\n",
      "Epoch [15/50], Loss: 0.33575088779131573\n",
      "Epoch [16/50], Loss: 0.3335108194086287\n",
      "Epoch [17/50], Loss: 0.3364243871635861\n",
      "Epoch [18/50], Loss: 0.33404554923375446\n",
      "Epoch [19/50], Loss: 0.33695909049775863\n",
      "Epoch [20/50], Loss: 0.3351668847931756\n",
      "Epoch [21/50], Loss: 0.3334234489334954\n",
      "Epoch [22/50], Loss: 0.33417702714602154\n",
      "Epoch [23/50], Loss: 0.33274244599872166\n",
      "Epoch [24/50], Loss: 0.3365065356095632\n",
      "Epoch [25/50], Loss: 0.3408759766154819\n",
      "Epoch [26/50], Loss: 0.3358657889895969\n",
      "Epoch [27/50], Loss: 0.3374414278401269\n",
      "Epoch [28/50], Loss: 0.3296120646927092\n",
      "Epoch [29/50], Loss: 0.3342900474866231\n",
      "Epoch [30/50], Loss: 0.33525272210439044\n",
      "Epoch [31/50], Loss: 0.3403074906931983\n",
      "Epoch [32/50], Loss: 0.337328980366389\n",
      "Epoch [33/50], Loss: 0.3412419425116645\n",
      "Epoch [34/50], Loss: 0.3331059316794078\n",
      "Epoch [35/50], Loss: 0.3366249402364095\n",
      "Epoch [36/50], Loss: 0.3335545063018799\n",
      "Epoch [37/50], Loss: 0.3328704569074843\n",
      "Epoch [38/50], Loss: 0.33419351776440936\n",
      "Epoch [39/50], Loss: 0.33584946062829757\n",
      "Epoch [40/50], Loss: 0.3357052670584785\n",
      "Epoch [41/50], Loss: 0.33382968770133126\n",
      "Epoch [42/50], Loss: 0.33151520126395756\n",
      "Epoch [43/50], Loss: 0.3325505753358205\n",
      "Epoch [44/50], Loss: 0.3321259535021252\n",
      "Epoch [45/50], Loss: 0.3395715090963576\n",
      "Epoch [46/50], Loss: 0.3353862696223789\n",
      "Epoch [47/50], Loss: 0.33255555894639754\n",
      "Epoch [48/50], Loss: 0.33574679493904114\n",
      "Epoch [49/50], Loss: 0.3342136740684509\n",
      "Epoch [50/50], Loss: 0.33530906173917985\n",
      "Training complete.\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "epoch_losses = []\n",
    "\n",
    "class RMSRELoss(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(RMSRELoss, self).__init__()\n",
    "\n",
    "    def forward(self, y_pred, y_true):\n",
    "        epsilon = 1e-6  # Small value to avoid division by zero\n",
    "        relative_error = (y_pred - y_true) / (y_true + epsilon)\n",
    "        return torch.sqrt(torch.mean(relative_error ** 2))\n",
    "    \n",
    "train_loader = DataLoader(dataset_train, batch_size=32, shuffle=True)\n",
    "test_loader = DataLoader(dataset_test, batch_size=32, shuffle=False)\n",
    "\n",
    "model = CombinedModel()\n",
    "criterion = RMSRELoss()  \n",
    "optimizer = optim.Adam(model.parameters(), lr=1e-3)  # Reduced learning rate\n",
    "\n",
    "# Learning rate scheduler\n",
    "scheduler = optim.lr_scheduler.StepLR(optimizer, step_size=50, gamma=0.1)\n",
    "device = torch.device(\"cuda\")\n",
    "\n",
    "model = model.to(device)\n",
    "criterion = criterion.to(device)\n",
    "\n",
    "num_epochs = 50\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "    for batch in train_loader:\n",
    "        sequential_data = batch['sequential_data'].to(device)\n",
    "        img_b = batch['img_b'].to(device)\n",
    "        img_l = batch['img_l'].to(device)\n",
    "        numeric_data = batch['numeric_data'].to(device)\n",
    "        viome_sequential = batch['viome_sequential'].to(device)\n",
    "        label = batch['label'].to(device)\n",
    "        \n",
    "        # Forward pass\n",
    "        outputs = model(sequential_data, img_b, img_l, numeric_data, viome_sequential)\n",
    "        loss = criterion(outputs, label)\n",
    "        \n",
    "        # Backward pass and optimization\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        running_loss += loss.item()\n",
    "    \n",
    "\n",
    "    epoch_loss = running_loss / len(train_loader)\n",
    "    epoch_losses.append(epoch_loss)\n",
    "    print(f\"Epoch [{epoch+1}/{num_epochs}], Loss: {epoch_loss}\")\n",
    "\n",
    "    scheduler.step()\n",
    "\n",
    "print(\"Training complete.\")\n",
    "\n",
    "plt.plot(range(1, num_epochs + 1), epoch_losses, marker='o')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.title('Training Loss over Epochs')\n",
    "plt.grid(True)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predictions saved to 'submission.csv' in Kaggle format.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "model.eval()  # Set the model to evaluation mode\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = model.to(device)\n",
    "test_loss = 0.0\n",
    "all_outputs = []\n",
    "all_labels = []\n",
    "\n",
    "with torch.no_grad():  # Disable gradient calculation\n",
    "    for batch in test_loader:\n",
    "        sequential_data = batch['sequential_data'].to(device)\n",
    "        img_b = batch['img_b'].to(device)\n",
    "        img_l = batch['img_l'].to(device)\n",
    "        numeric_data = batch['numeric_data'].to(device)\n",
    "        viome_sequential = batch['viome_sequential'].to(device)\n",
    "        \n",
    "        # Forward pass\n",
    "        outputs = model(sequential_data, img_b, img_l, numeric_data, viome_sequential)\n",
    "        \n",
    "        all_outputs += ([i[0] for i in outputs.tolist()])\n",
    "        \n",
    "test_ids = np.arange(len(all_outputs))  \n",
    "submission = pd.DataFrame({\"row_id\": test_ids, \"label\": all_outputs})\n",
    "submission.to_csv(\"submission.csv\", index=False)\n",
    "print(\"Predictions saved to 'submission.csv' in Kaggle format.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\10374\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torchvision\\models\\_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "c:\\Users\\10374\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torchvision\\models\\_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=None`.\n",
      "  warnings.warn(msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/50], Hidden Size: 32, LR: 0.001, Batch Size: 16, Loss: 0.9222\n",
      "Epoch [2/50], Hidden Size: 32, LR: 0.001, Batch Size: 16, Loss: 0.4521\n",
      "Epoch [3/50], Hidden Size: 32, LR: 0.001, Batch Size: 16, Loss: 0.3379\n",
      "Epoch [4/50], Hidden Size: 32, LR: 0.001, Batch Size: 16, Loss: 0.3320\n",
      "Epoch [5/50], Hidden Size: 32, LR: 0.001, Batch Size: 16, Loss: 0.3355\n",
      "Epoch [6/50], Hidden Size: 32, LR: 0.001, Batch Size: 16, Loss: 0.3353\n",
      "Epoch [7/50], Hidden Size: 32, LR: 0.001, Batch Size: 16, Loss: 0.3343\n",
      "Epoch [8/50], Hidden Size: 32, LR: 0.001, Batch Size: 16, Loss: 0.3314\n",
      "Epoch [9/50], Hidden Size: 32, LR: 0.001, Batch Size: 16, Loss: 0.3312\n",
      "Epoch [10/50], Hidden Size: 32, LR: 0.001, Batch Size: 16, Loss: 0.3356\n",
      "Epoch [11/50], Hidden Size: 32, LR: 0.001, Batch Size: 16, Loss: 0.3353\n",
      "Epoch [12/50], Hidden Size: 32, LR: 0.001, Batch Size: 16, Loss: 0.3386\n",
      "Epoch [13/50], Hidden Size: 32, LR: 0.001, Batch Size: 16, Loss: 0.3473\n",
      "Epoch [14/50], Hidden Size: 32, LR: 0.001, Batch Size: 16, Loss: 0.3319\n",
      "Epoch [15/50], Hidden Size: 32, LR: 0.001, Batch Size: 16, Loss: 0.3395\n",
      "Epoch [16/50], Hidden Size: 32, LR: 0.001, Batch Size: 16, Loss: 0.3420\n",
      "Epoch [17/50], Hidden Size: 32, LR: 0.001, Batch Size: 16, Loss: 0.3348\n",
      "Epoch [18/50], Hidden Size: 32, LR: 0.001, Batch Size: 16, Loss: 0.3328\n",
      "Epoch [19/50], Hidden Size: 32, LR: 0.001, Batch Size: 16, Loss: 0.3379\n",
      "Epoch [20/50], Hidden Size: 32, LR: 0.001, Batch Size: 16, Loss: 0.3323\n",
      "Epoch [21/50], Hidden Size: 32, LR: 0.001, Batch Size: 16, Loss: 0.3326\n",
      "Epoch [22/50], Hidden Size: 32, LR: 0.001, Batch Size: 16, Loss: 0.3536\n",
      "Epoch [23/50], Hidden Size: 32, LR: 0.001, Batch Size: 16, Loss: 0.3424\n",
      "Epoch [24/50], Hidden Size: 32, LR: 0.001, Batch Size: 16, Loss: 0.3360\n",
      "Epoch [25/50], Hidden Size: 32, LR: 0.001, Batch Size: 16, Loss: 0.3314\n",
      "Epoch [26/50], Hidden Size: 32, LR: 0.001, Batch Size: 16, Loss: 0.3336\n",
      "Epoch [27/50], Hidden Size: 32, LR: 0.001, Batch Size: 16, Loss: 0.3345\n",
      "Epoch [28/50], Hidden Size: 32, LR: 0.001, Batch Size: 16, Loss: 0.3319\n",
      "Epoch [29/50], Hidden Size: 32, LR: 0.001, Batch Size: 16, Loss: 0.3354\n",
      "Epoch [30/50], Hidden Size: 32, LR: 0.001, Batch Size: 16, Loss: 0.3331\n",
      "Epoch [31/50], Hidden Size: 32, LR: 0.001, Batch Size: 16, Loss: 0.3353\n",
      "Epoch [32/50], Hidden Size: 32, LR: 0.001, Batch Size: 16, Loss: 0.3384\n",
      "Epoch [33/50], Hidden Size: 32, LR: 0.001, Batch Size: 16, Loss: 0.3355\n",
      "Epoch [34/50], Hidden Size: 32, LR: 0.001, Batch Size: 16, Loss: 0.3370\n",
      "Epoch [35/50], Hidden Size: 32, LR: 0.001, Batch Size: 16, Loss: 0.3320\n",
      "Epoch [36/50], Hidden Size: 32, LR: 0.001, Batch Size: 16, Loss: 0.3337\n",
      "Epoch [37/50], Hidden Size: 32, LR: 0.001, Batch Size: 16, Loss: 0.3351\n",
      "Epoch [38/50], Hidden Size: 32, LR: 0.001, Batch Size: 16, Loss: 0.3285\n",
      "Epoch [39/50], Hidden Size: 32, LR: 0.001, Batch Size: 16, Loss: 0.3386\n",
      "Epoch [40/50], Hidden Size: 32, LR: 0.001, Batch Size: 16, Loss: 0.3333\n",
      "Epoch [41/50], Hidden Size: 32, LR: 0.001, Batch Size: 16, Loss: 0.3336\n",
      "Epoch [42/50], Hidden Size: 32, LR: 0.001, Batch Size: 16, Loss: 0.3347\n",
      "Epoch [43/50], Hidden Size: 32, LR: 0.001, Batch Size: 16, Loss: 0.3393\n",
      "Epoch [44/50], Hidden Size: 32, LR: 0.001, Batch Size: 16, Loss: 0.3305\n",
      "Epoch [45/50], Hidden Size: 32, LR: 0.001, Batch Size: 16, Loss: 0.3364\n",
      "Epoch [46/50], Hidden Size: 32, LR: 0.001, Batch Size: 16, Loss: 0.3332\n",
      "Epoch [47/50], Hidden Size: 32, LR: 0.001, Batch Size: 16, Loss: 0.3391\n",
      "Epoch [48/50], Hidden Size: 32, LR: 0.001, Batch Size: 16, Loss: 0.3331\n",
      "Epoch [49/50], Hidden Size: 32, LR: 0.001, Batch Size: 16, Loss: 0.3308\n",
      "Epoch [50/50], Hidden Size: 32, LR: 0.001, Batch Size: 16, Loss: 0.3357\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\10374\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torchvision\\models\\_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "c:\\Users\\10374\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torchvision\\models\\_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=None`.\n",
      "  warnings.warn(msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/50], Hidden Size: 32, LR: 0.001, Batch Size: 32, Loss: 0.9606\n",
      "Epoch [2/50], Hidden Size: 32, LR: 0.001, Batch Size: 32, Loss: 0.7038\n",
      "Epoch [3/50], Hidden Size: 32, LR: 0.001, Batch Size: 32, Loss: 0.3803\n",
      "Epoch [4/50], Hidden Size: 32, LR: 0.001, Batch Size: 32, Loss: 0.3452\n",
      "Epoch [5/50], Hidden Size: 32, LR: 0.001, Batch Size: 32, Loss: 0.3354\n",
      "Epoch [6/50], Hidden Size: 32, LR: 0.001, Batch Size: 32, Loss: 0.3336\n",
      "Epoch [7/50], Hidden Size: 32, LR: 0.001, Batch Size: 32, Loss: 0.3340\n",
      "Epoch [8/50], Hidden Size: 32, LR: 0.001, Batch Size: 32, Loss: 0.3333\n",
      "Epoch [9/50], Hidden Size: 32, LR: 0.001, Batch Size: 32, Loss: 0.3387\n",
      "Epoch [10/50], Hidden Size: 32, LR: 0.001, Batch Size: 32, Loss: 0.3349\n",
      "Epoch [11/50], Hidden Size: 32, LR: 0.001, Batch Size: 32, Loss: 0.3347\n",
      "Epoch [12/50], Hidden Size: 32, LR: 0.001, Batch Size: 32, Loss: 0.3358\n",
      "Epoch [13/50], Hidden Size: 32, LR: 0.001, Batch Size: 32, Loss: 0.3355\n",
      "Epoch [14/50], Hidden Size: 32, LR: 0.001, Batch Size: 32, Loss: 0.3344\n",
      "Epoch [15/50], Hidden Size: 32, LR: 0.001, Batch Size: 32, Loss: 0.3355\n",
      "Epoch [16/50], Hidden Size: 32, LR: 0.001, Batch Size: 32, Loss: 0.3347\n",
      "Epoch [17/50], Hidden Size: 32, LR: 0.001, Batch Size: 32, Loss: 0.3329\n",
      "Epoch [18/50], Hidden Size: 32, LR: 0.001, Batch Size: 32, Loss: 0.3368\n",
      "Epoch [19/50], Hidden Size: 32, LR: 0.001, Batch Size: 32, Loss: 0.3362\n",
      "Epoch [20/50], Hidden Size: 32, LR: 0.001, Batch Size: 32, Loss: 0.3342\n",
      "Epoch [21/50], Hidden Size: 32, LR: 0.001, Batch Size: 32, Loss: 0.3345\n",
      "Epoch [22/50], Hidden Size: 32, LR: 0.001, Batch Size: 32, Loss: 0.3357\n",
      "Epoch [23/50], Hidden Size: 32, LR: 0.001, Batch Size: 32, Loss: 0.3323\n",
      "Epoch [24/50], Hidden Size: 32, LR: 0.001, Batch Size: 32, Loss: 0.3347\n",
      "Epoch [25/50], Hidden Size: 32, LR: 0.001, Batch Size: 32, Loss: 0.3352\n",
      "Epoch [26/50], Hidden Size: 32, LR: 0.001, Batch Size: 32, Loss: 0.3320\n",
      "Epoch [27/50], Hidden Size: 32, LR: 0.001, Batch Size: 32, Loss: 0.3399\n",
      "Epoch [28/50], Hidden Size: 32, LR: 0.001, Batch Size: 32, Loss: 0.3358\n",
      "Epoch [29/50], Hidden Size: 32, LR: 0.001, Batch Size: 32, Loss: 0.3358\n",
      "Epoch [30/50], Hidden Size: 32, LR: 0.001, Batch Size: 32, Loss: 0.3355\n",
      "Epoch [31/50], Hidden Size: 32, LR: 0.001, Batch Size: 32, Loss: 0.3355\n",
      "Epoch [32/50], Hidden Size: 32, LR: 0.001, Batch Size: 32, Loss: 0.3365\n",
      "Epoch [33/50], Hidden Size: 32, LR: 0.001, Batch Size: 32, Loss: 0.3341\n",
      "Epoch [34/50], Hidden Size: 32, LR: 0.001, Batch Size: 32, Loss: 0.3360\n",
      "Epoch [35/50], Hidden Size: 32, LR: 0.001, Batch Size: 32, Loss: 0.3329\n",
      "Epoch [36/50], Hidden Size: 32, LR: 0.001, Batch Size: 32, Loss: 0.3341\n",
      "Epoch [37/50], Hidden Size: 32, LR: 0.001, Batch Size: 32, Loss: 0.3352\n",
      "Epoch [38/50], Hidden Size: 32, LR: 0.001, Batch Size: 32, Loss: 0.3368\n",
      "Epoch [39/50], Hidden Size: 32, LR: 0.001, Batch Size: 32, Loss: 0.3355\n",
      "Epoch [40/50], Hidden Size: 32, LR: 0.001, Batch Size: 32, Loss: 0.3350\n",
      "Epoch [41/50], Hidden Size: 32, LR: 0.001, Batch Size: 32, Loss: 0.3351\n",
      "Epoch [42/50], Hidden Size: 32, LR: 0.001, Batch Size: 32, Loss: 0.3344\n",
      "Epoch [43/50], Hidden Size: 32, LR: 0.001, Batch Size: 32, Loss: 0.3359\n",
      "Epoch [44/50], Hidden Size: 32, LR: 0.001, Batch Size: 32, Loss: 0.3335\n",
      "Epoch [45/50], Hidden Size: 32, LR: 0.001, Batch Size: 32, Loss: 0.3344\n",
      "Epoch [46/50], Hidden Size: 32, LR: 0.001, Batch Size: 32, Loss: 0.3383\n",
      "Epoch [47/50], Hidden Size: 32, LR: 0.001, Batch Size: 32, Loss: 0.3371\n",
      "Epoch [48/50], Hidden Size: 32, LR: 0.001, Batch Size: 32, Loss: 0.3343\n",
      "Epoch [49/50], Hidden Size: 32, LR: 0.001, Batch Size: 32, Loss: 0.3344\n",
      "Epoch [50/50], Hidden Size: 32, LR: 0.001, Batch Size: 32, Loss: 0.3374\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\10374\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torchvision\\models\\_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "c:\\Users\\10374\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torchvision\\models\\_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=None`.\n",
      "  warnings.warn(msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/50], Hidden Size: 32, LR: 0.001, Batch Size: 64, Loss: 0.9941\n",
      "Epoch [2/50], Hidden Size: 32, LR: 0.001, Batch Size: 64, Loss: 0.9525\n",
      "Epoch [3/50], Hidden Size: 32, LR: 0.001, Batch Size: 64, Loss: 0.8561\n",
      "Epoch [4/50], Hidden Size: 32, LR: 0.001, Batch Size: 64, Loss: 0.6962\n",
      "Epoch [5/50], Hidden Size: 32, LR: 0.001, Batch Size: 64, Loss: 0.4681\n",
      "Epoch [6/50], Hidden Size: 32, LR: 0.001, Batch Size: 64, Loss: 0.3543\n",
      "Epoch [7/50], Hidden Size: 32, LR: 0.001, Batch Size: 64, Loss: 0.3626\n",
      "Epoch [8/50], Hidden Size: 32, LR: 0.001, Batch Size: 64, Loss: 0.3457\n",
      "Epoch [9/50], Hidden Size: 32, LR: 0.001, Batch Size: 64, Loss: 0.3332\n",
      "Epoch [10/50], Hidden Size: 32, LR: 0.001, Batch Size: 64, Loss: 0.3326\n",
      "Epoch [11/50], Hidden Size: 32, LR: 0.001, Batch Size: 64, Loss: 0.3329\n",
      "Epoch [12/50], Hidden Size: 32, LR: 0.001, Batch Size: 64, Loss: 0.3283\n",
      "Epoch [13/50], Hidden Size: 32, LR: 0.001, Batch Size: 64, Loss: 0.3295\n",
      "Epoch [14/50], Hidden Size: 32, LR: 0.001, Batch Size: 64, Loss: 0.3376\n",
      "Epoch [15/50], Hidden Size: 32, LR: 0.001, Batch Size: 64, Loss: 0.3413\n",
      "Epoch [16/50], Hidden Size: 32, LR: 0.001, Batch Size: 64, Loss: 0.3341\n",
      "Epoch [17/50], Hidden Size: 32, LR: 0.001, Batch Size: 64, Loss: 0.3323\n",
      "Epoch [18/50], Hidden Size: 32, LR: 0.001, Batch Size: 64, Loss: 0.3292\n",
      "Epoch [19/50], Hidden Size: 32, LR: 0.001, Batch Size: 64, Loss: 0.3305\n",
      "Epoch [20/50], Hidden Size: 32, LR: 0.001, Batch Size: 64, Loss: 0.3368\n",
      "Epoch [21/50], Hidden Size: 32, LR: 0.001, Batch Size: 64, Loss: 0.3320\n",
      "Epoch [22/50], Hidden Size: 32, LR: 0.001, Batch Size: 64, Loss: 0.3265\n",
      "Epoch [23/50], Hidden Size: 32, LR: 0.001, Batch Size: 64, Loss: 0.3422\n",
      "Epoch [24/50], Hidden Size: 32, LR: 0.001, Batch Size: 64, Loss: 0.3371\n",
      "Epoch [25/50], Hidden Size: 32, LR: 0.001, Batch Size: 64, Loss: 0.3366\n",
      "Epoch [26/50], Hidden Size: 32, LR: 0.001, Batch Size: 64, Loss: 0.3360\n",
      "Epoch [27/50], Hidden Size: 32, LR: 0.001, Batch Size: 64, Loss: 0.3357\n",
      "Epoch [28/50], Hidden Size: 32, LR: 0.001, Batch Size: 64, Loss: 0.3393\n",
      "Epoch [29/50], Hidden Size: 32, LR: 0.001, Batch Size: 64, Loss: 0.3292\n",
      "Epoch [30/50], Hidden Size: 32, LR: 0.001, Batch Size: 64, Loss: 0.3334\n",
      "Epoch [31/50], Hidden Size: 32, LR: 0.001, Batch Size: 64, Loss: 0.3292\n",
      "Epoch [32/50], Hidden Size: 32, LR: 0.001, Batch Size: 64, Loss: 0.3377\n",
      "Epoch [33/50], Hidden Size: 32, LR: 0.001, Batch Size: 64, Loss: 0.3275\n",
      "Epoch [34/50], Hidden Size: 32, LR: 0.001, Batch Size: 64, Loss: 0.3258\n",
      "Epoch [35/50], Hidden Size: 32, LR: 0.001, Batch Size: 64, Loss: 0.3324\n",
      "Epoch [36/50], Hidden Size: 32, LR: 0.001, Batch Size: 64, Loss: 0.3393\n",
      "Epoch [37/50], Hidden Size: 32, LR: 0.001, Batch Size: 64, Loss: 0.3418\n",
      "Epoch [38/50], Hidden Size: 32, LR: 0.001, Batch Size: 64, Loss: 0.3280\n",
      "Epoch [39/50], Hidden Size: 32, LR: 0.001, Batch Size: 64, Loss: 0.3350\n",
      "Epoch [40/50], Hidden Size: 32, LR: 0.001, Batch Size: 64, Loss: 0.3372\n",
      "Epoch [41/50], Hidden Size: 32, LR: 0.001, Batch Size: 64, Loss: 0.3364\n",
      "Epoch [42/50], Hidden Size: 32, LR: 0.001, Batch Size: 64, Loss: 0.3291\n",
      "Epoch [43/50], Hidden Size: 32, LR: 0.001, Batch Size: 64, Loss: 0.3365\n",
      "Epoch [44/50], Hidden Size: 32, LR: 0.001, Batch Size: 64, Loss: 0.3394\n",
      "Epoch [45/50], Hidden Size: 32, LR: 0.001, Batch Size: 64, Loss: 0.3305\n",
      "Epoch [46/50], Hidden Size: 32, LR: 0.001, Batch Size: 64, Loss: 0.3337\n",
      "Epoch [47/50], Hidden Size: 32, LR: 0.001, Batch Size: 64, Loss: 0.3331\n",
      "Epoch [48/50], Hidden Size: 32, LR: 0.001, Batch Size: 64, Loss: 0.3376\n",
      "Epoch [49/50], Hidden Size: 32, LR: 0.001, Batch Size: 64, Loss: 0.3395\n",
      "Epoch [50/50], Hidden Size: 32, LR: 0.001, Batch Size: 64, Loss: 0.3345\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\10374\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torchvision\\models\\_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "c:\\Users\\10374\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torchvision\\models\\_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=None`.\n",
      "  warnings.warn(msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/50], Hidden Size: 32, LR: 0.0001, Batch Size: 16, Loss: 0.9978\n",
      "Epoch [2/50], Hidden Size: 32, LR: 0.0001, Batch Size: 16, Loss: 0.9885\n",
      "Epoch [3/50], Hidden Size: 32, LR: 0.0001, Batch Size: 16, Loss: 0.9674\n",
      "Epoch [4/50], Hidden Size: 32, LR: 0.0001, Batch Size: 16, Loss: 0.9276\n",
      "Epoch [5/50], Hidden Size: 32, LR: 0.0001, Batch Size: 16, Loss: 0.8711\n",
      "Epoch [6/50], Hidden Size: 32, LR: 0.0001, Batch Size: 16, Loss: 0.7948\n",
      "Epoch [7/50], Hidden Size: 32, LR: 0.0001, Batch Size: 16, Loss: 0.7010\n",
      "Epoch [8/50], Hidden Size: 32, LR: 0.0001, Batch Size: 16, Loss: 0.5892\n",
      "Epoch [9/50], Hidden Size: 32, LR: 0.0001, Batch Size: 16, Loss: 0.4734\n",
      "Epoch [10/50], Hidden Size: 32, LR: 0.0001, Batch Size: 16, Loss: 0.3713\n",
      "Epoch [11/50], Hidden Size: 32, LR: 0.0001, Batch Size: 16, Loss: 0.3352\n",
      "Epoch [12/50], Hidden Size: 32, LR: 0.0001, Batch Size: 16, Loss: 0.3304\n",
      "Epoch [13/50], Hidden Size: 32, LR: 0.0001, Batch Size: 16, Loss: 0.3317\n",
      "Epoch [14/50], Hidden Size: 32, LR: 0.0001, Batch Size: 16, Loss: 0.3310\n",
      "Epoch [15/50], Hidden Size: 32, LR: 0.0001, Batch Size: 16, Loss: 0.3334\n",
      "Epoch [16/50], Hidden Size: 32, LR: 0.0001, Batch Size: 16, Loss: 0.3318\n",
      "Epoch [17/50], Hidden Size: 32, LR: 0.0001, Batch Size: 16, Loss: 0.3321\n",
      "Epoch [18/50], Hidden Size: 32, LR: 0.0001, Batch Size: 16, Loss: 0.3345\n",
      "Epoch [19/50], Hidden Size: 32, LR: 0.0001, Batch Size: 16, Loss: 0.3341\n",
      "Epoch [20/50], Hidden Size: 32, LR: 0.0001, Batch Size: 16, Loss: 0.3321\n",
      "Epoch [21/50], Hidden Size: 32, LR: 0.0001, Batch Size: 16, Loss: 0.3305\n",
      "Epoch [22/50], Hidden Size: 32, LR: 0.0001, Batch Size: 16, Loss: 0.3322\n",
      "Epoch [23/50], Hidden Size: 32, LR: 0.0001, Batch Size: 16, Loss: 0.3330\n",
      "Epoch [24/50], Hidden Size: 32, LR: 0.0001, Batch Size: 16, Loss: 0.3275\n",
      "Epoch [25/50], Hidden Size: 32, LR: 0.0001, Batch Size: 16, Loss: 0.3323\n",
      "Epoch [26/50], Hidden Size: 32, LR: 0.0001, Batch Size: 16, Loss: 0.3328\n",
      "Epoch [27/50], Hidden Size: 32, LR: 0.0001, Batch Size: 16, Loss: 0.3315\n",
      "Epoch [28/50], Hidden Size: 32, LR: 0.0001, Batch Size: 16, Loss: 0.3303\n",
      "Epoch [29/50], Hidden Size: 32, LR: 0.0001, Batch Size: 16, Loss: 0.3313\n",
      "Epoch [30/50], Hidden Size: 32, LR: 0.0001, Batch Size: 16, Loss: 0.3294\n",
      "Epoch [31/50], Hidden Size: 32, LR: 0.0001, Batch Size: 16, Loss: 0.3317\n",
      "Epoch [32/50], Hidden Size: 32, LR: 0.0001, Batch Size: 16, Loss: 0.3329\n",
      "Epoch [33/50], Hidden Size: 32, LR: 0.0001, Batch Size: 16, Loss: 0.3304\n",
      "Epoch [34/50], Hidden Size: 32, LR: 0.0001, Batch Size: 16, Loss: 0.3301\n",
      "Epoch [35/50], Hidden Size: 32, LR: 0.0001, Batch Size: 16, Loss: 0.3317\n",
      "Epoch [36/50], Hidden Size: 32, LR: 0.0001, Batch Size: 16, Loss: 0.3315\n",
      "Epoch [37/50], Hidden Size: 32, LR: 0.0001, Batch Size: 16, Loss: 0.3332\n",
      "Epoch [38/50], Hidden Size: 32, LR: 0.0001, Batch Size: 16, Loss: 0.3309\n",
      "Epoch [39/50], Hidden Size: 32, LR: 0.0001, Batch Size: 16, Loss: 0.3292\n",
      "Epoch [40/50], Hidden Size: 32, LR: 0.0001, Batch Size: 16, Loss: 0.3313\n",
      "Epoch [41/50], Hidden Size: 32, LR: 0.0001, Batch Size: 16, Loss: 0.3322\n",
      "Epoch [42/50], Hidden Size: 32, LR: 0.0001, Batch Size: 16, Loss: 0.3325\n",
      "Epoch [43/50], Hidden Size: 32, LR: 0.0001, Batch Size: 16, Loss: 0.3343\n",
      "Epoch [44/50], Hidden Size: 32, LR: 0.0001, Batch Size: 16, Loss: 0.3354\n",
      "Epoch [45/50], Hidden Size: 32, LR: 0.0001, Batch Size: 16, Loss: 0.3340\n",
      "Epoch [46/50], Hidden Size: 32, LR: 0.0001, Batch Size: 16, Loss: 0.3288\n",
      "Epoch [47/50], Hidden Size: 32, LR: 0.0001, Batch Size: 16, Loss: 0.3288\n",
      "Epoch [48/50], Hidden Size: 32, LR: 0.0001, Batch Size: 16, Loss: 0.3311\n",
      "Epoch [49/50], Hidden Size: 32, LR: 0.0001, Batch Size: 16, Loss: 0.3321\n",
      "Epoch [50/50], Hidden Size: 32, LR: 0.0001, Batch Size: 16, Loss: 0.3334\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\10374\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torchvision\\models\\_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "c:\\Users\\10374\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torchvision\\models\\_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=None`.\n",
      "  warnings.warn(msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/50], Hidden Size: 32, LR: 0.0001, Batch Size: 32, Loss: 0.9989\n",
      "Epoch [2/50], Hidden Size: 32, LR: 0.0001, Batch Size: 32, Loss: 0.9959\n",
      "Epoch [3/50], Hidden Size: 32, LR: 0.0001, Batch Size: 32, Loss: 0.9910\n",
      "Epoch [4/50], Hidden Size: 32, LR: 0.0001, Batch Size: 32, Loss: 0.9829\n",
      "Epoch [5/50], Hidden Size: 32, LR: 0.0001, Batch Size: 32, Loss: 0.9698\n",
      "Epoch [6/50], Hidden Size: 32, LR: 0.0001, Batch Size: 32, Loss: 0.9510\n",
      "Epoch [7/50], Hidden Size: 32, LR: 0.0001, Batch Size: 32, Loss: 0.9269\n",
      "Epoch [8/50], Hidden Size: 32, LR: 0.0001, Batch Size: 32, Loss: 0.8982\n",
      "Epoch [9/50], Hidden Size: 32, LR: 0.0001, Batch Size: 32, Loss: 0.8629\n",
      "Epoch [10/50], Hidden Size: 32, LR: 0.0001, Batch Size: 32, Loss: 0.8230\n",
      "Epoch [11/50], Hidden Size: 32, LR: 0.0001, Batch Size: 32, Loss: 0.7770\n",
      "Epoch [12/50], Hidden Size: 32, LR: 0.0001, Batch Size: 32, Loss: 0.7239\n",
      "Epoch [13/50], Hidden Size: 32, LR: 0.0001, Batch Size: 32, Loss: 0.6633\n",
      "Epoch [14/50], Hidden Size: 32, LR: 0.0001, Batch Size: 32, Loss: 0.6020\n",
      "Epoch [15/50], Hidden Size: 32, LR: 0.0001, Batch Size: 32, Loss: 0.5361\n",
      "Epoch [16/50], Hidden Size: 32, LR: 0.0001, Batch Size: 32, Loss: 0.4688\n",
      "Epoch [17/50], Hidden Size: 32, LR: 0.0001, Batch Size: 32, Loss: 0.4104\n",
      "Epoch [18/50], Hidden Size: 32, LR: 0.0001, Batch Size: 32, Loss: 0.3641\n",
      "Epoch [19/50], Hidden Size: 32, LR: 0.0001, Batch Size: 32, Loss: 0.3415\n",
      "Epoch [20/50], Hidden Size: 32, LR: 0.0001, Batch Size: 32, Loss: 0.3351\n",
      "Epoch [21/50], Hidden Size: 32, LR: 0.0001, Batch Size: 32, Loss: 0.3339\n",
      "Epoch [22/50], Hidden Size: 32, LR: 0.0001, Batch Size: 32, Loss: 0.3320\n",
      "Epoch [23/50], Hidden Size: 32, LR: 0.0001, Batch Size: 32, Loss: 0.3353\n",
      "Epoch [24/50], Hidden Size: 32, LR: 0.0001, Batch Size: 32, Loss: 0.3338\n",
      "Epoch [25/50], Hidden Size: 32, LR: 0.0001, Batch Size: 32, Loss: 0.3337\n",
      "Epoch [26/50], Hidden Size: 32, LR: 0.0001, Batch Size: 32, Loss: 0.3344\n",
      "Epoch [27/50], Hidden Size: 32, LR: 0.0001, Batch Size: 32, Loss: 0.3344\n",
      "Epoch [28/50], Hidden Size: 32, LR: 0.0001, Batch Size: 32, Loss: 0.3334\n",
      "Epoch [29/50], Hidden Size: 32, LR: 0.0001, Batch Size: 32, Loss: 0.3336\n",
      "Epoch [30/50], Hidden Size: 32, LR: 0.0001, Batch Size: 32, Loss: 0.3330\n",
      "Epoch [31/50], Hidden Size: 32, LR: 0.0001, Batch Size: 32, Loss: 0.3334\n",
      "Epoch [32/50], Hidden Size: 32, LR: 0.0001, Batch Size: 32, Loss: 0.3342\n",
      "Epoch [33/50], Hidden Size: 32, LR: 0.0001, Batch Size: 32, Loss: 0.3337\n",
      "Epoch [34/50], Hidden Size: 32, LR: 0.0001, Batch Size: 32, Loss: 0.3344\n",
      "Epoch [35/50], Hidden Size: 32, LR: 0.0001, Batch Size: 32, Loss: 0.3348\n",
      "Epoch [36/50], Hidden Size: 32, LR: 0.0001, Batch Size: 32, Loss: 0.3335\n",
      "Epoch [37/50], Hidden Size: 32, LR: 0.0001, Batch Size: 32, Loss: 0.3339\n",
      "Epoch [38/50], Hidden Size: 32, LR: 0.0001, Batch Size: 32, Loss: 0.3335\n",
      "Epoch [39/50], Hidden Size: 32, LR: 0.0001, Batch Size: 32, Loss: 0.3335\n",
      "Epoch [40/50], Hidden Size: 32, LR: 0.0001, Batch Size: 32, Loss: 0.3336\n",
      "Epoch [41/50], Hidden Size: 32, LR: 0.0001, Batch Size: 32, Loss: 0.3331\n",
      "Epoch [42/50], Hidden Size: 32, LR: 0.0001, Batch Size: 32, Loss: 0.3345\n",
      "Epoch [43/50], Hidden Size: 32, LR: 0.0001, Batch Size: 32, Loss: 0.3345\n",
      "Epoch [44/50], Hidden Size: 32, LR: 0.0001, Batch Size: 32, Loss: 0.3353\n",
      "Epoch [45/50], Hidden Size: 32, LR: 0.0001, Batch Size: 32, Loss: 0.3335\n",
      "Epoch [46/50], Hidden Size: 32, LR: 0.0001, Batch Size: 32, Loss: 0.3318\n",
      "Epoch [47/50], Hidden Size: 32, LR: 0.0001, Batch Size: 32, Loss: 0.3339\n",
      "Epoch [48/50], Hidden Size: 32, LR: 0.0001, Batch Size: 32, Loss: 0.3345\n",
      "Epoch [49/50], Hidden Size: 32, LR: 0.0001, Batch Size: 32, Loss: 0.3348\n",
      "Epoch [50/50], Hidden Size: 32, LR: 0.0001, Batch Size: 32, Loss: 0.3338\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\10374\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torchvision\\models\\_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "c:\\Users\\10374\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torchvision\\models\\_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=None`.\n",
      "  warnings.warn(msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/50], Hidden Size: 32, LR: 0.0001, Batch Size: 64, Loss: 0.9996\n",
      "Epoch [2/50], Hidden Size: 32, LR: 0.0001, Batch Size: 64, Loss: 0.9984\n",
      "Epoch [3/50], Hidden Size: 32, LR: 0.0001, Batch Size: 64, Loss: 0.9967\n",
      "Epoch [4/50], Hidden Size: 32, LR: 0.0001, Batch Size: 64, Loss: 0.9947\n",
      "Epoch [5/50], Hidden Size: 32, LR: 0.0001, Batch Size: 64, Loss: 0.9922\n",
      "Epoch [6/50], Hidden Size: 32, LR: 0.0001, Batch Size: 64, Loss: 0.9883\n",
      "Epoch [7/50], Hidden Size: 32, LR: 0.0001, Batch Size: 64, Loss: 0.9835\n",
      "Epoch [8/50], Hidden Size: 32, LR: 0.0001, Batch Size: 64, Loss: 0.9775\n",
      "Epoch [9/50], Hidden Size: 32, LR: 0.0001, Batch Size: 64, Loss: 0.9700\n",
      "Epoch [10/50], Hidden Size: 32, LR: 0.0001, Batch Size: 64, Loss: 0.9622\n",
      "Epoch [11/50], Hidden Size: 32, LR: 0.0001, Batch Size: 64, Loss: 0.9522\n",
      "Epoch [12/50], Hidden Size: 32, LR: 0.0001, Batch Size: 64, Loss: 0.9425\n",
      "Epoch [13/50], Hidden Size: 32, LR: 0.0001, Batch Size: 64, Loss: 0.9293\n",
      "Epoch [14/50], Hidden Size: 32, LR: 0.0001, Batch Size: 64, Loss: 0.9172\n",
      "Epoch [15/50], Hidden Size: 32, LR: 0.0001, Batch Size: 64, Loss: 0.9033\n",
      "Epoch [16/50], Hidden Size: 32, LR: 0.0001, Batch Size: 64, Loss: 0.8867\n",
      "Epoch [17/50], Hidden Size: 32, LR: 0.0001, Batch Size: 64, Loss: 0.8707\n",
      "Epoch [18/50], Hidden Size: 32, LR: 0.0001, Batch Size: 64, Loss: 0.8536\n",
      "Epoch [19/50], Hidden Size: 32, LR: 0.0001, Batch Size: 64, Loss: 0.8324\n",
      "Epoch [20/50], Hidden Size: 32, LR: 0.0001, Batch Size: 64, Loss: 0.8087\n",
      "Epoch [21/50], Hidden Size: 32, LR: 0.0001, Batch Size: 64, Loss: 0.7892\n",
      "Epoch [22/50], Hidden Size: 32, LR: 0.0001, Batch Size: 64, Loss: 0.7627\n",
      "Epoch [23/50], Hidden Size: 32, LR: 0.0001, Batch Size: 64, Loss: 0.7345\n",
      "Epoch [24/50], Hidden Size: 32, LR: 0.0001, Batch Size: 64, Loss: 0.7034\n",
      "Epoch [25/50], Hidden Size: 32, LR: 0.0001, Batch Size: 64, Loss: 0.6734\n",
      "Epoch [26/50], Hidden Size: 32, LR: 0.0001, Batch Size: 64, Loss: 0.6410\n",
      "Epoch [27/50], Hidden Size: 32, LR: 0.0001, Batch Size: 64, Loss: 0.6092\n",
      "Epoch [28/50], Hidden Size: 32, LR: 0.0001, Batch Size: 64, Loss: 0.5705\n",
      "Epoch [29/50], Hidden Size: 32, LR: 0.0001, Batch Size: 64, Loss: 0.5417\n",
      "Epoch [30/50], Hidden Size: 32, LR: 0.0001, Batch Size: 64, Loss: 0.5047\n",
      "Epoch [31/50], Hidden Size: 32, LR: 0.0001, Batch Size: 64, Loss: 0.4739\n",
      "Epoch [32/50], Hidden Size: 32, LR: 0.0001, Batch Size: 64, Loss: 0.4467\n",
      "Epoch [33/50], Hidden Size: 32, LR: 0.0001, Batch Size: 64, Loss: 0.4097\n",
      "Epoch [34/50], Hidden Size: 32, LR: 0.0001, Batch Size: 64, Loss: 0.3937\n",
      "Epoch [35/50], Hidden Size: 32, LR: 0.0001, Batch Size: 64, Loss: 0.3575\n",
      "Epoch [36/50], Hidden Size: 32, LR: 0.0001, Batch Size: 64, Loss: 0.3450\n",
      "Epoch [37/50], Hidden Size: 32, LR: 0.0001, Batch Size: 64, Loss: 0.3343\n",
      "Epoch [38/50], Hidden Size: 32, LR: 0.0001, Batch Size: 64, Loss: 0.3371\n",
      "Epoch [39/50], Hidden Size: 32, LR: 0.0001, Batch Size: 64, Loss: 0.3371\n",
      "Epoch [40/50], Hidden Size: 32, LR: 0.0001, Batch Size: 64, Loss: 0.3376\n",
      "Epoch [41/50], Hidden Size: 32, LR: 0.0001, Batch Size: 64, Loss: 0.3401\n",
      "Epoch [42/50], Hidden Size: 32, LR: 0.0001, Batch Size: 64, Loss: 0.3332\n",
      "Epoch [43/50], Hidden Size: 32, LR: 0.0001, Batch Size: 64, Loss: 0.3312\n",
      "Epoch [44/50], Hidden Size: 32, LR: 0.0001, Batch Size: 64, Loss: 0.3353\n",
      "Epoch [45/50], Hidden Size: 32, LR: 0.0001, Batch Size: 64, Loss: 0.3274\n",
      "Epoch [46/50], Hidden Size: 32, LR: 0.0001, Batch Size: 64, Loss: 0.3356\n",
      "Epoch [47/50], Hidden Size: 32, LR: 0.0001, Batch Size: 64, Loss: 0.3419\n",
      "Epoch [48/50], Hidden Size: 32, LR: 0.0001, Batch Size: 64, Loss: 0.3310\n",
      "Epoch [49/50], Hidden Size: 32, LR: 0.0001, Batch Size: 64, Loss: 0.3316\n",
      "Epoch [50/50], Hidden Size: 32, LR: 0.0001, Batch Size: 64, Loss: 0.3376\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\10374\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torchvision\\models\\_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "c:\\Users\\10374\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torchvision\\models\\_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=None`.\n",
      "  warnings.warn(msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/50], Hidden Size: 64, LR: 0.001, Batch Size: 16, Loss: 0.9390\n",
      "Epoch [2/50], Hidden Size: 64, LR: 0.001, Batch Size: 16, Loss: 0.4934\n",
      "Epoch [3/50], Hidden Size: 64, LR: 0.001, Batch Size: 16, Loss: 0.3472\n",
      "Epoch [4/50], Hidden Size: 64, LR: 0.001, Batch Size: 16, Loss: 0.3361\n",
      "Epoch [5/50], Hidden Size: 64, LR: 0.001, Batch Size: 16, Loss: 0.3359\n",
      "Epoch [6/50], Hidden Size: 64, LR: 0.001, Batch Size: 16, Loss: 0.3303\n",
      "Epoch [7/50], Hidden Size: 64, LR: 0.001, Batch Size: 16, Loss: 0.3345\n",
      "Epoch [8/50], Hidden Size: 64, LR: 0.001, Batch Size: 16, Loss: 0.3361\n",
      "Epoch [9/50], Hidden Size: 64, LR: 0.001, Batch Size: 16, Loss: 0.3379\n",
      "Epoch [10/50], Hidden Size: 64, LR: 0.001, Batch Size: 16, Loss: 0.3355\n",
      "Epoch [11/50], Hidden Size: 64, LR: 0.001, Batch Size: 16, Loss: 0.3352\n",
      "Epoch [12/50], Hidden Size: 64, LR: 0.001, Batch Size: 16, Loss: 0.3319\n",
      "Epoch [13/50], Hidden Size: 64, LR: 0.001, Batch Size: 16, Loss: 0.3336\n",
      "Epoch [14/50], Hidden Size: 64, LR: 0.001, Batch Size: 16, Loss: 0.3379\n",
      "Epoch [15/50], Hidden Size: 64, LR: 0.001, Batch Size: 16, Loss: 0.3329\n",
      "Epoch [16/50], Hidden Size: 64, LR: 0.001, Batch Size: 16, Loss: 0.3319\n",
      "Epoch [17/50], Hidden Size: 64, LR: 0.001, Batch Size: 16, Loss: 0.3376\n",
      "Epoch [18/50], Hidden Size: 64, LR: 0.001, Batch Size: 16, Loss: 0.3336\n",
      "Epoch [19/50], Hidden Size: 64, LR: 0.001, Batch Size: 16, Loss: 0.3333\n",
      "Epoch [20/50], Hidden Size: 64, LR: 0.001, Batch Size: 16, Loss: 0.3348\n",
      "Epoch [21/50], Hidden Size: 64, LR: 0.001, Batch Size: 16, Loss: 0.3366\n",
      "Epoch [22/50], Hidden Size: 64, LR: 0.001, Batch Size: 16, Loss: 0.3342\n",
      "Epoch [23/50], Hidden Size: 64, LR: 0.001, Batch Size: 16, Loss: 0.3336\n",
      "Epoch [24/50], Hidden Size: 64, LR: 0.001, Batch Size: 16, Loss: 0.3362\n",
      "Epoch [25/50], Hidden Size: 64, LR: 0.001, Batch Size: 16, Loss: 0.3366\n",
      "Epoch [26/50], Hidden Size: 64, LR: 0.001, Batch Size: 16, Loss: 0.3336\n",
      "Epoch [27/50], Hidden Size: 64, LR: 0.001, Batch Size: 16, Loss: 0.3357\n",
      "Epoch [28/50], Hidden Size: 64, LR: 0.001, Batch Size: 16, Loss: 0.3415\n",
      "Epoch [29/50], Hidden Size: 64, LR: 0.001, Batch Size: 16, Loss: 0.3387\n",
      "Epoch [30/50], Hidden Size: 64, LR: 0.001, Batch Size: 16, Loss: 0.3326\n",
      "Epoch [31/50], Hidden Size: 64, LR: 0.001, Batch Size: 16, Loss: 0.3325\n",
      "Epoch [32/50], Hidden Size: 64, LR: 0.001, Batch Size: 16, Loss: 0.3318\n",
      "Epoch [33/50], Hidden Size: 64, LR: 0.001, Batch Size: 16, Loss: 0.3425\n",
      "Epoch [34/50], Hidden Size: 64, LR: 0.001, Batch Size: 16, Loss: 0.3383\n",
      "Epoch [35/50], Hidden Size: 64, LR: 0.001, Batch Size: 16, Loss: 0.3340\n",
      "Epoch [36/50], Hidden Size: 64, LR: 0.001, Batch Size: 16, Loss: 0.3343\n",
      "Epoch [37/50], Hidden Size: 64, LR: 0.001, Batch Size: 16, Loss: 0.3338\n",
      "Epoch [38/50], Hidden Size: 64, LR: 0.001, Batch Size: 16, Loss: 0.3342\n",
      "Epoch [39/50], Hidden Size: 64, LR: 0.001, Batch Size: 16, Loss: 0.3340\n",
      "Epoch [40/50], Hidden Size: 64, LR: 0.001, Batch Size: 16, Loss: 0.3347\n",
      "Epoch [41/50], Hidden Size: 64, LR: 0.001, Batch Size: 16, Loss: 0.3360\n",
      "Epoch [42/50], Hidden Size: 64, LR: 0.001, Batch Size: 16, Loss: 0.3319\n",
      "Epoch [43/50], Hidden Size: 64, LR: 0.001, Batch Size: 16, Loss: 0.3356\n",
      "Epoch [44/50], Hidden Size: 64, LR: 0.001, Batch Size: 16, Loss: 0.3332\n",
      "Epoch [45/50], Hidden Size: 64, LR: 0.001, Batch Size: 16, Loss: 0.3358\n",
      "Epoch [46/50], Hidden Size: 64, LR: 0.001, Batch Size: 16, Loss: 0.3375\n",
      "Epoch [47/50], Hidden Size: 64, LR: 0.001, Batch Size: 16, Loss: 0.3341\n",
      "Epoch [48/50], Hidden Size: 64, LR: 0.001, Batch Size: 16, Loss: 0.3387\n",
      "Epoch [49/50], Hidden Size: 64, LR: 0.001, Batch Size: 16, Loss: 0.3340\n",
      "Epoch [50/50], Hidden Size: 64, LR: 0.001, Batch Size: 16, Loss: 0.3350\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\10374\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torchvision\\models\\_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "c:\\Users\\10374\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torchvision\\models\\_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=None`.\n",
      "  warnings.warn(msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/50], Hidden Size: 64, LR: 0.001, Batch Size: 32, Loss: 0.9739\n",
      "Epoch [2/50], Hidden Size: 64, LR: 0.001, Batch Size: 32, Loss: 0.7999\n",
      "Epoch [3/50], Hidden Size: 64, LR: 0.001, Batch Size: 32, Loss: 0.4388\n",
      "Epoch [4/50], Hidden Size: 64, LR: 0.001, Batch Size: 32, Loss: 0.3716\n",
      "Epoch [5/50], Hidden Size: 64, LR: 0.001, Batch Size: 32, Loss: 0.3346\n",
      "Epoch [6/50], Hidden Size: 64, LR: 0.001, Batch Size: 32, Loss: 0.3337\n",
      "Epoch [7/50], Hidden Size: 64, LR: 0.001, Batch Size: 32, Loss: 0.3337\n",
      "Epoch [8/50], Hidden Size: 64, LR: 0.001, Batch Size: 32, Loss: 0.3351\n",
      "Epoch [9/50], Hidden Size: 64, LR: 0.001, Batch Size: 32, Loss: 0.3353\n",
      "Epoch [10/50], Hidden Size: 64, LR: 0.001, Batch Size: 32, Loss: 0.3346\n",
      "Epoch [11/50], Hidden Size: 64, LR: 0.001, Batch Size: 32, Loss: 0.3348\n",
      "Epoch [12/50], Hidden Size: 64, LR: 0.001, Batch Size: 32, Loss: 0.3329\n",
      "Epoch [13/50], Hidden Size: 64, LR: 0.001, Batch Size: 32, Loss: 0.3343\n",
      "Epoch [14/50], Hidden Size: 64, LR: 0.001, Batch Size: 32, Loss: 0.3342\n",
      "Epoch [15/50], Hidden Size: 64, LR: 0.001, Batch Size: 32, Loss: 0.3347\n",
      "Epoch [16/50], Hidden Size: 64, LR: 0.001, Batch Size: 32, Loss: 0.3315\n",
      "Epoch [17/50], Hidden Size: 64, LR: 0.001, Batch Size: 32, Loss: 0.3341\n",
      "Epoch [18/50], Hidden Size: 64, LR: 0.001, Batch Size: 32, Loss: 0.3336\n",
      "Epoch [19/50], Hidden Size: 64, LR: 0.001, Batch Size: 32, Loss: 0.3351\n",
      "Epoch [20/50], Hidden Size: 64, LR: 0.001, Batch Size: 32, Loss: 0.3339\n",
      "Epoch [21/50], Hidden Size: 64, LR: 0.001, Batch Size: 32, Loss: 0.3347\n",
      "Epoch [22/50], Hidden Size: 64, LR: 0.001, Batch Size: 32, Loss: 0.3316\n",
      "Epoch [23/50], Hidden Size: 64, LR: 0.001, Batch Size: 32, Loss: 0.3340\n",
      "Epoch [24/50], Hidden Size: 64, LR: 0.001, Batch Size: 32, Loss: 0.3334\n",
      "Epoch [25/50], Hidden Size: 64, LR: 0.001, Batch Size: 32, Loss: 0.3346\n",
      "Epoch [26/50], Hidden Size: 64, LR: 0.001, Batch Size: 32, Loss: 0.3348\n",
      "Epoch [27/50], Hidden Size: 64, LR: 0.001, Batch Size: 32, Loss: 0.3352\n",
      "Epoch [28/50], Hidden Size: 64, LR: 0.001, Batch Size: 32, Loss: 0.3385\n",
      "Epoch [29/50], Hidden Size: 64, LR: 0.001, Batch Size: 32, Loss: 0.3352\n",
      "Epoch [30/50], Hidden Size: 64, LR: 0.001, Batch Size: 32, Loss: 0.3354\n",
      "Epoch [31/50], Hidden Size: 64, LR: 0.001, Batch Size: 32, Loss: 0.3348\n",
      "Epoch [32/50], Hidden Size: 64, LR: 0.001, Batch Size: 32, Loss: 0.3344\n",
      "Epoch [33/50], Hidden Size: 64, LR: 0.001, Batch Size: 32, Loss: 0.3339\n",
      "Epoch [34/50], Hidden Size: 64, LR: 0.001, Batch Size: 32, Loss: 0.3347\n",
      "Epoch [35/50], Hidden Size: 64, LR: 0.001, Batch Size: 32, Loss: 0.3356\n",
      "Epoch [36/50], Hidden Size: 64, LR: 0.001, Batch Size: 32, Loss: 0.3336\n",
      "Epoch [37/50], Hidden Size: 64, LR: 0.001, Batch Size: 32, Loss: 0.3359\n",
      "Epoch [38/50], Hidden Size: 64, LR: 0.001, Batch Size: 32, Loss: 0.3334\n",
      "Epoch [39/50], Hidden Size: 64, LR: 0.001, Batch Size: 32, Loss: 0.3343\n",
      "Epoch [40/50], Hidden Size: 64, LR: 0.001, Batch Size: 32, Loss: 0.3367\n",
      "Epoch [41/50], Hidden Size: 64, LR: 0.001, Batch Size: 32, Loss: 0.3382\n",
      "Epoch [42/50], Hidden Size: 64, LR: 0.001, Batch Size: 32, Loss: 0.3383\n",
      "Epoch [43/50], Hidden Size: 64, LR: 0.001, Batch Size: 32, Loss: 0.3334\n",
      "Epoch [44/50], Hidden Size: 64, LR: 0.001, Batch Size: 32, Loss: 0.3329\n",
      "Epoch [45/50], Hidden Size: 64, LR: 0.001, Batch Size: 32, Loss: 0.3337\n",
      "Epoch [46/50], Hidden Size: 64, LR: 0.001, Batch Size: 32, Loss: 0.3360\n",
      "Epoch [47/50], Hidden Size: 64, LR: 0.001, Batch Size: 32, Loss: 0.3358\n",
      "Epoch [48/50], Hidden Size: 64, LR: 0.001, Batch Size: 32, Loss: 0.3370\n",
      "Epoch [49/50], Hidden Size: 64, LR: 0.001, Batch Size: 32, Loss: 0.3339\n",
      "Epoch [50/50], Hidden Size: 64, LR: 0.001, Batch Size: 32, Loss: 0.3360\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\10374\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torchvision\\models\\_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "c:\\Users\\10374\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torchvision\\models\\_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=None`.\n",
      "  warnings.warn(msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/50], Hidden Size: 64, LR: 0.001, Batch Size: 64, Loss: 0.9931\n",
      "Epoch [2/50], Hidden Size: 64, LR: 0.001, Batch Size: 64, Loss: 0.9493\n",
      "Epoch [3/50], Hidden Size: 64, LR: 0.001, Batch Size: 64, Loss: 0.8533\n",
      "Epoch [4/50], Hidden Size: 64, LR: 0.001, Batch Size: 64, Loss: 0.6944\n",
      "Epoch [5/50], Hidden Size: 64, LR: 0.001, Batch Size: 64, Loss: 0.4712\n",
      "Epoch [6/50], Hidden Size: 64, LR: 0.001, Batch Size: 64, Loss: 0.3578\n",
      "Epoch [7/50], Hidden Size: 64, LR: 0.001, Batch Size: 64, Loss: 0.3724\n",
      "Epoch [8/50], Hidden Size: 64, LR: 0.001, Batch Size: 64, Loss: 0.3385\n",
      "Epoch [9/50], Hidden Size: 64, LR: 0.001, Batch Size: 64, Loss: 0.3372\n",
      "Epoch [10/50], Hidden Size: 64, LR: 0.001, Batch Size: 64, Loss: 0.3371\n",
      "Epoch [11/50], Hidden Size: 64, LR: 0.001, Batch Size: 64, Loss: 0.3374\n",
      "Epoch [12/50], Hidden Size: 64, LR: 0.001, Batch Size: 64, Loss: 0.3393\n",
      "Epoch [13/50], Hidden Size: 64, LR: 0.001, Batch Size: 64, Loss: 0.3280\n",
      "Epoch [14/50], Hidden Size: 64, LR: 0.001, Batch Size: 64, Loss: 0.3341\n",
      "Epoch [15/50], Hidden Size: 64, LR: 0.001, Batch Size: 64, Loss: 0.3371\n",
      "Epoch [16/50], Hidden Size: 64, LR: 0.001, Batch Size: 64, Loss: 0.3274\n",
      "Epoch [17/50], Hidden Size: 64, LR: 0.001, Batch Size: 64, Loss: 0.3319\n",
      "Epoch [18/50], Hidden Size: 64, LR: 0.001, Batch Size: 64, Loss: 0.3343\n",
      "Epoch [19/50], Hidden Size: 64, LR: 0.001, Batch Size: 64, Loss: 0.3343\n",
      "Epoch [20/50], Hidden Size: 64, LR: 0.001, Batch Size: 64, Loss: 0.3373\n",
      "Epoch [21/50], Hidden Size: 64, LR: 0.001, Batch Size: 64, Loss: 0.3307\n",
      "Epoch [22/50], Hidden Size: 64, LR: 0.001, Batch Size: 64, Loss: 0.3356\n",
      "Epoch [23/50], Hidden Size: 64, LR: 0.001, Batch Size: 64, Loss: 0.3364\n",
      "Epoch [24/50], Hidden Size: 64, LR: 0.001, Batch Size: 64, Loss: 0.3353\n",
      "Epoch [25/50], Hidden Size: 64, LR: 0.001, Batch Size: 64, Loss: 0.3393\n",
      "Epoch [26/50], Hidden Size: 64, LR: 0.001, Batch Size: 64, Loss: 0.3421\n",
      "Epoch [27/50], Hidden Size: 64, LR: 0.001, Batch Size: 64, Loss: 0.3317\n",
      "Epoch [28/50], Hidden Size: 64, LR: 0.001, Batch Size: 64, Loss: 0.3377\n",
      "Epoch [29/50], Hidden Size: 64, LR: 0.001, Batch Size: 64, Loss: 0.3365\n",
      "Epoch [30/50], Hidden Size: 64, LR: 0.001, Batch Size: 64, Loss: 0.3314\n",
      "Epoch [31/50], Hidden Size: 64, LR: 0.001, Batch Size: 64, Loss: 0.3426\n",
      "Epoch [32/50], Hidden Size: 64, LR: 0.001, Batch Size: 64, Loss: 0.3371\n",
      "Epoch [33/50], Hidden Size: 64, LR: 0.001, Batch Size: 64, Loss: 0.3385\n",
      "Epoch [34/50], Hidden Size: 64, LR: 0.001, Batch Size: 64, Loss: 0.3340\n",
      "Epoch [35/50], Hidden Size: 64, LR: 0.001, Batch Size: 64, Loss: 0.3360\n",
      "Epoch [36/50], Hidden Size: 64, LR: 0.001, Batch Size: 64, Loss: 0.3345\n",
      "Epoch [37/50], Hidden Size: 64, LR: 0.001, Batch Size: 64, Loss: 0.3432\n",
      "Epoch [38/50], Hidden Size: 64, LR: 0.001, Batch Size: 64, Loss: 0.3389\n",
      "Epoch [39/50], Hidden Size: 64, LR: 0.001, Batch Size: 64, Loss: 0.3313\n",
      "Epoch [40/50], Hidden Size: 64, LR: 0.001, Batch Size: 64, Loss: 0.3391\n",
      "Epoch [41/50], Hidden Size: 64, LR: 0.001, Batch Size: 64, Loss: 0.3344\n",
      "Epoch [42/50], Hidden Size: 64, LR: 0.001, Batch Size: 64, Loss: 0.3299\n",
      "Epoch [43/50], Hidden Size: 64, LR: 0.001, Batch Size: 64, Loss: 0.3291\n",
      "Epoch [44/50], Hidden Size: 64, LR: 0.001, Batch Size: 64, Loss: 0.3339\n",
      "Epoch [45/50], Hidden Size: 64, LR: 0.001, Batch Size: 64, Loss: 0.3356\n",
      "Epoch [46/50], Hidden Size: 64, LR: 0.001, Batch Size: 64, Loss: 0.3275\n",
      "Epoch [47/50], Hidden Size: 64, LR: 0.001, Batch Size: 64, Loss: 0.3325\n",
      "Epoch [48/50], Hidden Size: 64, LR: 0.001, Batch Size: 64, Loss: 0.3380\n",
      "Epoch [49/50], Hidden Size: 64, LR: 0.001, Batch Size: 64, Loss: 0.3347\n",
      "Epoch [50/50], Hidden Size: 64, LR: 0.001, Batch Size: 64, Loss: 0.3368\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\10374\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torchvision\\models\\_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "c:\\Users\\10374\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torchvision\\models\\_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=None`.\n",
      "  warnings.warn(msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/50], Hidden Size: 64, LR: 0.0001, Batch Size: 16, Loss: 0.9978\n",
      "Epoch [2/50], Hidden Size: 64, LR: 0.0001, Batch Size: 16, Loss: 0.9896\n",
      "Epoch [3/50], Hidden Size: 64, LR: 0.0001, Batch Size: 16, Loss: 0.9681\n",
      "Epoch [4/50], Hidden Size: 64, LR: 0.0001, Batch Size: 16, Loss: 0.9271\n",
      "Epoch [5/50], Hidden Size: 64, LR: 0.0001, Batch Size: 16, Loss: 0.8665\n",
      "Epoch [6/50], Hidden Size: 64, LR: 0.0001, Batch Size: 16, Loss: 0.7866\n",
      "Epoch [7/50], Hidden Size: 64, LR: 0.0001, Batch Size: 16, Loss: 0.6877\n",
      "Epoch [8/50], Hidden Size: 64, LR: 0.0001, Batch Size: 16, Loss: 0.5682\n",
      "Epoch [9/50], Hidden Size: 64, LR: 0.0001, Batch Size: 16, Loss: 0.4423\n",
      "Epoch [10/50], Hidden Size: 64, LR: 0.0001, Batch Size: 16, Loss: 0.3537\n",
      "Epoch [11/50], Hidden Size: 64, LR: 0.0001, Batch Size: 16, Loss: 0.3327\n",
      "Epoch [12/50], Hidden Size: 64, LR: 0.0001, Batch Size: 16, Loss: 0.3298\n",
      "Epoch [13/50], Hidden Size: 64, LR: 0.0001, Batch Size: 16, Loss: 0.3310\n",
      "Epoch [14/50], Hidden Size: 64, LR: 0.0001, Batch Size: 16, Loss: 0.3336\n",
      "Epoch [15/50], Hidden Size: 64, LR: 0.0001, Batch Size: 16, Loss: 0.3297\n",
      "Epoch [16/50], Hidden Size: 64, LR: 0.0001, Batch Size: 16, Loss: 0.3336\n",
      "Epoch [17/50], Hidden Size: 64, LR: 0.0001, Batch Size: 16, Loss: 0.3349\n",
      "Epoch [18/50], Hidden Size: 64, LR: 0.0001, Batch Size: 16, Loss: 0.3303\n",
      "Epoch [19/50], Hidden Size: 64, LR: 0.0001, Batch Size: 16, Loss: 0.3287\n",
      "Epoch [20/50], Hidden Size: 64, LR: 0.0001, Batch Size: 16, Loss: 0.3313\n",
      "Epoch [21/50], Hidden Size: 64, LR: 0.0001, Batch Size: 16, Loss: 0.3311\n",
      "Epoch [22/50], Hidden Size: 64, LR: 0.0001, Batch Size: 16, Loss: 0.3328\n",
      "Epoch [23/50], Hidden Size: 64, LR: 0.0001, Batch Size: 16, Loss: 0.3331\n",
      "Epoch [24/50], Hidden Size: 64, LR: 0.0001, Batch Size: 16, Loss: 0.3324\n",
      "Epoch [25/50], Hidden Size: 64, LR: 0.0001, Batch Size: 16, Loss: 0.3321\n",
      "Epoch [26/50], Hidden Size: 64, LR: 0.0001, Batch Size: 16, Loss: 0.3331\n",
      "Epoch [27/50], Hidden Size: 64, LR: 0.0001, Batch Size: 16, Loss: 0.3328\n",
      "Epoch [28/50], Hidden Size: 64, LR: 0.0001, Batch Size: 16, Loss: 0.3325\n",
      "Epoch [29/50], Hidden Size: 64, LR: 0.0001, Batch Size: 16, Loss: 0.3315\n",
      "Epoch [30/50], Hidden Size: 64, LR: 0.0001, Batch Size: 16, Loss: 0.3307\n",
      "Epoch [31/50], Hidden Size: 64, LR: 0.0001, Batch Size: 16, Loss: 0.3315\n",
      "Epoch [32/50], Hidden Size: 64, LR: 0.0001, Batch Size: 16, Loss: 0.3302\n",
      "Epoch [33/50], Hidden Size: 64, LR: 0.0001, Batch Size: 16, Loss: 0.3329\n",
      "Epoch [34/50], Hidden Size: 64, LR: 0.0001, Batch Size: 16, Loss: 0.3340\n",
      "Epoch [35/50], Hidden Size: 64, LR: 0.0001, Batch Size: 16, Loss: 0.3257\n",
      "Epoch [36/50], Hidden Size: 64, LR: 0.0001, Batch Size: 16, Loss: 0.3348\n",
      "Epoch [37/50], Hidden Size: 64, LR: 0.0001, Batch Size: 16, Loss: 0.3328\n",
      "Epoch [38/50], Hidden Size: 64, LR: 0.0001, Batch Size: 16, Loss: 0.3296\n",
      "Epoch [39/50], Hidden Size: 64, LR: 0.0001, Batch Size: 16, Loss: 0.3316\n",
      "Epoch [40/50], Hidden Size: 64, LR: 0.0001, Batch Size: 16, Loss: 0.3297\n",
      "Epoch [41/50], Hidden Size: 64, LR: 0.0001, Batch Size: 16, Loss: 0.3323\n",
      "Epoch [42/50], Hidden Size: 64, LR: 0.0001, Batch Size: 16, Loss: 0.3325\n",
      "Epoch [43/50], Hidden Size: 64, LR: 0.0001, Batch Size: 16, Loss: 0.3319\n",
      "Epoch [44/50], Hidden Size: 64, LR: 0.0001, Batch Size: 16, Loss: 0.3343\n",
      "Epoch [45/50], Hidden Size: 64, LR: 0.0001, Batch Size: 16, Loss: 0.3305\n",
      "Epoch [46/50], Hidden Size: 64, LR: 0.0001, Batch Size: 16, Loss: 0.3311\n",
      "Epoch [47/50], Hidden Size: 64, LR: 0.0001, Batch Size: 16, Loss: 0.3323\n",
      "Epoch [48/50], Hidden Size: 64, LR: 0.0001, Batch Size: 16, Loss: 0.3336\n",
      "Epoch [49/50], Hidden Size: 64, LR: 0.0001, Batch Size: 16, Loss: 0.3300\n",
      "Epoch [50/50], Hidden Size: 64, LR: 0.0001, Batch Size: 16, Loss: 0.3343\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\10374\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torchvision\\models\\_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "c:\\Users\\10374\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torchvision\\models\\_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=None`.\n",
      "  warnings.warn(msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/50], Hidden Size: 64, LR: 0.0001, Batch Size: 32, Loss: 0.9992\n",
      "Epoch [2/50], Hidden Size: 64, LR: 0.0001, Batch Size: 32, Loss: 0.9967\n",
      "Epoch [3/50], Hidden Size: 64, LR: 0.0001, Batch Size: 32, Loss: 0.9928\n",
      "Epoch [4/50], Hidden Size: 64, LR: 0.0001, Batch Size: 32, Loss: 0.9862\n",
      "Epoch [5/50], Hidden Size: 64, LR: 0.0001, Batch Size: 32, Loss: 0.9756\n",
      "Epoch [6/50], Hidden Size: 64, LR: 0.0001, Batch Size: 32, Loss: 0.9605\n",
      "Epoch [7/50], Hidden Size: 64, LR: 0.0001, Batch Size: 32, Loss: 0.9402\n",
      "Epoch [8/50], Hidden Size: 64, LR: 0.0001, Batch Size: 32, Loss: 0.9162\n",
      "Epoch [9/50], Hidden Size: 64, LR: 0.0001, Batch Size: 32, Loss: 0.8876\n",
      "Epoch [10/50], Hidden Size: 64, LR: 0.0001, Batch Size: 32, Loss: 0.8526\n",
      "Epoch [11/50], Hidden Size: 64, LR: 0.0001, Batch Size: 32, Loss: 0.8131\n",
      "Epoch [12/50], Hidden Size: 64, LR: 0.0001, Batch Size: 32, Loss: 0.7685\n",
      "Epoch [13/50], Hidden Size: 64, LR: 0.0001, Batch Size: 32, Loss: 0.7175\n",
      "Epoch [14/50], Hidden Size: 64, LR: 0.0001, Batch Size: 32, Loss: 0.6610\n",
      "Epoch [15/50], Hidden Size: 64, LR: 0.0001, Batch Size: 32, Loss: 0.5992\n",
      "Epoch [16/50], Hidden Size: 64, LR: 0.0001, Batch Size: 32, Loss: 0.5352\n",
      "Epoch [17/50], Hidden Size: 64, LR: 0.0001, Batch Size: 32, Loss: 0.4710\n",
      "Epoch [18/50], Hidden Size: 64, LR: 0.0001, Batch Size: 32, Loss: 0.4102\n",
      "Epoch [19/50], Hidden Size: 64, LR: 0.0001, Batch Size: 32, Loss: 0.3676\n",
      "Epoch [20/50], Hidden Size: 64, LR: 0.0001, Batch Size: 32, Loss: 0.3433\n",
      "Epoch [21/50], Hidden Size: 64, LR: 0.0001, Batch Size: 32, Loss: 0.3328\n",
      "Epoch [22/50], Hidden Size: 64, LR: 0.0001, Batch Size: 32, Loss: 0.3336\n",
      "Epoch [23/50], Hidden Size: 64, LR: 0.0001, Batch Size: 32, Loss: 0.3351\n",
      "Epoch [24/50], Hidden Size: 64, LR: 0.0001, Batch Size: 32, Loss: 0.3331\n",
      "Epoch [25/50], Hidden Size: 64, LR: 0.0001, Batch Size: 32, Loss: 0.3344\n",
      "Epoch [26/50], Hidden Size: 64, LR: 0.0001, Batch Size: 32, Loss: 0.3331\n",
      "Epoch [27/50], Hidden Size: 64, LR: 0.0001, Batch Size: 32, Loss: 0.3327\n",
      "Epoch [28/50], Hidden Size: 64, LR: 0.0001, Batch Size: 32, Loss: 0.3339\n",
      "Epoch [29/50], Hidden Size: 64, LR: 0.0001, Batch Size: 32, Loss: 0.3323\n",
      "Epoch [30/50], Hidden Size: 64, LR: 0.0001, Batch Size: 32, Loss: 0.3347\n",
      "Epoch [31/50], Hidden Size: 64, LR: 0.0001, Batch Size: 32, Loss: 0.3332\n",
      "Epoch [32/50], Hidden Size: 64, LR: 0.0001, Batch Size: 32, Loss: 0.3309\n",
      "Epoch [33/50], Hidden Size: 64, LR: 0.0001, Batch Size: 32, Loss: 0.3331\n",
      "Epoch [34/50], Hidden Size: 64, LR: 0.0001, Batch Size: 32, Loss: 0.3339\n",
      "Epoch [35/50], Hidden Size: 64, LR: 0.0001, Batch Size: 32, Loss: 0.3323\n",
      "Epoch [36/50], Hidden Size: 64, LR: 0.0001, Batch Size: 32, Loss: 0.3344\n",
      "Epoch [37/50], Hidden Size: 64, LR: 0.0001, Batch Size: 32, Loss: 0.3327\n",
      "Epoch [38/50], Hidden Size: 64, LR: 0.0001, Batch Size: 32, Loss: 0.3333\n",
      "Epoch [39/50], Hidden Size: 64, LR: 0.0001, Batch Size: 32, Loss: 0.3337\n",
      "Epoch [40/50], Hidden Size: 64, LR: 0.0001, Batch Size: 32, Loss: 0.3340\n",
      "Epoch [41/50], Hidden Size: 64, LR: 0.0001, Batch Size: 32, Loss: 0.3339\n",
      "Epoch [42/50], Hidden Size: 64, LR: 0.0001, Batch Size: 32, Loss: 0.3326\n",
      "Epoch [43/50], Hidden Size: 64, LR: 0.0001, Batch Size: 32, Loss: 0.3331\n",
      "Epoch [44/50], Hidden Size: 64, LR: 0.0001, Batch Size: 32, Loss: 0.3322\n",
      "Epoch [45/50], Hidden Size: 64, LR: 0.0001, Batch Size: 32, Loss: 0.3344\n",
      "Epoch [46/50], Hidden Size: 64, LR: 0.0001, Batch Size: 32, Loss: 0.3335\n",
      "Epoch [47/50], Hidden Size: 64, LR: 0.0001, Batch Size: 32, Loss: 0.3321\n",
      "Epoch [48/50], Hidden Size: 64, LR: 0.0001, Batch Size: 32, Loss: 0.3330\n",
      "Epoch [49/50], Hidden Size: 64, LR: 0.0001, Batch Size: 32, Loss: 0.3330\n",
      "Epoch [50/50], Hidden Size: 64, LR: 0.0001, Batch Size: 32, Loss: 0.3340\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\10374\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torchvision\\models\\_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "c:\\Users\\10374\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torchvision\\models\\_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=None`.\n",
      "  warnings.warn(msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/50], Hidden Size: 64, LR: 0.0001, Batch Size: 64, Loss: 0.9996\n",
      "Epoch [2/50], Hidden Size: 64, LR: 0.0001, Batch Size: 64, Loss: 0.9980\n",
      "Epoch [3/50], Hidden Size: 64, LR: 0.0001, Batch Size: 64, Loss: 0.9958\n",
      "Epoch [4/50], Hidden Size: 64, LR: 0.0001, Batch Size: 64, Loss: 0.9932\n",
      "Epoch [5/50], Hidden Size: 64, LR: 0.0001, Batch Size: 64, Loss: 0.9898\n",
      "Epoch [6/50], Hidden Size: 64, LR: 0.0001, Batch Size: 64, Loss: 0.9851\n",
      "Epoch [7/50], Hidden Size: 64, LR: 0.0001, Batch Size: 64, Loss: 0.9785\n",
      "Epoch [8/50], Hidden Size: 64, LR: 0.0001, Batch Size: 64, Loss: 0.9711\n",
      "Epoch [9/50], Hidden Size: 64, LR: 0.0001, Batch Size: 64, Loss: 0.9623\n",
      "Epoch [10/50], Hidden Size: 64, LR: 0.0001, Batch Size: 64, Loss: 0.9512\n",
      "Epoch [11/50], Hidden Size: 64, LR: 0.0001, Batch Size: 64, Loss: 0.9400\n",
      "Epoch [12/50], Hidden Size: 64, LR: 0.0001, Batch Size: 64, Loss: 0.9280\n",
      "Epoch [13/50], Hidden Size: 64, LR: 0.0001, Batch Size: 64, Loss: 0.9106\n",
      "Epoch [14/50], Hidden Size: 64, LR: 0.0001, Batch Size: 64, Loss: 0.8961\n",
      "Epoch [15/50], Hidden Size: 64, LR: 0.0001, Batch Size: 64, Loss: 0.8781\n",
      "Epoch [16/50], Hidden Size: 64, LR: 0.0001, Batch Size: 64, Loss: 0.8566\n",
      "Epoch [17/50], Hidden Size: 64, LR: 0.0001, Batch Size: 64, Loss: 0.8349\n",
      "Epoch [18/50], Hidden Size: 64, LR: 0.0001, Batch Size: 64, Loss: 0.8127\n",
      "Epoch [19/50], Hidden Size: 64, LR: 0.0001, Batch Size: 64, Loss: 0.7849\n",
      "Epoch [20/50], Hidden Size: 64, LR: 0.0001, Batch Size: 64, Loss: 0.7577\n",
      "Epoch [21/50], Hidden Size: 64, LR: 0.0001, Batch Size: 64, Loss: 0.7320\n",
      "Epoch [22/50], Hidden Size: 64, LR: 0.0001, Batch Size: 64, Loss: 0.7000\n",
      "Epoch [23/50], Hidden Size: 64, LR: 0.0001, Batch Size: 64, Loss: 0.6645\n",
      "Epoch [24/50], Hidden Size: 64, LR: 0.0001, Batch Size: 64, Loss: 0.6247\n",
      "Epoch [25/50], Hidden Size: 64, LR: 0.0001, Batch Size: 64, Loss: 0.5955\n",
      "Epoch [26/50], Hidden Size: 64, LR: 0.0001, Batch Size: 64, Loss: 0.5590\n",
      "Epoch [27/50], Hidden Size: 64, LR: 0.0001, Batch Size: 64, Loss: 0.5207\n",
      "Epoch [28/50], Hidden Size: 64, LR: 0.0001, Batch Size: 64, Loss: 0.4813\n",
      "Epoch [29/50], Hidden Size: 64, LR: 0.0001, Batch Size: 64, Loss: 0.4499\n",
      "Epoch [30/50], Hidden Size: 64, LR: 0.0001, Batch Size: 64, Loss: 0.4116\n",
      "Epoch [31/50], Hidden Size: 64, LR: 0.0001, Batch Size: 64, Loss: 0.3807\n",
      "Epoch [32/50], Hidden Size: 64, LR: 0.0001, Batch Size: 64, Loss: 0.3672\n",
      "Epoch [33/50], Hidden Size: 64, LR: 0.0001, Batch Size: 64, Loss: 0.3517\n",
      "Epoch [34/50], Hidden Size: 64, LR: 0.0001, Batch Size: 64, Loss: 0.3392\n",
      "Epoch [35/50], Hidden Size: 64, LR: 0.0001, Batch Size: 64, Loss: 0.3294\n",
      "Epoch [36/50], Hidden Size: 64, LR: 0.0001, Batch Size: 64, Loss: 0.3333\n",
      "Epoch [37/50], Hidden Size: 64, LR: 0.0001, Batch Size: 64, Loss: 0.3350\n",
      "Epoch [38/50], Hidden Size: 64, LR: 0.0001, Batch Size: 64, Loss: 0.3315\n",
      "Epoch [39/50], Hidden Size: 64, LR: 0.0001, Batch Size: 64, Loss: 0.3314\n",
      "Epoch [40/50], Hidden Size: 64, LR: 0.0001, Batch Size: 64, Loss: 0.3359\n",
      "Epoch [41/50], Hidden Size: 64, LR: 0.0001, Batch Size: 64, Loss: 0.3345\n",
      "Epoch [42/50], Hidden Size: 64, LR: 0.0001, Batch Size: 64, Loss: 0.3351\n",
      "Epoch [43/50], Hidden Size: 64, LR: 0.0001, Batch Size: 64, Loss: 0.3293\n",
      "Epoch [44/50], Hidden Size: 64, LR: 0.0001, Batch Size: 64, Loss: 0.3314\n",
      "Epoch [45/50], Hidden Size: 64, LR: 0.0001, Batch Size: 64, Loss: 0.3234\n",
      "Epoch [46/50], Hidden Size: 64, LR: 0.0001, Batch Size: 64, Loss: 0.3234\n",
      "Epoch [47/50], Hidden Size: 64, LR: 0.0001, Batch Size: 64, Loss: 0.3283\n",
      "Epoch [48/50], Hidden Size: 64, LR: 0.0001, Batch Size: 64, Loss: 0.3432\n",
      "Epoch [49/50], Hidden Size: 64, LR: 0.0001, Batch Size: 64, Loss: 0.3257\n",
      "Epoch [50/50], Hidden Size: 64, LR: 0.0001, Batch Size: 64, Loss: 0.3286\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\10374\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torchvision\\models\\_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "c:\\Users\\10374\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torchvision\\models\\_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=None`.\n",
      "  warnings.warn(msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/50], Hidden Size: 128, LR: 0.001, Batch Size: 16, Loss: 0.9020\n",
      "Epoch [2/50], Hidden Size: 128, LR: 0.001, Batch Size: 16, Loss: 0.4277\n",
      "Epoch [3/50], Hidden Size: 128, LR: 0.001, Batch Size: 16, Loss: 0.3361\n",
      "Epoch [4/50], Hidden Size: 128, LR: 0.001, Batch Size: 16, Loss: 0.3342\n",
      "Epoch [5/50], Hidden Size: 128, LR: 0.001, Batch Size: 16, Loss: 0.3357\n",
      "Epoch [6/50], Hidden Size: 128, LR: 0.001, Batch Size: 16, Loss: 0.3313\n",
      "Epoch [7/50], Hidden Size: 128, LR: 0.001, Batch Size: 16, Loss: 0.3368\n",
      "Epoch [8/50], Hidden Size: 128, LR: 0.001, Batch Size: 16, Loss: 0.3339\n",
      "Epoch [9/50], Hidden Size: 128, LR: 0.001, Batch Size: 16, Loss: 0.3381\n",
      "Epoch [10/50], Hidden Size: 128, LR: 0.001, Batch Size: 16, Loss: 0.3337\n",
      "Epoch [11/50], Hidden Size: 128, LR: 0.001, Batch Size: 16, Loss: 0.3332\n",
      "Epoch [12/50], Hidden Size: 128, LR: 0.001, Batch Size: 16, Loss: 0.3312\n",
      "Epoch [13/50], Hidden Size: 128, LR: 0.001, Batch Size: 16, Loss: 0.3345\n",
      "Epoch [14/50], Hidden Size: 128, LR: 0.001, Batch Size: 16, Loss: 0.3362\n",
      "Epoch [15/50], Hidden Size: 128, LR: 0.001, Batch Size: 16, Loss: 0.3309\n",
      "Epoch [16/50], Hidden Size: 128, LR: 0.001, Batch Size: 16, Loss: 0.3346\n",
      "Epoch [17/50], Hidden Size: 128, LR: 0.001, Batch Size: 16, Loss: 0.3374\n",
      "Epoch [18/50], Hidden Size: 128, LR: 0.001, Batch Size: 16, Loss: 0.3473\n",
      "Epoch [19/50], Hidden Size: 128, LR: 0.001, Batch Size: 16, Loss: 0.3323\n",
      "Epoch [20/50], Hidden Size: 128, LR: 0.001, Batch Size: 16, Loss: 0.3331\n",
      "Epoch [21/50], Hidden Size: 128, LR: 0.001, Batch Size: 16, Loss: 0.3311\n",
      "Epoch [22/50], Hidden Size: 128, LR: 0.001, Batch Size: 16, Loss: 0.3398\n",
      "Epoch [23/50], Hidden Size: 128, LR: 0.001, Batch Size: 16, Loss: 0.3355\n",
      "Epoch [24/50], Hidden Size: 128, LR: 0.001, Batch Size: 16, Loss: 0.3346\n",
      "Epoch [25/50], Hidden Size: 128, LR: 0.001, Batch Size: 16, Loss: 0.3353\n",
      "Epoch [26/50], Hidden Size: 128, LR: 0.001, Batch Size: 16, Loss: 0.3360\n",
      "Epoch [27/50], Hidden Size: 128, LR: 0.001, Batch Size: 16, Loss: 0.3403\n",
      "Epoch [28/50], Hidden Size: 128, LR: 0.001, Batch Size: 16, Loss: 0.3331\n",
      "Epoch [29/50], Hidden Size: 128, LR: 0.001, Batch Size: 16, Loss: 0.3338\n",
      "Epoch [30/50], Hidden Size: 128, LR: 0.001, Batch Size: 16, Loss: 0.3342\n",
      "Epoch [31/50], Hidden Size: 128, LR: 0.001, Batch Size: 16, Loss: 0.3339\n",
      "Epoch [32/50], Hidden Size: 128, LR: 0.001, Batch Size: 16, Loss: 0.3389\n",
      "Epoch [33/50], Hidden Size: 128, LR: 0.001, Batch Size: 16, Loss: 0.3407\n",
      "Epoch [34/50], Hidden Size: 128, LR: 0.001, Batch Size: 16, Loss: 0.3351\n",
      "Epoch [35/50], Hidden Size: 128, LR: 0.001, Batch Size: 16, Loss: 0.3317\n",
      "Epoch [36/50], Hidden Size: 128, LR: 0.001, Batch Size: 16, Loss: 0.3334\n",
      "Epoch [37/50], Hidden Size: 128, LR: 0.001, Batch Size: 16, Loss: 0.3336\n",
      "Epoch [38/50], Hidden Size: 128, LR: 0.001, Batch Size: 16, Loss: 0.3350\n",
      "Epoch [39/50], Hidden Size: 128, LR: 0.001, Batch Size: 16, Loss: 0.3407\n",
      "Epoch [40/50], Hidden Size: 128, LR: 0.001, Batch Size: 16, Loss: 0.3400\n",
      "Epoch [41/50], Hidden Size: 128, LR: 0.001, Batch Size: 16, Loss: 0.3339\n",
      "Epoch [42/50], Hidden Size: 128, LR: 0.001, Batch Size: 16, Loss: 0.3351\n",
      "Epoch [43/50], Hidden Size: 128, LR: 0.001, Batch Size: 16, Loss: 0.3328\n",
      "Epoch [44/50], Hidden Size: 128, LR: 0.001, Batch Size: 16, Loss: 0.3332\n",
      "Epoch [45/50], Hidden Size: 128, LR: 0.001, Batch Size: 16, Loss: 0.3346\n",
      "Epoch [46/50], Hidden Size: 128, LR: 0.001, Batch Size: 16, Loss: 0.3333\n",
      "Epoch [47/50], Hidden Size: 128, LR: 0.001, Batch Size: 16, Loss: 0.3342\n",
      "Epoch [48/50], Hidden Size: 128, LR: 0.001, Batch Size: 16, Loss: 0.3329\n",
      "Epoch [49/50], Hidden Size: 128, LR: 0.001, Batch Size: 16, Loss: 0.3407\n",
      "Epoch [50/50], Hidden Size: 128, LR: 0.001, Batch Size: 16, Loss: 0.3348\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\10374\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torchvision\\models\\_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "c:\\Users\\10374\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torchvision\\models\\_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=None`.\n",
      "  warnings.warn(msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/50], Hidden Size: 128, LR: 0.001, Batch Size: 32, Loss: 0.9773\n",
      "Epoch [2/50], Hidden Size: 128, LR: 0.001, Batch Size: 32, Loss: 0.8262\n",
      "Epoch [3/50], Hidden Size: 128, LR: 0.001, Batch Size: 32, Loss: 0.4931\n",
      "Epoch [4/50], Hidden Size: 128, LR: 0.001, Batch Size: 32, Loss: 0.3642\n",
      "Epoch [5/50], Hidden Size: 128, LR: 0.001, Batch Size: 32, Loss: 0.3356\n",
      "Epoch [6/50], Hidden Size: 128, LR: 0.001, Batch Size: 32, Loss: 0.3353\n",
      "Epoch [7/50], Hidden Size: 128, LR: 0.001, Batch Size: 32, Loss: 0.3344\n",
      "Epoch [8/50], Hidden Size: 128, LR: 0.001, Batch Size: 32, Loss: 0.3327\n",
      "Epoch [9/50], Hidden Size: 128, LR: 0.001, Batch Size: 32, Loss: 0.3354\n",
      "Epoch [10/50], Hidden Size: 128, LR: 0.001, Batch Size: 32, Loss: 0.3326\n",
      "Epoch [11/50], Hidden Size: 128, LR: 0.001, Batch Size: 32, Loss: 0.3349\n",
      "Epoch [12/50], Hidden Size: 128, LR: 0.001, Batch Size: 32, Loss: 0.3347\n",
      "Epoch [13/50], Hidden Size: 128, LR: 0.001, Batch Size: 32, Loss: 0.3363\n",
      "Epoch [14/50], Hidden Size: 128, LR: 0.001, Batch Size: 32, Loss: 0.3354\n",
      "Epoch [15/50], Hidden Size: 128, LR: 0.001, Batch Size: 32, Loss: 0.3346\n",
      "Epoch [16/50], Hidden Size: 128, LR: 0.001, Batch Size: 32, Loss: 0.3339\n",
      "Epoch [17/50], Hidden Size: 128, LR: 0.001, Batch Size: 32, Loss: 0.3345\n",
      "Epoch [18/50], Hidden Size: 128, LR: 0.001, Batch Size: 32, Loss: 0.3351\n",
      "Epoch [19/50], Hidden Size: 128, LR: 0.001, Batch Size: 32, Loss: 0.3341\n",
      "Epoch [20/50], Hidden Size: 128, LR: 0.001, Batch Size: 32, Loss: 0.3314\n",
      "Epoch [21/50], Hidden Size: 128, LR: 0.001, Batch Size: 32, Loss: 0.3343\n",
      "Epoch [22/50], Hidden Size: 128, LR: 0.001, Batch Size: 32, Loss: 0.3344\n",
      "Epoch [23/50], Hidden Size: 128, LR: 0.001, Batch Size: 32, Loss: 0.3346\n",
      "Epoch [24/50], Hidden Size: 128, LR: 0.001, Batch Size: 32, Loss: 0.3348\n",
      "Epoch [25/50], Hidden Size: 128, LR: 0.001, Batch Size: 32, Loss: 0.3319\n",
      "Epoch [26/50], Hidden Size: 128, LR: 0.001, Batch Size: 32, Loss: 0.3344\n",
      "Epoch [27/50], Hidden Size: 128, LR: 0.001, Batch Size: 32, Loss: 0.3326\n",
      "Epoch [28/50], Hidden Size: 128, LR: 0.001, Batch Size: 32, Loss: 0.3352\n",
      "Epoch [29/50], Hidden Size: 128, LR: 0.001, Batch Size: 32, Loss: 0.3357\n",
      "Epoch [30/50], Hidden Size: 128, LR: 0.001, Batch Size: 32, Loss: 0.3378\n",
      "Epoch [31/50], Hidden Size: 128, LR: 0.001, Batch Size: 32, Loss: 0.3326\n",
      "Epoch [32/50], Hidden Size: 128, LR: 0.001, Batch Size: 32, Loss: 0.3374\n",
      "Epoch [33/50], Hidden Size: 128, LR: 0.001, Batch Size: 32, Loss: 0.3359\n",
      "Epoch [34/50], Hidden Size: 128, LR: 0.001, Batch Size: 32, Loss: 0.3356\n",
      "Epoch [35/50], Hidden Size: 128, LR: 0.001, Batch Size: 32, Loss: 0.3348\n",
      "Epoch [36/50], Hidden Size: 128, LR: 0.001, Batch Size: 32, Loss: 0.3342\n",
      "Epoch [37/50], Hidden Size: 128, LR: 0.001, Batch Size: 32, Loss: 0.3348\n",
      "Epoch [38/50], Hidden Size: 128, LR: 0.001, Batch Size: 32, Loss: 0.3346\n",
      "Epoch [39/50], Hidden Size: 128, LR: 0.001, Batch Size: 32, Loss: 0.3344\n",
      "Epoch [40/50], Hidden Size: 128, LR: 0.001, Batch Size: 32, Loss: 0.3350\n",
      "Epoch [41/50], Hidden Size: 128, LR: 0.001, Batch Size: 32, Loss: 0.3353\n",
      "Epoch [42/50], Hidden Size: 128, LR: 0.001, Batch Size: 32, Loss: 0.3368\n",
      "Epoch [43/50], Hidden Size: 128, LR: 0.001, Batch Size: 32, Loss: 0.3354\n",
      "Epoch [44/50], Hidden Size: 128, LR: 0.001, Batch Size: 32, Loss: 0.3382\n",
      "Epoch [45/50], Hidden Size: 128, LR: 0.001, Batch Size: 32, Loss: 0.3355\n",
      "Epoch [46/50], Hidden Size: 128, LR: 0.001, Batch Size: 32, Loss: 0.3353\n",
      "Epoch [47/50], Hidden Size: 128, LR: 0.001, Batch Size: 32, Loss: 0.3355\n",
      "Epoch [48/50], Hidden Size: 128, LR: 0.001, Batch Size: 32, Loss: 0.3344\n",
      "Epoch [49/50], Hidden Size: 128, LR: 0.001, Batch Size: 32, Loss: 0.3344\n",
      "Epoch [50/50], Hidden Size: 128, LR: 0.001, Batch Size: 32, Loss: 0.3345\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\10374\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torchvision\\models\\_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "c:\\Users\\10374\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torchvision\\models\\_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=None`.\n",
      "  warnings.warn(msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/50], Hidden Size: 128, LR: 0.001, Batch Size: 64, Loss: 0.9936\n",
      "Epoch [2/50], Hidden Size: 128, LR: 0.001, Batch Size: 64, Loss: 0.9478\n",
      "Epoch [3/50], Hidden Size: 128, LR: 0.001, Batch Size: 64, Loss: 0.8378\n",
      "Epoch [4/50], Hidden Size: 128, LR: 0.001, Batch Size: 64, Loss: 0.6622\n",
      "Epoch [5/50], Hidden Size: 128, LR: 0.001, Batch Size: 64, Loss: 0.4256\n",
      "Epoch [6/50], Hidden Size: 128, LR: 0.001, Batch Size: 64, Loss: 0.3641\n",
      "Epoch [7/50], Hidden Size: 128, LR: 0.001, Batch Size: 64, Loss: 0.3702\n",
      "Epoch [8/50], Hidden Size: 128, LR: 0.001, Batch Size: 64, Loss: 0.3351\n",
      "Epoch [9/50], Hidden Size: 128, LR: 0.001, Batch Size: 64, Loss: 0.3292\n",
      "Epoch [10/50], Hidden Size: 128, LR: 0.001, Batch Size: 64, Loss: 0.3294\n",
      "Epoch [11/50], Hidden Size: 128, LR: 0.001, Batch Size: 64, Loss: 0.3384\n",
      "Epoch [12/50], Hidden Size: 128, LR: 0.001, Batch Size: 64, Loss: 0.3389\n",
      "Epoch [13/50], Hidden Size: 128, LR: 0.001, Batch Size: 64, Loss: 0.3397\n",
      "Epoch [14/50], Hidden Size: 128, LR: 0.001, Batch Size: 64, Loss: 0.3291\n",
      "Epoch [15/50], Hidden Size: 128, LR: 0.001, Batch Size: 64, Loss: 0.3260\n",
      "Epoch [16/50], Hidden Size: 128, LR: 0.001, Batch Size: 64, Loss: 0.3459\n",
      "Epoch [17/50], Hidden Size: 128, LR: 0.001, Batch Size: 64, Loss: 0.3365\n",
      "Epoch [18/50], Hidden Size: 128, LR: 0.001, Batch Size: 64, Loss: 0.3346\n",
      "Epoch [19/50], Hidden Size: 128, LR: 0.001, Batch Size: 64, Loss: 0.3297\n",
      "Epoch [20/50], Hidden Size: 128, LR: 0.001, Batch Size: 64, Loss: 0.3372\n",
      "Epoch [21/50], Hidden Size: 128, LR: 0.001, Batch Size: 64, Loss: 0.3314\n",
      "Epoch [22/50], Hidden Size: 128, LR: 0.001, Batch Size: 64, Loss: 0.3240\n",
      "Epoch [23/50], Hidden Size: 128, LR: 0.001, Batch Size: 64, Loss: 0.3376\n",
      "Epoch [24/50], Hidden Size: 128, LR: 0.001, Batch Size: 64, Loss: 0.3357\n",
      "Epoch [25/50], Hidden Size: 128, LR: 0.001, Batch Size: 64, Loss: 0.3322\n",
      "Epoch [26/50], Hidden Size: 128, LR: 0.001, Batch Size: 64, Loss: 0.3352\n",
      "Epoch [27/50], Hidden Size: 128, LR: 0.001, Batch Size: 64, Loss: 0.3333\n",
      "Epoch [28/50], Hidden Size: 128, LR: 0.001, Batch Size: 64, Loss: 0.3372\n",
      "Epoch [29/50], Hidden Size: 128, LR: 0.001, Batch Size: 64, Loss: 0.3317\n",
      "Epoch [30/50], Hidden Size: 128, LR: 0.001, Batch Size: 64, Loss: 0.3393\n",
      "Epoch [31/50], Hidden Size: 128, LR: 0.001, Batch Size: 64, Loss: 0.3386\n",
      "Epoch [32/50], Hidden Size: 128, LR: 0.001, Batch Size: 64, Loss: 0.3397\n",
      "Epoch [33/50], Hidden Size: 128, LR: 0.001, Batch Size: 64, Loss: 0.3334\n",
      "Epoch [34/50], Hidden Size: 128, LR: 0.001, Batch Size: 64, Loss: 0.3403\n",
      "Epoch [35/50], Hidden Size: 128, LR: 0.001, Batch Size: 64, Loss: 0.3363\n",
      "Epoch [36/50], Hidden Size: 128, LR: 0.001, Batch Size: 64, Loss: 0.3363\n",
      "Epoch [37/50], Hidden Size: 128, LR: 0.001, Batch Size: 64, Loss: 0.3352\n",
      "Epoch [38/50], Hidden Size: 128, LR: 0.001, Batch Size: 64, Loss: 0.3371\n",
      "Epoch [39/50], Hidden Size: 128, LR: 0.001, Batch Size: 64, Loss: 0.3306\n",
      "Epoch [40/50], Hidden Size: 128, LR: 0.001, Batch Size: 64, Loss: 0.3371\n",
      "Epoch [41/50], Hidden Size: 128, LR: 0.001, Batch Size: 64, Loss: 0.3336\n",
      "Epoch [42/50], Hidden Size: 128, LR: 0.001, Batch Size: 64, Loss: 0.3432\n",
      "Epoch [43/50], Hidden Size: 128, LR: 0.001, Batch Size: 64, Loss: 0.3259\n",
      "Epoch [44/50], Hidden Size: 128, LR: 0.001, Batch Size: 64, Loss: 0.3282\n",
      "Epoch [45/50], Hidden Size: 128, LR: 0.001, Batch Size: 64, Loss: 0.3354\n",
      "Epoch [46/50], Hidden Size: 128, LR: 0.001, Batch Size: 64, Loss: 0.3344\n",
      "Epoch [47/50], Hidden Size: 128, LR: 0.001, Batch Size: 64, Loss: 0.3339\n",
      "Epoch [48/50], Hidden Size: 128, LR: 0.001, Batch Size: 64, Loss: 0.3426\n",
      "Epoch [49/50], Hidden Size: 128, LR: 0.001, Batch Size: 64, Loss: 0.3326\n",
      "Epoch [50/50], Hidden Size: 128, LR: 0.001, Batch Size: 64, Loss: 0.3326\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\10374\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torchvision\\models\\_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "c:\\Users\\10374\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torchvision\\models\\_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=None`.\n",
      "  warnings.warn(msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/50], Hidden Size: 128, LR: 0.0001, Batch Size: 16, Loss: 0.9967\n",
      "Epoch [2/50], Hidden Size: 128, LR: 0.0001, Batch Size: 16, Loss: 0.9855\n",
      "Epoch [3/50], Hidden Size: 128, LR: 0.0001, Batch Size: 16, Loss: 0.9581\n",
      "Epoch [4/50], Hidden Size: 128, LR: 0.0001, Batch Size: 16, Loss: 0.9084\n",
      "Epoch [5/50], Hidden Size: 128, LR: 0.0001, Batch Size: 16, Loss: 0.8368\n",
      "Epoch [6/50], Hidden Size: 128, LR: 0.0001, Batch Size: 16, Loss: 0.7393\n",
      "Epoch [7/50], Hidden Size: 128, LR: 0.0001, Batch Size: 16, Loss: 0.6215\n",
      "Epoch [8/50], Hidden Size: 128, LR: 0.0001, Batch Size: 16, Loss: 0.4811\n",
      "Epoch [9/50], Hidden Size: 128, LR: 0.0001, Batch Size: 16, Loss: 0.3732\n",
      "Epoch [10/50], Hidden Size: 128, LR: 0.0001, Batch Size: 16, Loss: 0.3346\n",
      "Epoch [11/50], Hidden Size: 128, LR: 0.0001, Batch Size: 16, Loss: 0.3321\n",
      "Epoch [12/50], Hidden Size: 128, LR: 0.0001, Batch Size: 16, Loss: 0.3325\n",
      "Epoch [13/50], Hidden Size: 128, LR: 0.0001, Batch Size: 16, Loss: 0.3355\n",
      "Epoch [14/50], Hidden Size: 128, LR: 0.0001, Batch Size: 16, Loss: 0.3304\n",
      "Epoch [15/50], Hidden Size: 128, LR: 0.0001, Batch Size: 16, Loss: 0.3334\n",
      "Epoch [16/50], Hidden Size: 128, LR: 0.0001, Batch Size: 16, Loss: 0.3305\n",
      "Epoch [17/50], Hidden Size: 128, LR: 0.0001, Batch Size: 16, Loss: 0.3318\n",
      "Epoch [18/50], Hidden Size: 128, LR: 0.0001, Batch Size: 16, Loss: 0.3292\n",
      "Epoch [19/50], Hidden Size: 128, LR: 0.0001, Batch Size: 16, Loss: 0.3303\n",
      "Epoch [20/50], Hidden Size: 128, LR: 0.0001, Batch Size: 16, Loss: 0.3335\n",
      "Epoch [21/50], Hidden Size: 128, LR: 0.0001, Batch Size: 16, Loss: 0.3307\n",
      "Epoch [22/50], Hidden Size: 128, LR: 0.0001, Batch Size: 16, Loss: 0.3335\n",
      "Epoch [23/50], Hidden Size: 128, LR: 0.0001, Batch Size: 16, Loss: 0.3317\n",
      "Epoch [24/50], Hidden Size: 128, LR: 0.0001, Batch Size: 16, Loss: 0.3317\n",
      "Epoch [25/50], Hidden Size: 128, LR: 0.0001, Batch Size: 16, Loss: 0.3333\n",
      "Epoch [26/50], Hidden Size: 128, LR: 0.0001, Batch Size: 16, Loss: 0.3307\n",
      "Epoch [27/50], Hidden Size: 128, LR: 0.0001, Batch Size: 16, Loss: 0.3314\n",
      "Epoch [28/50], Hidden Size: 128, LR: 0.0001, Batch Size: 16, Loss: 0.3295\n",
      "Epoch [29/50], Hidden Size: 128, LR: 0.0001, Batch Size: 16, Loss: 0.3333\n",
      "Epoch [30/50], Hidden Size: 128, LR: 0.0001, Batch Size: 16, Loss: 0.3302\n",
      "Epoch [31/50], Hidden Size: 128, LR: 0.0001, Batch Size: 16, Loss: 0.3324\n",
      "Epoch [32/50], Hidden Size: 128, LR: 0.0001, Batch Size: 16, Loss: 0.3302\n",
      "Epoch [33/50], Hidden Size: 128, LR: 0.0001, Batch Size: 16, Loss: 0.3344\n",
      "Epoch [34/50], Hidden Size: 128, LR: 0.0001, Batch Size: 16, Loss: 0.3316\n",
      "Epoch [35/50], Hidden Size: 128, LR: 0.0001, Batch Size: 16, Loss: 0.3312\n",
      "Epoch [36/50], Hidden Size: 128, LR: 0.0001, Batch Size: 16, Loss: 0.3306\n",
      "Epoch [37/50], Hidden Size: 128, LR: 0.0001, Batch Size: 16, Loss: 0.3333\n",
      "Epoch [38/50], Hidden Size: 128, LR: 0.0001, Batch Size: 16, Loss: 0.3334\n",
      "Epoch [39/50], Hidden Size: 128, LR: 0.0001, Batch Size: 16, Loss: 0.3331\n",
      "Epoch [40/50], Hidden Size: 128, LR: 0.0001, Batch Size: 16, Loss: 0.3301\n",
      "Epoch [41/50], Hidden Size: 128, LR: 0.0001, Batch Size: 16, Loss: 0.3328\n",
      "Epoch [42/50], Hidden Size: 128, LR: 0.0001, Batch Size: 16, Loss: 0.3336\n",
      "Epoch [43/50], Hidden Size: 128, LR: 0.0001, Batch Size: 16, Loss: 0.3345\n",
      "Epoch [44/50], Hidden Size: 128, LR: 0.0001, Batch Size: 16, Loss: 0.3294\n",
      "Epoch [45/50], Hidden Size: 128, LR: 0.0001, Batch Size: 16, Loss: 0.3318\n",
      "Epoch [46/50], Hidden Size: 128, LR: 0.0001, Batch Size: 16, Loss: 0.3281\n",
      "Epoch [47/50], Hidden Size: 128, LR: 0.0001, Batch Size: 16, Loss: 0.3299\n",
      "Epoch [48/50], Hidden Size: 128, LR: 0.0001, Batch Size: 16, Loss: 0.3304\n",
      "Epoch [49/50], Hidden Size: 128, LR: 0.0001, Batch Size: 16, Loss: 0.3326\n",
      "Epoch [50/50], Hidden Size: 128, LR: 0.0001, Batch Size: 16, Loss: 0.3307\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\10374\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torchvision\\models\\_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "c:\\Users\\10374\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torchvision\\models\\_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=None`.\n",
      "  warnings.warn(msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/50], Hidden Size: 128, LR: 0.0001, Batch Size: 32, Loss: 0.9994\n",
      "Epoch [2/50], Hidden Size: 128, LR: 0.0001, Batch Size: 32, Loss: 0.9966\n",
      "Epoch [3/50], Hidden Size: 128, LR: 0.0001, Batch Size: 32, Loss: 0.9920\n",
      "Epoch [4/50], Hidden Size: 128, LR: 0.0001, Batch Size: 32, Loss: 0.9844\n",
      "Epoch [5/50], Hidden Size: 128, LR: 0.0001, Batch Size: 32, Loss: 0.9722\n",
      "Epoch [6/50], Hidden Size: 128, LR: 0.0001, Batch Size: 32, Loss: 0.9551\n",
      "Epoch [7/50], Hidden Size: 128, LR: 0.0001, Batch Size: 32, Loss: 0.9335\n",
      "Epoch [8/50], Hidden Size: 128, LR: 0.0001, Batch Size: 32, Loss: 0.9076\n",
      "Epoch [9/50], Hidden Size: 128, LR: 0.0001, Batch Size: 32, Loss: 0.8772\n",
      "Epoch [10/50], Hidden Size: 128, LR: 0.0001, Batch Size: 32, Loss: 0.8430\n",
      "Epoch [11/50], Hidden Size: 128, LR: 0.0001, Batch Size: 32, Loss: 0.8021\n",
      "Epoch [12/50], Hidden Size: 128, LR: 0.0001, Batch Size: 32, Loss: 0.7550\n",
      "Epoch [13/50], Hidden Size: 128, LR: 0.0001, Batch Size: 32, Loss: 0.7032\n",
      "Epoch [14/50], Hidden Size: 128, LR: 0.0001, Batch Size: 32, Loss: 0.6457\n",
      "Epoch [15/50], Hidden Size: 128, LR: 0.0001, Batch Size: 32, Loss: 0.5845\n",
      "Epoch [16/50], Hidden Size: 128, LR: 0.0001, Batch Size: 32, Loss: 0.5199\n",
      "Epoch [17/50], Hidden Size: 128, LR: 0.0001, Batch Size: 32, Loss: 0.4560\n",
      "Epoch [18/50], Hidden Size: 128, LR: 0.0001, Batch Size: 32, Loss: 0.4022\n",
      "Epoch [19/50], Hidden Size: 128, LR: 0.0001, Batch Size: 32, Loss: 0.3630\n",
      "Epoch [20/50], Hidden Size: 128, LR: 0.0001, Batch Size: 32, Loss: 0.3400\n",
      "Epoch [21/50], Hidden Size: 128, LR: 0.0001, Batch Size: 32, Loss: 0.3344\n",
      "Epoch [22/50], Hidden Size: 128, LR: 0.0001, Batch Size: 32, Loss: 0.3334\n",
      "Epoch [23/50], Hidden Size: 128, LR: 0.0001, Batch Size: 32, Loss: 0.3347\n",
      "Epoch [24/50], Hidden Size: 128, LR: 0.0001, Batch Size: 32, Loss: 0.3326\n",
      "Epoch [25/50], Hidden Size: 128, LR: 0.0001, Batch Size: 32, Loss: 0.3333\n",
      "Epoch [26/50], Hidden Size: 128, LR: 0.0001, Batch Size: 32, Loss: 0.3345\n",
      "Epoch [27/50], Hidden Size: 128, LR: 0.0001, Batch Size: 32, Loss: 0.3342\n",
      "Epoch [28/50], Hidden Size: 128, LR: 0.0001, Batch Size: 32, Loss: 0.3342\n",
      "Epoch [29/50], Hidden Size: 128, LR: 0.0001, Batch Size: 32, Loss: 0.3323\n",
      "Epoch [30/50], Hidden Size: 128, LR: 0.0001, Batch Size: 32, Loss: 0.3331\n",
      "Epoch [31/50], Hidden Size: 128, LR: 0.0001, Batch Size: 32, Loss: 0.3323\n",
      "Epoch [32/50], Hidden Size: 128, LR: 0.0001, Batch Size: 32, Loss: 0.3347\n",
      "Epoch [33/50], Hidden Size: 128, LR: 0.0001, Batch Size: 32, Loss: 0.3339\n",
      "Epoch [34/50], Hidden Size: 128, LR: 0.0001, Batch Size: 32, Loss: 0.3312\n",
      "Epoch [35/50], Hidden Size: 128, LR: 0.0001, Batch Size: 32, Loss: 0.3343\n",
      "Epoch [36/50], Hidden Size: 128, LR: 0.0001, Batch Size: 32, Loss: 0.3325\n",
      "Epoch [37/50], Hidden Size: 128, LR: 0.0001, Batch Size: 32, Loss: 0.3334\n",
      "Epoch [38/50], Hidden Size: 128, LR: 0.0001, Batch Size: 32, Loss: 0.3299\n",
      "Epoch [39/50], Hidden Size: 128, LR: 0.0001, Batch Size: 32, Loss: 0.3333\n",
      "Epoch [40/50], Hidden Size: 128, LR: 0.0001, Batch Size: 32, Loss: 0.3349\n",
      "Epoch [41/50], Hidden Size: 128, LR: 0.0001, Batch Size: 32, Loss: 0.3347\n",
      "Epoch [42/50], Hidden Size: 128, LR: 0.0001, Batch Size: 32, Loss: 0.3313\n",
      "Epoch [43/50], Hidden Size: 128, LR: 0.0001, Batch Size: 32, Loss: 0.3318\n",
      "Epoch [44/50], Hidden Size: 128, LR: 0.0001, Batch Size: 32, Loss: 0.3321\n",
      "Epoch [45/50], Hidden Size: 128, LR: 0.0001, Batch Size: 32, Loss: 0.3338\n",
      "Epoch [46/50], Hidden Size: 128, LR: 0.0001, Batch Size: 32, Loss: 0.3334\n",
      "Epoch [47/50], Hidden Size: 128, LR: 0.0001, Batch Size: 32, Loss: 0.3336\n",
      "Epoch [48/50], Hidden Size: 128, LR: 0.0001, Batch Size: 32, Loss: 0.3327\n",
      "Epoch [49/50], Hidden Size: 128, LR: 0.0001, Batch Size: 32, Loss: 0.3338\n",
      "Epoch [50/50], Hidden Size: 128, LR: 0.0001, Batch Size: 32, Loss: 0.3319\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\10374\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torchvision\\models\\_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "c:\\Users\\10374\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torchvision\\models\\_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=None`.\n",
      "  warnings.warn(msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/50], Hidden Size: 128, LR: 0.0001, Batch Size: 64, Loss: 0.9995\n",
      "Epoch [2/50], Hidden Size: 128, LR: 0.0001, Batch Size: 64, Loss: 0.9979\n",
      "Epoch [3/50], Hidden Size: 128, LR: 0.0001, Batch Size: 64, Loss: 0.9958\n",
      "Epoch [4/50], Hidden Size: 128, LR: 0.0001, Batch Size: 64, Loss: 0.9931\n",
      "Epoch [5/50], Hidden Size: 128, LR: 0.0001, Batch Size: 64, Loss: 0.9893\n",
      "Epoch [6/50], Hidden Size: 128, LR: 0.0001, Batch Size: 64, Loss: 0.9847\n",
      "Epoch [7/50], Hidden Size: 128, LR: 0.0001, Batch Size: 64, Loss: 0.9785\n",
      "Epoch [8/50], Hidden Size: 128, LR: 0.0001, Batch Size: 64, Loss: 0.9706\n",
      "Epoch [9/50], Hidden Size: 128, LR: 0.0001, Batch Size: 64, Loss: 0.9618\n",
      "Epoch [10/50], Hidden Size: 128, LR: 0.0001, Batch Size: 64, Loss: 0.9516\n",
      "Epoch [11/50], Hidden Size: 128, LR: 0.0001, Batch Size: 64, Loss: 0.9396\n",
      "Epoch [12/50], Hidden Size: 128, LR: 0.0001, Batch Size: 64, Loss: 0.9255\n",
      "Epoch [13/50], Hidden Size: 128, LR: 0.0001, Batch Size: 64, Loss: 0.9108\n",
      "Epoch [14/50], Hidden Size: 128, LR: 0.0001, Batch Size: 64, Loss: 0.8975\n",
      "Epoch [15/50], Hidden Size: 128, LR: 0.0001, Batch Size: 64, Loss: 0.8780\n",
      "Epoch [16/50], Hidden Size: 128, LR: 0.0001, Batch Size: 64, Loss: 0.8587\n",
      "Epoch [17/50], Hidden Size: 128, LR: 0.0001, Batch Size: 64, Loss: 0.8397\n",
      "Epoch [18/50], Hidden Size: 128, LR: 0.0001, Batch Size: 64, Loss: 0.8194\n",
      "Epoch [19/50], Hidden Size: 128, LR: 0.0001, Batch Size: 64, Loss: 0.7942\n",
      "Epoch [20/50], Hidden Size: 128, LR: 0.0001, Batch Size: 64, Loss: 0.7668\n",
      "Epoch [21/50], Hidden Size: 128, LR: 0.0001, Batch Size: 64, Loss: 0.7442\n",
      "Epoch [22/50], Hidden Size: 128, LR: 0.0001, Batch Size: 64, Loss: 0.7102\n",
      "Epoch [23/50], Hidden Size: 128, LR: 0.0001, Batch Size: 64, Loss: 0.6840\n",
      "Epoch [24/50], Hidden Size: 128, LR: 0.0001, Batch Size: 64, Loss: 0.6543\n",
      "Epoch [25/50], Hidden Size: 128, LR: 0.0001, Batch Size: 64, Loss: 0.6168\n",
      "Epoch [26/50], Hidden Size: 128, LR: 0.0001, Batch Size: 64, Loss: 0.5844\n",
      "Epoch [27/50], Hidden Size: 128, LR: 0.0001, Batch Size: 64, Loss: 0.5531\n",
      "Epoch [28/50], Hidden Size: 128, LR: 0.0001, Batch Size: 64, Loss: 0.5152\n",
      "Epoch [29/50], Hidden Size: 128, LR: 0.0001, Batch Size: 64, Loss: 0.4763\n",
      "Epoch [30/50], Hidden Size: 128, LR: 0.0001, Batch Size: 64, Loss: 0.4479\n",
      "Epoch [31/50], Hidden Size: 128, LR: 0.0001, Batch Size: 64, Loss: 0.4198\n",
      "Epoch [32/50], Hidden Size: 128, LR: 0.0001, Batch Size: 64, Loss: 0.3910\n",
      "Epoch [33/50], Hidden Size: 128, LR: 0.0001, Batch Size: 64, Loss: 0.3635\n",
      "Epoch [34/50], Hidden Size: 128, LR: 0.0001, Batch Size: 64, Loss: 0.3515\n",
      "Epoch [35/50], Hidden Size: 128, LR: 0.0001, Batch Size: 64, Loss: 0.3436\n",
      "Epoch [36/50], Hidden Size: 128, LR: 0.0001, Batch Size: 64, Loss: 0.3361\n",
      "Epoch [37/50], Hidden Size: 128, LR: 0.0001, Batch Size: 64, Loss: 0.3323\n",
      "Epoch [38/50], Hidden Size: 128, LR: 0.0001, Batch Size: 64, Loss: 0.3272\n",
      "Epoch [39/50], Hidden Size: 128, LR: 0.0001, Batch Size: 64, Loss: 0.3333\n",
      "Epoch [40/50], Hidden Size: 128, LR: 0.0001, Batch Size: 64, Loss: 0.3302\n",
      "Epoch [41/50], Hidden Size: 128, LR: 0.0001, Batch Size: 64, Loss: 0.3378\n",
      "Epoch [42/50], Hidden Size: 128, LR: 0.0001, Batch Size: 64, Loss: 0.3328\n",
      "Epoch [43/50], Hidden Size: 128, LR: 0.0001, Batch Size: 64, Loss: 0.3374\n",
      "Epoch [44/50], Hidden Size: 128, LR: 0.0001, Batch Size: 64, Loss: 0.3357\n",
      "Epoch [45/50], Hidden Size: 128, LR: 0.0001, Batch Size: 64, Loss: 0.3336\n",
      "Epoch [46/50], Hidden Size: 128, LR: 0.0001, Batch Size: 64, Loss: 0.3350\n",
      "Epoch [47/50], Hidden Size: 128, LR: 0.0001, Batch Size: 64, Loss: 0.3353\n",
      "Epoch [48/50], Hidden Size: 128, LR: 0.0001, Batch Size: 64, Loss: 0.3245\n",
      "Epoch [49/50], Hidden Size: 128, LR: 0.0001, Batch Size: 64, Loss: 0.3322\n",
      "Epoch [50/50], Hidden Size: 128, LR: 0.0001, Batch Size: 64, Loss: 0.3302\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "CombinedModel(\n",
       "  (seq_net): LSTM(2, 64, batch_first=True)\n",
       "  (seq_fc): Linear(in_features=64, out_features=32, bias=True)\n",
       "  (viome_net): LSTM(1, 64, batch_first=True)\n",
       "  (viome_fc): Linear(in_features=64, out_features=32, bias=True)\n",
       "  (img_net): ResNet(\n",
       "    (conv1): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n",
       "    (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (relu): ReLU(inplace=True)\n",
       "    (maxpool): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n",
       "    (layer1): Sequential(\n",
       "      (0): Bottleneck(\n",
       "        (conv1): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "        (downsample): Sequential(\n",
       "          (0): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        )\n",
       "      )\n",
       "      (1): Bottleneck(\n",
       "        (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "      )\n",
       "      (2): Bottleneck(\n",
       "        (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "      )\n",
       "    )\n",
       "    (layer2): Sequential(\n",
       "      (0): Bottleneck(\n",
       "        (conv1): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "        (downsample): Sequential(\n",
       "          (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "          (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        )\n",
       "      )\n",
       "      (1): Bottleneck(\n",
       "        (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "      )\n",
       "      (2): Bottleneck(\n",
       "        (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "      )\n",
       "      (3): Bottleneck(\n",
       "        (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "      )\n",
       "    )\n",
       "    (layer3): Sequential(\n",
       "      (0): Bottleneck(\n",
       "        (conv1): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "        (downsample): Sequential(\n",
       "          (0): Conv2d(512, 1024, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "          (1): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        )\n",
       "      )\n",
       "      (1): Bottleneck(\n",
       "        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "      )\n",
       "      (2): Bottleneck(\n",
       "        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "      )\n",
       "      (3): Bottleneck(\n",
       "        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "      )\n",
       "      (4): Bottleneck(\n",
       "        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "      )\n",
       "      (5): Bottleneck(\n",
       "        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "      )\n",
       "    )\n",
       "    (layer4): Sequential(\n",
       "      (0): Bottleneck(\n",
       "        (conv1): Conv2d(1024, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "        (downsample): Sequential(\n",
       "          (0): Conv2d(1024, 2048, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "          (1): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        )\n",
       "      )\n",
       "      (1): Bottleneck(\n",
       "        (conv1): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "      )\n",
       "      (2): Bottleneck(\n",
       "        (conv1): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "      )\n",
       "    )\n",
       "    (avgpool): AdaptiveAvgPool2d(output_size=(1, 1))\n",
       "    (fc): Linear(in_features=2048, out_features=32, bias=True)\n",
       "  )\n",
       "  (num_net): Sequential(\n",
       "    (0): Linear(in_features=20, out_features=32, bias=True)\n",
       "    (1): ReLU()\n",
       "    (2): Linear(in_features=32, out_features=16, bias=True)\n",
       "  )\n",
       "  (fc): Sequential(\n",
       "    (0): Linear(in_features=144, out_features=64, bias=True)\n",
       "    (1): ReLU()\n",
       "    (2): Linear(in_features=64, out_features=1, bias=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Hyperparameters\n",
    "hidden_sizes = [32, 64, 128]\n",
    "learning_rates = [1e-3, 1e-4]\n",
    "batch_sizes = [16, 32, 64]\n",
    "\n",
    "best_loss = float('inf')\n",
    "best_model = None\n",
    "\n",
    "# Iterate over different combinations of hyperparameters to find the best model\n",
    "for hidden_size in hidden_sizes:\n",
    "    for lr in learning_rates:\n",
    "        for batch_size in batch_sizes:\n",
    "            # Create the dataset loader for training and testing\n",
    "            train_loader = DataLoader(dataset_train, batch_size=batch_size, shuffle=True)\n",
    "            test_loader = DataLoader(dataset_test, batch_size=batch_size, shuffle=False)\n",
    "            \n",
    "            model = CombinedModel(hidden_size=hidden_size)\n",
    "            criterion = RMSRELoss()  \n",
    "            optimizer = optim.Adam(model.parameters(), lr=lr)\n",
    "            \n",
    "            # Learning rate scheduler\n",
    "            scheduler = optim.lr_scheduler.StepLR(optimizer, step_size=50, gamma=0.1)\n",
    "            \n",
    "            # Set up the device\n",
    "            device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "            model = model.to(device)\n",
    "            criterion = criterion.to(device)\n",
    "            \n",
    "            # Training process\n",
    "            num_epochs = 50\n",
    "            for epoch in range(num_epochs):\n",
    "                model.train()\n",
    "                running_loss = 0.0\n",
    "                for batch in train_loader:\n",
    "                    # Move data to device\n",
    "                    sequential_data = batch['sequential_data'].to(device)\n",
    "                    img_b = batch['img_b'].to(device)\n",
    "                    img_l = batch['img_l'].to(device)\n",
    "                    numeric_data = batch['numeric_data'].to(device)\n",
    "                    viome_sequential = batch['viome_sequential'].to(device)\n",
    "                    label = batch['label'].to(device)\n",
    "                    \n",
    "                    # Forward pass\n",
    "                    outputs = model(sequential_data, img_b, img_l, numeric_data, viome_sequential)\n",
    "                    loss = criterion(outputs, label)\n",
    "                    \n",
    "                    # Backward pass and optimization\n",
    "                    optimizer.zero_grad()\n",
    "                    loss.backward()\n",
    "                    optimizer.step()\n",
    "                    \n",
    "                    running_loss += loss.item()\n",
    "                scheduler.step()\n",
    "                \n",
    "                avg_loss = running_loss / len(train_loader)\n",
    "                print(f\"Epoch [{epoch+1}/{num_epochs}], Hidden Size: {hidden_size}, LR: {lr}, Batch Size: {batch_size}, Loss: {avg_loss:.4f}\")\n",
    "            \n",
    "            # Save the best model\n",
    "            if avg_loss < best_loss:\n",
    "                best_loss = avg_loss\n",
    "                best_model = model\n",
    "\n",
    "# Use the best trained model for predictions\n",
    "best_model.eval()  # Set the model to evaluation mode\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predictions saved to 'submission.csv' in Kaggle format.\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "# Use the best trained model for predictions\n",
    "best_model.eval()  # Set the model to evaluation mode\n",
    "test_loss = 0.0\n",
    "all_outputs = []\n",
    "all_labels = []\n",
    "\n",
    "with torch.no_grad():  # Disable gradient calculation\n",
    "    for batch in test_loader:\n",
    "        sequential_data = batch['sequential_data'].to(device)\n",
    "        img_b = batch['img_b'].to(device)\n",
    "        img_l = batch['img_l'].to(device)\n",
    "        numeric_data = batch['numeric_data'].to(device)\n",
    "        viome_sequential = batch['viome_sequential'].to(device)\n",
    "        \n",
    "        # Forward pass\n",
    "        outputs = model(sequential_data, img_b, img_l, numeric_data, viome_sequential)\n",
    "        \n",
    "        all_outputs += ([i[0] for i in outputs.tolist()])\n",
    "        \n",
    "test_ids = np.arange(len(all_outputs)) \n",
    "submission = pd.DataFrame({\"row_id\": test_ids, \"label\": all_outputs})\n",
    "submission.to_csv(\"submission.csv\", index=False)\n",
    "print(\"Predictions saved to 'submission.csv' in Kaggle format.\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
