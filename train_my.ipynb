{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import numpy as np\n",
    "import ast\n",
    "from PIL import Image\n",
    "from datetime import datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "cgm_test = pd.read_csv('633FinalData/cgm_test.csv')\n",
    "cgm_train = pd.read_csv('633FinalData/cgm_train.csv')\n",
    "img_train = pd.read_csv('633FinalData/img_train.csv')\n",
    "img_test = pd.read_csv('633FinalData/img_test.csv')\n",
    "cgm_test['cgm'] = cgm_test['CGM Data'].apply(lambda x: ast.literal_eval(x))\n",
    "cgm_train['cgm'] = cgm_train['CGM Data'].apply(lambda x: ast.literal_eval(x))\n",
    "def get_img(img):\n",
    "    # Convert the string representation of the image to a list\n",
    "    img = ast.literal_eval(img)\n",
    "    \n",
    "    # Convert the list to a NumPy array\n",
    "    img = np.array(img)\n",
    "    \n",
    "    # Convert the NumPy array to a PIL Image\n",
    "    img = Image.fromarray(np.uint8(img))\n",
    "    \n",
    "    # Resize the image\n",
    "    img = img.resize((64,64))\n",
    "    \n",
    "    # Convert the resized image back to a NumPy array\n",
    "    img = np.array(img)\n",
    "    \n",
    "    if len(img.shape) == 2:\n",
    "        img = np.stack((img,)*3, axis=-1)\n",
    "    return img\n",
    "\n",
    "img_train['img_b'] = img_train['Image Before Breakfast'].apply(get_img)\n",
    "img_train['img_l'] = img_train['Image Before Lunch'].apply(get_img)\n",
    "img_test['img_b'] = img_test['Image Before Breakfast'].apply(get_img)\n",
    "img_test['img_l'] = img_test['Image Before Lunch'].apply(get_img)\n",
    "\n",
    "def to_step(t):\n",
    "    date_obj = datetime.strptime(t, '%Y-%m-%d %H:%M:%S')\n",
    "    return (date_obj.hour*60 + date_obj.minute)//5\n",
    "\n",
    "def cgm_to_steps(cgm):\n",
    "    steps = [0 for _ in range(288)]\n",
    "    for t,value in cgm:\n",
    "        steps[to_step(t)] = value\n",
    "    return steps\n",
    "\n",
    "def time_to_step(t1,t2):\n",
    "    if t1 == '{}' or t2 == '{}':\n",
    "        return [0 for _ in range(288)]\n",
    "    steps = [0 for _ in range(288)]\n",
    "    steps[to_step(t1)] = 1\n",
    "    steps[to_step(t2)] = 1\n",
    "    return steps\n",
    "\n",
    "# Drop rows with NaT values in 'start_time' or 'end_time'\n",
    "# cgm_test = cgm_test.dropna()\n",
    "cgm_train = cgm_train.dropna()\n",
    "# Drop rows containing the string '{}' in any column\n",
    "# cgm_test = cgm_test[~cgm_test.apply(lambda row: row.astype(str).str.contains('{}').any(), axis=1)]\n",
    "cgm_train = cgm_train[~cgm_train.apply(lambda row: row.astype(str).str.contains('{}').any(), axis=1)]\n",
    "\n",
    "cgm_test['cgm_sequential'] = cgm_test['cgm'].apply(cgm_to_steps)\n",
    "cgm_train['cgm_sequential'] = cgm_train['cgm'].apply(cgm_to_steps)\n",
    "cgm_train['when_to_eat'] = cgm_train[['Breakfast Time', 'Lunch Time']].apply(lambda x: time_to_step(x['Breakfast Time'], x['Lunch Time']), axis=1)\n",
    "cgm_test['when_to_eat'] = cgm_test[['Breakfast Time', 'Lunch Time']].apply(lambda x: time_to_step(x['Breakfast Time'], x['Lunch Time']), axis=1)\n",
    "viome_test = pd.read_csv('633FinalData/demo_viome_test.csv')\n",
    "viome_train = pd.read_csv('633FinalData/demo_viome_train.csv')\n",
    "viome_test= pd.get_dummies(viome_test, columns=['Race'])\n",
    "viome_train= pd.get_dummies(viome_train, columns=['Race'])\n",
    "viome_test['viome_sequential'] = viome_test['Viome'].apply(lambda x :[float(x) for x in x.split(',')])\n",
    "viome_train['viome_sequential'] = viome_train['Viome'].apply(lambda x :[float(x) for x in x.split(',')])\n",
    "combined_train = pd.merge(cgm_train, img_train, on=['Subject ID', 'Day'])\n",
    "combined_train = pd.merge(combined_train, viome_train, on=['Subject ID'])\n",
    "combined_test = pd.merge(cgm_test, img_test, on=['Subject ID', 'Day'])\n",
    "combined_test = pd.merge(combined_test, viome_test, on=['Subject ID'])\n",
    "to_drop_train = ['Subject ID','Day','Breakfast Time','Lunch Time','CGM Data','Image Before Breakfast','Image Before Lunch','Viome']\n",
    "combined_train = combined_train.drop(to_drop_train, axis=1)\n",
    "to_drop_test = ['Subject ID','Day','Breakfast Time','Lunch Time','CGM Data','Image Before Breakfast','Image Before Lunch','Viome']\n",
    "to_drop_test = ['Subject ID','Day','Breakfast Time','Lunch Time','CGM Data','Image Before Breakfast','Image Before Lunch','Viome']\n",
    "combined_test = combined_test.drop(to_drop_train, axis=1)\n",
    "label_train = pd.read_csv('633FinalData/label_train.csv')\n",
    "label_train = label_train['Lunch Calories']\n",
    "combined_train['label'] = label_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomDataset(Dataset):\n",
    "    def __init__(self, dataframe):\n",
    "        self.dataframe = dataframe\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.dataframe)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        row = self.dataframe.iloc[idx]\n",
    "\n",
    "        # Small epsilon value to avoid division by zero\n",
    "        epsilon = 1e-6\n",
    "\n",
    "        # Extract sequential data\n",
    "        cgm_sequential = np.array(row['cgm_sequential'], dtype=float)\n",
    "        when_to_eat = np.array(row['when_to_eat'], dtype=float)\n",
    "\n",
    "        # Normalize sequential data\n",
    "        cgm_sequential = (cgm_sequential - np.mean(cgm_sequential)) / (np.std(cgm_sequential) + epsilon)\n",
    "        when_to_eat = (when_to_eat - np.mean(when_to_eat)) / (np.std(when_to_eat) + epsilon)\n",
    "\n",
    "        # Stack cgm_sequential and when_to_eat to create a sequential data array with 2 features\n",
    "        sequential_data = np.stack((cgm_sequential, when_to_eat), axis=-1)\n",
    "\n",
    "        # Extract image data\n",
    "        img_b = np.array(row['img_b'], dtype=float)\n",
    "        img_l = np.array(row['img_l'], dtype=float)\n",
    "\n",
    "        # Normalize image data\n",
    "        img_b = (img_b - np.mean(img_b)) / (np.std(img_b) + epsilon)\n",
    "        img_l = (img_l - np.mean(img_l)) / (np.std(img_l) + epsilon)\n",
    "\n",
    "        # Ensure img_b has 3 channels\n",
    "        if img_b.ndim == 2:  # Grayscale image\n",
    "            img_b = np.stack((img_b, img_b, img_b), axis=-1)\n",
    "\n",
    "        # Ensure img_l has 3 channels\n",
    "        if img_l.ndim == 2:  # Grayscale image\n",
    "            img_l = np.stack((img_l, img_l, img_l), axis=-1)\n",
    "\n",
    "        # Convert image data to PIL Image and then to tensor\n",
    "        img_b = Image.fromarray(np.uint8(img_b))\n",
    "        img_l = Image.fromarray(np.uint8(img_l))\n",
    "        img_b = torch.tensor(np.array(img_b), dtype=torch.float32).permute(2, 0, 1)  # Convert to CxHxW\n",
    "        img_l = torch.tensor(np.array(img_l), dtype=torch.float32).permute(2, 0, 1)  # Convert to CxHxW\n",
    "\n",
    "        # Extract numeric data\n",
    "        numeric_data = row[['Age', 'Gender', 'Weight', 'Height', 'Diabetes Status', 'A1C',\n",
    "                            'Baseline Fasting Glucose', 'Insulin', 'Triglycerides', 'Cholesterol',\n",
    "                            'HDL', 'Non-HDL', 'LDL', 'VLDL', 'CHO/HDL Ratio', 'HOMA-IR', 'BMI',\n",
    "                            'Race_African American', 'Race_Hispanic/Latino', 'Race_White']].values.astype(np.float32)\n",
    "\n",
    "        # Normalize numeric data\n",
    "        numeric_data = (numeric_data - np.mean(numeric_data)) / (np.std(numeric_data) + epsilon)\n",
    "        numeric_data = torch.tensor(numeric_data, dtype=torch.float32)\n",
    "\n",
    "        # Extract and reshape viome_sequential\n",
    "        viome_sequential = np.array(row['viome_sequential'], dtype=float).reshape(27, 1)\n",
    "\n",
    "        # Normalize viome_sequential\n",
    "        viome_sequential = (viome_sequential - np.mean(viome_sequential)) / (np.std(viome_sequential) + epsilon)\n",
    "\n",
    "        label_train = row['label']\n",
    "\n",
    "        return {\n",
    "            'sequential_data': torch.tensor(sequential_data, dtype=torch.float32),\n",
    "            'viome_sequential': torch.tensor(viome_sequential, dtype=torch.float32),\n",
    "            'img_b': img_b,\n",
    "            'img_l': img_l,\n",
    "            'numeric_data': numeric_data,\n",
    "            'label': label_train\n",
    "        }\n",
    "        \n",
    "class CustomTestDataset(Dataset):\n",
    "    def __init__(self, dataframe):\n",
    "        self.dataframe = dataframe\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.dataframe)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        row = self.dataframe.iloc[idx]\n",
    "\n",
    "        # Small epsilon value to avoid division by zero\n",
    "        epsilon = 1e-6\n",
    "\n",
    "        # Extract sequential data\n",
    "        cgm_sequential = np.array(row['cgm_sequential'], dtype=float)\n",
    "        when_to_eat = np.array(row['when_to_eat'], dtype=float)\n",
    "\n",
    "        # Normalize sequential data\n",
    "        cgm_sequential = (cgm_sequential - np.mean(cgm_sequential)) / (np.std(cgm_sequential) + epsilon)\n",
    "        when_to_eat = (when_to_eat - np.mean(when_to_eat)) / (np.std(when_to_eat) + epsilon)\n",
    "\n",
    "        # Stack cgm_sequential and when_to_eat to create a sequential data array with 2 features\n",
    "        sequential_data = np.stack((cgm_sequential, when_to_eat), axis=-1)\n",
    "\n",
    "        # Extract image data\n",
    "        img_b = np.array(row['img_b'], dtype=float)\n",
    "        img_l = np.array(row['img_l'], dtype=float)\n",
    "\n",
    "        # Normalize image data\n",
    "        img_b = (img_b - np.mean(img_b)) / (np.std(img_b) + epsilon)\n",
    "        img_l = (img_l - np.mean(img_l)) / (np.std(img_l) + epsilon)\n",
    "\n",
    "        # Ensure img_b has 3 channels\n",
    "        if img_b.ndim == 2:  # Grayscale image\n",
    "            img_b = np.stack((img_b, img_b, img_b), axis=-1)\n",
    "\n",
    "        # Ensure img_l has 3 channels\n",
    "        if img_l.ndim == 2:  # Grayscale image\n",
    "            img_l = np.stack((img_l, img_l, img_l), axis=-1)\n",
    "\n",
    "        # Convert image data to PIL Image and then to tensor\n",
    "        img_b = Image.fromarray(np.uint8(img_b))\n",
    "        img_l = Image.fromarray(np.uint8(img_l))\n",
    "        img_b = torch.tensor(np.array(img_b), dtype=torch.float32).permute(2, 0, 1)  # Convert to CxHxW\n",
    "        img_l = torch.tensor(np.array(img_l), dtype=torch.float32).permute(2, 0, 1)  # Convert to CxHxW\n",
    "\n",
    "        # Extract numeric data\n",
    "        numeric_data = row[['Age', 'Gender', 'Weight', 'Height', 'Diabetes Status', 'A1C',\n",
    "                            'Baseline Fasting Glucose', 'Insulin', 'Triglycerides', 'Cholesterol',\n",
    "                            'HDL', 'Non-HDL', 'LDL', 'VLDL', 'CHO/HDL Ratio', 'HOMA-IR', 'BMI',\n",
    "                            'Race_African American', 'Race_Hispanic/Latino', 'Race_White']].values.astype(np.float32)\n",
    "\n",
    "        # Normalize numeric data\n",
    "        numeric_data = (numeric_data - np.mean(numeric_data)) / (np.std(numeric_data) + epsilon)\n",
    "        numeric_data = torch.tensor(numeric_data, dtype=torch.float32)\n",
    "\n",
    "        # Extract and reshape viome_sequential\n",
    "        viome_sequential = np.array(row['viome_sequential'], dtype=float).reshape(27, 1)\n",
    "\n",
    "        # Normalize viome_sequential\n",
    "        viome_sequential = (viome_sequential - np.mean(viome_sequential)) / (np.std(viome_sequential) + epsilon)\n",
    "\n",
    "\n",
    "        return {\n",
    "            'sequential_data': torch.tensor(sequential_data, dtype=torch.float32),\n",
    "            'viome_sequential': torch.tensor(viome_sequential, dtype=torch.float32),\n",
    "            'img_b': img_b,\n",
    "            'img_l': img_l,\n",
    "            'numeric_data': numeric_data,\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample 0:\n",
      "  sequential_data shape: torch.Size([288, 2])\n",
      "  viome_sequential shape: torch.Size([27, 1])\n",
      "  img_b shape: torch.Size([3, 64, 64])\n",
      "  img_l shape: torch.Size([3, 64, 64])\n",
      "  numeric_data shape: torch.Size([20])\n",
      "  label: 830\n"
     ]
    }
   ],
   "source": [
    "dataset_train = CustomDataset(combined_train)\n",
    "dataset_test = CustomTestDataset(combined_test)\n",
    "combined_train['img_b'].apply(lambda x: x.shape).unique()\n",
    "\n",
    "for i in range(1):\n",
    "    sample = dataset_train[i]\n",
    "    print(f\"Sample {i}:\")\n",
    "    print(f\"  sequential_data shape: {sample['sequential_data'].shape}\")\n",
    "    print(f\"  viome_sequential shape: {sample['viome_sequential'].shape}\")\n",
    "    print(f\"  img_b shape: {sample['img_b'].shape}\")\n",
    "    print(f\"  img_l shape: {sample['img_l'].shape}\")\n",
    "    print(f\"  numeric_data shape: {sample['numeric_data'].shape}\")\n",
    "    print(f\"  label: {sample['label']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader\n",
    "import torchvision.models as models\n",
    "\n",
    "class CombinedModel(nn.Module):\n",
    "    def __init__(self, hidden_size=64):\n",
    "        super(CombinedModel, self).__init__()\n",
    "        \n",
    "        # Sequential data sub-network\n",
    "        self.seq_net = nn.LSTM(input_size=2, hidden_size=hidden_size, num_layers=1, batch_first=True)\n",
    "        self.seq_fc = nn.Linear(hidden_size, 32)\n",
    "        \n",
    "        # Viome sequential data sub-network\n",
    "        self.viome_net = nn.LSTM(input_size=1, hidden_size=hidden_size, num_layers=1, batch_first=True)\n",
    "        self.viome_fc = nn.Linear(hidden_size, 32)\n",
    "        \n",
    "        # Image data sub-network using pre-trained ResNet\n",
    "        self.img_net = models.resnet50(pretrained=True)\n",
    "        self.img_net.fc = nn.Linear(self.img_net.fc.in_features, 32)  # Modify the final layer\n",
    "        \n",
    "        # Numeric data sub-network\n",
    "        self.num_net = nn.Sequential(\n",
    "            nn.Linear(20, 32),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(32, 16)\n",
    "        )\n",
    "        \n",
    "        # Combined fully connected layers\n",
    "        self.fc = nn.Sequential(\n",
    "            nn.Linear(32 + 32 + 32 + 32 + 16, 64),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(64, 1)  # Assuming a regression task\n",
    "        )\n",
    "    \n",
    "    def forward(self, sequential_data, img_b, img_l, numeric_data, viome_sequential):\n",
    "        # Process sequential data\n",
    "        seq_out, _ = self.seq_net(sequential_data)\n",
    "        seq_out = self.seq_fc(seq_out[:, -1, :])  # Take the last output of the LSTM and pass through a linear layer\n",
    "        \n",
    "        # Process viome sequential data\n",
    "        viome_out, _ = self.viome_net(viome_sequential)\n",
    "        viome_out = self.viome_fc(viome_out[:, -1, :])  # Take the last output of the LSTM and pass through a linear layer\n",
    "        \n",
    "        # Process image data\n",
    "        img_b_out = self.img_net(img_b)\n",
    "        img_l_out = self.img_net(img_l)\n",
    "        \n",
    "        # Process numeric data\n",
    "        num_out = self.num_net(numeric_data)\n",
    "        \n",
    "        # Concatenate all outputs\n",
    "        combined = torch.cat((seq_out, viome_out, img_b_out, img_l_out, num_out), dim=1)\n",
    "        \n",
    "        # Pass through fully connected layers\n",
    "        out = self.fc(combined)\n",
    "        \n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\10374\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torchvision\\models\\_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "c:\\Users\\10374\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torchvision\\models\\_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ResNet50_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet50_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/50], Loss: 0.9914306534661187\n",
      "Epoch [2/50], Loss: 0.9216074877315097\n",
      "Epoch [3/50], Loss: 0.7258072826597426\n",
      "Epoch [4/50], Loss: 0.4035237696435716\n",
      "Epoch [5/50], Loss: 0.3466027147240109\n",
      "Epoch [6/50], Loss: 0.33700646956761676\n",
      "Epoch [7/50], Loss: 0.33526159326235455\n",
      "Epoch [8/50], Loss: 0.3371482491493225\n",
      "Epoch [9/50], Loss: 0.3340679837597741\n",
      "Epoch [10/50], Loss: 0.33478137850761414\n",
      "Epoch [11/50], Loss: 0.3326859838432736\n",
      "Epoch [12/50], Loss: 0.33184731668896145\n",
      "Epoch [13/50], Loss: 0.334768854909473\n",
      "Epoch [14/50], Loss: 0.33731939726405674\n",
      "Epoch [15/50], Loss: 0.3347898523012797\n",
      "Epoch [16/50], Loss: 0.3354033629099528\n",
      "Epoch [17/50], Loss: 0.3323684599664476\n",
      "Epoch [18/50], Loss: 0.33615812328126693\n",
      "Epoch [19/50], Loss: 0.33494271172417533\n",
      "Epoch [20/50], Loss: 0.33474687735239667\n",
      "Epoch [21/50], Loss: 0.3345928324593438\n",
      "Epoch [22/50], Loss: 0.3344409465789795\n",
      "Epoch [23/50], Loss: 0.33568842543496025\n",
      "Epoch [24/50], Loss: 0.33447230855623883\n",
      "Epoch [25/50], Loss: 0.3347878224319882\n",
      "Epoch [26/50], Loss: 0.3374152183532715\n",
      "Epoch [27/50], Loss: 0.3352941373984019\n",
      "Epoch [28/50], Loss: 0.3343766861491733\n",
      "Epoch [29/50], Loss: 0.33490058448579574\n",
      "Epoch [30/50], Loss: 0.33306030763520134\n",
      "Epoch [31/50], Loss: 0.3331166340245141\n",
      "Epoch [32/50], Loss: 0.33552297618654037\n",
      "Epoch [33/50], Loss: 0.33772111932436627\n",
      "Epoch [34/50], Loss: 0.3361862401167552\n",
      "Epoch [35/50], Loss: 0.33176950613657635\n",
      "Epoch [36/50], Loss: 0.3328709337446425\n",
      "Epoch [37/50], Loss: 0.33209315604633755\n",
      "Epoch [38/50], Loss: 0.3347630467679765\n",
      "Epoch [39/50], Loss: 0.3337745997640822\n",
      "Epoch [40/50], Loss: 0.3353876206609938\n",
      "Epoch [41/50], Loss: 0.3337201741006639\n",
      "Epoch [42/50], Loss: 0.33422831694285077\n",
      "Epoch [43/50], Loss: 0.33638032608562046\n",
      "Epoch [44/50], Loss: 0.3341337972217136\n",
      "Epoch [45/50], Loss: 0.3343702687157525\n",
      "Epoch [46/50], Loss: 0.3351069258319007\n",
      "Epoch [47/50], Loss: 0.3349515431457096\n",
      "Epoch [48/50], Loss: 0.33623184429274666\n",
      "Epoch [49/50], Loss: 0.33356776502397323\n",
      "Epoch [50/50], Loss: 0.33543501959906685\n",
      "Training complete.\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "epoch_losses = []\n",
    "\n",
    "class RMSRELoss(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(RMSRELoss, self).__init__()\n",
    "\n",
    "    def forward(self, y_pred, y_true):\n",
    "        epsilon = 1e-6  # Small value to avoid division by zero\n",
    "        relative_error = (y_pred - y_true) / (y_true + epsilon)\n",
    "        return torch.sqrt(torch.mean(relative_error ** 2))\n",
    "    \n",
    "train_loader = DataLoader(dataset_train, batch_size=32, shuffle=True)\n",
    "test_loader = DataLoader(dataset_test, batch_size=32, shuffle=False)\n",
    "\n",
    "model = CombinedModel()\n",
    "criterion = RMSRELoss()  \n",
    "optimizer = optim.Adam(model.parameters(), lr=1e-3)  # Reduced learning rate\n",
    "\n",
    "# Learning rate scheduler\n",
    "scheduler = optim.lr_scheduler.StepLR(optimizer, step_size=50, gamma=0.1)\n",
    "# 检查是否有可用的GPU\n",
    "device = torch.device(\"cuda\")\n",
    "\n",
    "# 将模型和损失函数移动到设备\n",
    "model = model.to(device)\n",
    "criterion = criterion.to(device)\n",
    "\n",
    "num_epochs = 50\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "    for batch in train_loader:\n",
    "        # 将数据移动到设备\n",
    "        sequential_data = batch['sequential_data'].to(device)\n",
    "        img_b = batch['img_b'].to(device)\n",
    "        img_l = batch['img_l'].to(device)\n",
    "        numeric_data = batch['numeric_data'].to(device)\n",
    "        viome_sequential = batch['viome_sequential'].to(device)\n",
    "        label = batch['label'].to(device)\n",
    "        \n",
    "        # Forward pass\n",
    "        outputs = model(sequential_data, img_b, img_l, numeric_data, viome_sequential)\n",
    "        loss = criterion(outputs, label)\n",
    "        \n",
    "        # Backward pass and optimization\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        running_loss += loss.item()\n",
    "    \n",
    "    # 计算并保存每个 epoch 的平均损失\n",
    "    epoch_loss = running_loss / len(train_loader)\n",
    "    epoch_losses.append(epoch_loss)\n",
    "    print(f\"Epoch [{epoch+1}/{num_epochs}], Loss: {epoch_loss}\")\n",
    "\n",
    "    # 更新学习率\n",
    "    scheduler.step()\n",
    "\n",
    "print(\"Training complete.\")\n",
    "\n",
    "# 绘制训练过程中的 loss 变化\n",
    "plt.plot(range(1, num_epochs + 1), epoch_losses, marker='o')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.title('Training Loss over Epochs')\n",
    "plt.grid(True)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predictions saved to 'submission.csv' in Kaggle format.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "model.eval()  # Set the model to evaluation mode\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = model.to(device)\n",
    "test_loss = 0.0\n",
    "all_outputs = []\n",
    "all_labels = []\n",
    "\n",
    "with torch.no_grad():  # Disable gradient calculation\n",
    "    for batch in test_loader:\n",
    "        # 将数据移动到 GPU\n",
    "        sequential_data = batch['sequential_data'].to(device)\n",
    "        img_b = batch['img_b'].to(device)\n",
    "        img_l = batch['img_l'].to(device)\n",
    "        numeric_data = batch['numeric_data'].to(device)\n",
    "        viome_sequential = batch['viome_sequential'].to(device)\n",
    "        \n",
    "        # Forward pass\n",
    "        outputs = model(sequential_data, img_b, img_l, numeric_data, viome_sequential)\n",
    "        \n",
    "        # 收集预测结果\n",
    "        all_outputs += ([i[0] for i in outputs.tolist()])\n",
    "        \n",
    "test_ids = np.arange(len(all_outputs))  # row_id 从 0 开始，按顺序生成\n",
    "submission = pd.DataFrame({\"row_id\": test_ids, \"label\": all_outputs})\n",
    "submission.to_csv(\"submission.csv\", index=False)\n",
    "print(\"Predictions saved to 'submission.csv' in Kaggle format.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\10374\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torchvision\\models\\_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "c:\\Users\\10374\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torchvision\\models\\_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ResNet50_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet50_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/50], Hidden Size: 32, LR: 0.001, Batch Size: 16, Loss: 0.9600\n",
      "Epoch [2/50], Hidden Size: 32, LR: 0.001, Batch Size: 16, Loss: 0.5766\n",
      "Epoch [3/50], Hidden Size: 32, LR: 0.001, Batch Size: 16, Loss: 0.3553\n",
      "Epoch [4/50], Hidden Size: 32, LR: 0.001, Batch Size: 16, Loss: 0.3318\n",
      "Epoch [5/50], Hidden Size: 32, LR: 0.001, Batch Size: 16, Loss: 0.3347\n",
      "Epoch [6/50], Hidden Size: 32, LR: 0.001, Batch Size: 16, Loss: 0.3332\n",
      "Epoch [7/50], Hidden Size: 32, LR: 0.001, Batch Size: 16, Loss: 0.3324\n",
      "Epoch [8/50], Hidden Size: 32, LR: 0.001, Batch Size: 16, Loss: 0.3352\n",
      "Epoch [9/50], Hidden Size: 32, LR: 0.001, Batch Size: 16, Loss: 0.3388\n",
      "Epoch [10/50], Hidden Size: 32, LR: 0.001, Batch Size: 16, Loss: 0.3352\n",
      "Epoch [11/50], Hidden Size: 32, LR: 0.001, Batch Size: 16, Loss: 0.3390\n",
      "Epoch [12/50], Hidden Size: 32, LR: 0.001, Batch Size: 16, Loss: 0.3424\n",
      "Epoch [13/50], Hidden Size: 32, LR: 0.001, Batch Size: 16, Loss: 0.3339\n",
      "Epoch [14/50], Hidden Size: 32, LR: 0.001, Batch Size: 16, Loss: 0.3358\n",
      "Epoch [15/50], Hidden Size: 32, LR: 0.001, Batch Size: 16, Loss: 0.3409\n",
      "Epoch [16/50], Hidden Size: 32, LR: 0.001, Batch Size: 16, Loss: 0.3410\n",
      "Epoch [17/50], Hidden Size: 32, LR: 0.001, Batch Size: 16, Loss: 0.3376\n",
      "Epoch [18/50], Hidden Size: 32, LR: 0.001, Batch Size: 16, Loss: 0.3384\n",
      "Epoch [19/50], Hidden Size: 32, LR: 0.001, Batch Size: 16, Loss: 0.3330\n",
      "Epoch [20/50], Hidden Size: 32, LR: 0.001, Batch Size: 16, Loss: 0.3353\n",
      "Epoch [21/50], Hidden Size: 32, LR: 0.001, Batch Size: 16, Loss: 0.3344\n",
      "Epoch [22/50], Hidden Size: 32, LR: 0.001, Batch Size: 16, Loss: 0.3369\n",
      "Epoch [23/50], Hidden Size: 32, LR: 0.001, Batch Size: 16, Loss: 0.3359\n",
      "Epoch [24/50], Hidden Size: 32, LR: 0.001, Batch Size: 16, Loss: 0.3305\n",
      "Epoch [25/50], Hidden Size: 32, LR: 0.001, Batch Size: 16, Loss: 0.3344\n",
      "Epoch [26/50], Hidden Size: 32, LR: 0.001, Batch Size: 16, Loss: 0.3380\n",
      "Epoch [27/50], Hidden Size: 32, LR: 0.001, Batch Size: 16, Loss: 0.3326\n",
      "Epoch [28/50], Hidden Size: 32, LR: 0.001, Batch Size: 16, Loss: 0.3328\n",
      "Epoch [29/50], Hidden Size: 32, LR: 0.001, Batch Size: 16, Loss: 0.3349\n",
      "Epoch [30/50], Hidden Size: 32, LR: 0.001, Batch Size: 16, Loss: 0.3336\n",
      "Epoch [31/50], Hidden Size: 32, LR: 0.001, Batch Size: 16, Loss: 0.3337\n",
      "Epoch [32/50], Hidden Size: 32, LR: 0.001, Batch Size: 16, Loss: 0.3358\n",
      "Epoch [33/50], Hidden Size: 32, LR: 0.001, Batch Size: 16, Loss: 0.3355\n",
      "Epoch [34/50], Hidden Size: 32, LR: 0.001, Batch Size: 16, Loss: 0.3364\n",
      "Epoch [35/50], Hidden Size: 32, LR: 0.001, Batch Size: 16, Loss: 0.3356\n",
      "Epoch [36/50], Hidden Size: 32, LR: 0.001, Batch Size: 16, Loss: 0.3389\n",
      "Epoch [37/50], Hidden Size: 32, LR: 0.001, Batch Size: 16, Loss: 0.3378\n",
      "Epoch [38/50], Hidden Size: 32, LR: 0.001, Batch Size: 16, Loss: 0.3333\n",
      "Epoch [39/50], Hidden Size: 32, LR: 0.001, Batch Size: 16, Loss: 0.3332\n",
      "Epoch [40/50], Hidden Size: 32, LR: 0.001, Batch Size: 16, Loss: 0.3369\n",
      "Epoch [41/50], Hidden Size: 32, LR: 0.001, Batch Size: 16, Loss: 0.3348\n",
      "Epoch [42/50], Hidden Size: 32, LR: 0.001, Batch Size: 16, Loss: 0.3300\n",
      "Epoch [43/50], Hidden Size: 32, LR: 0.001, Batch Size: 16, Loss: 0.3311\n",
      "Epoch [44/50], Hidden Size: 32, LR: 0.001, Batch Size: 16, Loss: 0.3373\n",
      "Epoch [45/50], Hidden Size: 32, LR: 0.001, Batch Size: 16, Loss: 0.3318\n",
      "Epoch [46/50], Hidden Size: 32, LR: 0.001, Batch Size: 16, Loss: 0.3340\n",
      "Epoch [47/50], Hidden Size: 32, LR: 0.001, Batch Size: 16, Loss: 0.3327\n",
      "Epoch [48/50], Hidden Size: 32, LR: 0.001, Batch Size: 16, Loss: 0.3313\n",
      "Epoch [49/50], Hidden Size: 32, LR: 0.001, Batch Size: 16, Loss: 0.3347\n",
      "Epoch [50/50], Hidden Size: 32, LR: 0.001, Batch Size: 16, Loss: 0.3408\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\10374\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torchvision\\models\\_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "c:\\Users\\10374\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torchvision\\models\\_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ResNet50_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet50_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/50], Hidden Size: 32, LR: 0.001, Batch Size: 32, Loss: 0.9898\n",
      "Epoch [2/50], Hidden Size: 32, LR: 0.001, Batch Size: 32, Loss: 0.9086\n",
      "Epoch [3/50], Hidden Size: 32, LR: 0.001, Batch Size: 32, Loss: 0.7010\n",
      "Epoch [4/50], Hidden Size: 32, LR: 0.001, Batch Size: 32, Loss: 0.3977\n",
      "Epoch [5/50], Hidden Size: 32, LR: 0.001, Batch Size: 32, Loss: 0.3395\n",
      "Epoch [6/50], Hidden Size: 32, LR: 0.001, Batch Size: 32, Loss: 0.3421\n",
      "Epoch [7/50], Hidden Size: 32, LR: 0.001, Batch Size: 32, Loss: 0.3420\n",
      "Epoch [8/50], Hidden Size: 32, LR: 0.001, Batch Size: 32, Loss: 0.3376\n",
      "Epoch [9/50], Hidden Size: 32, LR: 0.001, Batch Size: 32, Loss: 0.3344\n",
      "Epoch [10/50], Hidden Size: 32, LR: 0.001, Batch Size: 32, Loss: 0.3342\n",
      "Epoch [11/50], Hidden Size: 32, LR: 0.001, Batch Size: 32, Loss: 0.3309\n",
      "Epoch [12/50], Hidden Size: 32, LR: 0.001, Batch Size: 32, Loss: 0.3351\n",
      "Epoch [13/50], Hidden Size: 32, LR: 0.001, Batch Size: 32, Loss: 0.3347\n",
      "Epoch [14/50], Hidden Size: 32, LR: 0.001, Batch Size: 32, Loss: 0.3363\n",
      "Epoch [15/50], Hidden Size: 32, LR: 0.001, Batch Size: 32, Loss: 0.3345\n",
      "Epoch [16/50], Hidden Size: 32, LR: 0.001, Batch Size: 32, Loss: 0.3351\n",
      "Epoch [17/50], Hidden Size: 32, LR: 0.001, Batch Size: 32, Loss: 0.3342\n",
      "Epoch [18/50], Hidden Size: 32, LR: 0.001, Batch Size: 32, Loss: 0.3339\n",
      "Epoch [19/50], Hidden Size: 32, LR: 0.001, Batch Size: 32, Loss: 0.3344\n",
      "Epoch [20/50], Hidden Size: 32, LR: 0.001, Batch Size: 32, Loss: 0.3373\n",
      "Epoch [21/50], Hidden Size: 32, LR: 0.001, Batch Size: 32, Loss: 0.3339\n",
      "Epoch [22/50], Hidden Size: 32, LR: 0.001, Batch Size: 32, Loss: 0.3351\n",
      "Epoch [23/50], Hidden Size: 32, LR: 0.001, Batch Size: 32, Loss: 0.3351\n",
      "Epoch [24/50], Hidden Size: 32, LR: 0.001, Batch Size: 32, Loss: 0.3341\n",
      "Epoch [25/50], Hidden Size: 32, LR: 0.001, Batch Size: 32, Loss: 0.3326\n",
      "Epoch [26/50], Hidden Size: 32, LR: 0.001, Batch Size: 32, Loss: 0.3339\n",
      "Epoch [27/50], Hidden Size: 32, LR: 0.001, Batch Size: 32, Loss: 0.3339\n",
      "Epoch [28/50], Hidden Size: 32, LR: 0.001, Batch Size: 32, Loss: 0.3342\n",
      "Epoch [29/50], Hidden Size: 32, LR: 0.001, Batch Size: 32, Loss: 0.3345\n",
      "Epoch [30/50], Hidden Size: 32, LR: 0.001, Batch Size: 32, Loss: 0.3334\n",
      "Epoch [31/50], Hidden Size: 32, LR: 0.001, Batch Size: 32, Loss: 0.3339\n",
      "Epoch [32/50], Hidden Size: 32, LR: 0.001, Batch Size: 32, Loss: 0.3334\n",
      "Epoch [33/50], Hidden Size: 32, LR: 0.001, Batch Size: 32, Loss: 0.3340\n",
      "Epoch [34/50], Hidden Size: 32, LR: 0.001, Batch Size: 32, Loss: 0.3353\n",
      "Epoch [35/50], Hidden Size: 32, LR: 0.001, Batch Size: 32, Loss: 0.3369\n",
      "Epoch [36/50], Hidden Size: 32, LR: 0.001, Batch Size: 32, Loss: 0.3362\n",
      "Epoch [37/50], Hidden Size: 32, LR: 0.001, Batch Size: 32, Loss: 0.3363\n",
      "Epoch [38/50], Hidden Size: 32, LR: 0.001, Batch Size: 32, Loss: 0.3373\n",
      "Epoch [39/50], Hidden Size: 32, LR: 0.001, Batch Size: 32, Loss: 0.3347\n",
      "Epoch [40/50], Hidden Size: 32, LR: 0.001, Batch Size: 32, Loss: 0.3325\n",
      "Epoch [41/50], Hidden Size: 32, LR: 0.001, Batch Size: 32, Loss: 0.3356\n",
      "Epoch [42/50], Hidden Size: 32, LR: 0.001, Batch Size: 32, Loss: 0.3360\n",
      "Epoch [43/50], Hidden Size: 32, LR: 0.001, Batch Size: 32, Loss: 0.3338\n",
      "Epoch [44/50], Hidden Size: 32, LR: 0.001, Batch Size: 32, Loss: 0.3398\n",
      "Epoch [45/50], Hidden Size: 32, LR: 0.001, Batch Size: 32, Loss: 0.3362\n",
      "Epoch [46/50], Hidden Size: 32, LR: 0.001, Batch Size: 32, Loss: 0.3349\n",
      "Epoch [47/50], Hidden Size: 32, LR: 0.001, Batch Size: 32, Loss: 0.3353\n",
      "Epoch [48/50], Hidden Size: 32, LR: 0.001, Batch Size: 32, Loss: 0.3360\n",
      "Epoch [49/50], Hidden Size: 32, LR: 0.001, Batch Size: 32, Loss: 0.3347\n",
      "Epoch [50/50], Hidden Size: 32, LR: 0.001, Batch Size: 32, Loss: 0.3349\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\10374\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torchvision\\models\\_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "c:\\Users\\10374\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torchvision\\models\\_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ResNet50_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet50_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/50], Hidden Size: 32, LR: 0.001, Batch Size: 64, Loss: 0.9981\n",
      "Epoch [2/50], Hidden Size: 32, LR: 0.001, Batch Size: 64, Loss: 0.9854\n",
      "Epoch [3/50], Hidden Size: 32, LR: 0.001, Batch Size: 64, Loss: 0.9512\n",
      "Epoch [4/50], Hidden Size: 32, LR: 0.001, Batch Size: 64, Loss: 0.8846\n",
      "Epoch [5/50], Hidden Size: 32, LR: 0.001, Batch Size: 64, Loss: 0.7793\n",
      "Epoch [6/50], Hidden Size: 32, LR: 0.001, Batch Size: 64, Loss: 0.6092\n",
      "Epoch [7/50], Hidden Size: 32, LR: 0.001, Batch Size: 64, Loss: 0.4212\n",
      "Epoch [8/50], Hidden Size: 32, LR: 0.001, Batch Size: 64, Loss: 0.3415\n",
      "Epoch [9/50], Hidden Size: 32, LR: 0.001, Batch Size: 64, Loss: 0.3593\n",
      "Epoch [10/50], Hidden Size: 32, LR: 0.001, Batch Size: 64, Loss: 0.3427\n",
      "Epoch [11/50], Hidden Size: 32, LR: 0.001, Batch Size: 64, Loss: 0.3379\n",
      "Epoch [12/50], Hidden Size: 32, LR: 0.001, Batch Size: 64, Loss: 0.3312\n",
      "Epoch [13/50], Hidden Size: 32, LR: 0.001, Batch Size: 64, Loss: 0.3334\n",
      "Epoch [14/50], Hidden Size: 32, LR: 0.001, Batch Size: 64, Loss: 0.3398\n",
      "Epoch [15/50], Hidden Size: 32, LR: 0.001, Batch Size: 64, Loss: 0.3288\n",
      "Epoch [16/50], Hidden Size: 32, LR: 0.001, Batch Size: 64, Loss: 0.3235\n",
      "Epoch [17/50], Hidden Size: 32, LR: 0.001, Batch Size: 64, Loss: 0.3324\n",
      "Epoch [18/50], Hidden Size: 32, LR: 0.001, Batch Size: 64, Loss: 0.3290\n",
      "Epoch [19/50], Hidden Size: 32, LR: 0.001, Batch Size: 64, Loss: 0.3414\n",
      "Epoch [20/50], Hidden Size: 32, LR: 0.001, Batch Size: 64, Loss: 0.3335\n",
      "Epoch [21/50], Hidden Size: 32, LR: 0.001, Batch Size: 64, Loss: 0.3325\n",
      "Epoch [22/50], Hidden Size: 32, LR: 0.001, Batch Size: 64, Loss: 0.3327\n",
      "Epoch [23/50], Hidden Size: 32, LR: 0.001, Batch Size: 64, Loss: 0.3309\n",
      "Epoch [24/50], Hidden Size: 32, LR: 0.001, Batch Size: 64, Loss: 0.3318\n",
      "Epoch [25/50], Hidden Size: 32, LR: 0.001, Batch Size: 64, Loss: 0.3387\n",
      "Epoch [26/50], Hidden Size: 32, LR: 0.001, Batch Size: 64, Loss: 0.3382\n",
      "Epoch [27/50], Hidden Size: 32, LR: 0.001, Batch Size: 64, Loss: 0.3314\n",
      "Epoch [28/50], Hidden Size: 32, LR: 0.001, Batch Size: 64, Loss: 0.3342\n",
      "Epoch [29/50], Hidden Size: 32, LR: 0.001, Batch Size: 64, Loss: 0.3389\n",
      "Epoch [30/50], Hidden Size: 32, LR: 0.001, Batch Size: 64, Loss: 0.3383\n",
      "Epoch [31/50], Hidden Size: 32, LR: 0.001, Batch Size: 64, Loss: 0.3288\n",
      "Epoch [32/50], Hidden Size: 32, LR: 0.001, Batch Size: 64, Loss: 0.3323\n",
      "Epoch [33/50], Hidden Size: 32, LR: 0.001, Batch Size: 64, Loss: 0.3387\n",
      "Epoch [34/50], Hidden Size: 32, LR: 0.001, Batch Size: 64, Loss: 0.3324\n",
      "Epoch [35/50], Hidden Size: 32, LR: 0.001, Batch Size: 64, Loss: 0.3344\n",
      "Epoch [36/50], Hidden Size: 32, LR: 0.001, Batch Size: 64, Loss: 0.3327\n",
      "Epoch [37/50], Hidden Size: 32, LR: 0.001, Batch Size: 64, Loss: 0.3323\n",
      "Epoch [38/50], Hidden Size: 32, LR: 0.001, Batch Size: 64, Loss: 0.3324\n",
      "Epoch [39/50], Hidden Size: 32, LR: 0.001, Batch Size: 64, Loss: 0.3349\n",
      "Epoch [40/50], Hidden Size: 32, LR: 0.001, Batch Size: 64, Loss: 0.3412\n",
      "Epoch [41/50], Hidden Size: 32, LR: 0.001, Batch Size: 64, Loss: 0.3292\n",
      "Epoch [42/50], Hidden Size: 32, LR: 0.001, Batch Size: 64, Loss: 0.3368\n",
      "Epoch [43/50], Hidden Size: 32, LR: 0.001, Batch Size: 64, Loss: 0.3275\n",
      "Epoch [44/50], Hidden Size: 32, LR: 0.001, Batch Size: 64, Loss: 0.3353\n",
      "Epoch [45/50], Hidden Size: 32, LR: 0.001, Batch Size: 64, Loss: 0.3384\n",
      "Epoch [46/50], Hidden Size: 32, LR: 0.001, Batch Size: 64, Loss: 0.3372\n",
      "Epoch [47/50], Hidden Size: 32, LR: 0.001, Batch Size: 64, Loss: 0.3303\n",
      "Epoch [48/50], Hidden Size: 32, LR: 0.001, Batch Size: 64, Loss: 0.3313\n",
      "Epoch [49/50], Hidden Size: 32, LR: 0.001, Batch Size: 64, Loss: 0.3402\n",
      "Epoch [50/50], Hidden Size: 32, LR: 0.001, Batch Size: 64, Loss: 0.3318\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\10374\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torchvision\\models\\_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "c:\\Users\\10374\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torchvision\\models\\_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ResNet50_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet50_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/50], Hidden Size: 32, LR: 0.0001, Batch Size: 16, Loss: 0.9997\n",
      "Epoch [2/50], Hidden Size: 32, LR: 0.0001, Batch Size: 16, Loss: 0.9975\n",
      "Epoch [3/50], Hidden Size: 32, LR: 0.0001, Batch Size: 16, Loss: 0.9929\n",
      "Epoch [4/50], Hidden Size: 32, LR: 0.0001, Batch Size: 16, Loss: 0.9839\n",
      "Epoch [5/50], Hidden Size: 32, LR: 0.0001, Batch Size: 16, Loss: 0.9662\n",
      "Epoch [6/50], Hidden Size: 32, LR: 0.0001, Batch Size: 16, Loss: 0.9362\n",
      "Epoch [7/50], Hidden Size: 32, LR: 0.0001, Batch Size: 16, Loss: 0.8903\n",
      "Epoch [8/50], Hidden Size: 32, LR: 0.0001, Batch Size: 16, Loss: 0.8310\n",
      "Epoch [9/50], Hidden Size: 32, LR: 0.0001, Batch Size: 16, Loss: 0.7568\n",
      "Epoch [10/50], Hidden Size: 32, LR: 0.0001, Batch Size: 16, Loss: 0.6739\n",
      "Epoch [11/50], Hidden Size: 32, LR: 0.0001, Batch Size: 16, Loss: 0.5772\n",
      "Epoch [12/50], Hidden Size: 32, LR: 0.0001, Batch Size: 16, Loss: 0.4687\n",
      "Epoch [13/50], Hidden Size: 32, LR: 0.0001, Batch Size: 16, Loss: 0.3842\n",
      "Epoch [14/50], Hidden Size: 32, LR: 0.0001, Batch Size: 16, Loss: 0.3401\n",
      "Epoch [15/50], Hidden Size: 32, LR: 0.0001, Batch Size: 16, Loss: 0.3331\n",
      "Epoch [16/50], Hidden Size: 32, LR: 0.0001, Batch Size: 16, Loss: 0.3345\n",
      "Epoch [17/50], Hidden Size: 32, LR: 0.0001, Batch Size: 16, Loss: 0.3343\n",
      "Epoch [18/50], Hidden Size: 32, LR: 0.0001, Batch Size: 16, Loss: 0.3287\n",
      "Epoch [19/50], Hidden Size: 32, LR: 0.0001, Batch Size: 16, Loss: 0.3307\n",
      "Epoch [20/50], Hidden Size: 32, LR: 0.0001, Batch Size: 16, Loss: 0.3326\n",
      "Epoch [21/50], Hidden Size: 32, LR: 0.0001, Batch Size: 16, Loss: 0.3351\n",
      "Epoch [22/50], Hidden Size: 32, LR: 0.0001, Batch Size: 16, Loss: 0.3330\n",
      "Epoch [23/50], Hidden Size: 32, LR: 0.0001, Batch Size: 16, Loss: 0.3310\n",
      "Epoch [24/50], Hidden Size: 32, LR: 0.0001, Batch Size: 16, Loss: 0.3316\n",
      "Epoch [25/50], Hidden Size: 32, LR: 0.0001, Batch Size: 16, Loss: 0.3325\n",
      "Epoch [26/50], Hidden Size: 32, LR: 0.0001, Batch Size: 16, Loss: 0.3332\n",
      "Epoch [27/50], Hidden Size: 32, LR: 0.0001, Batch Size: 16, Loss: 0.3345\n",
      "Epoch [28/50], Hidden Size: 32, LR: 0.0001, Batch Size: 16, Loss: 0.3344\n",
      "Epoch [29/50], Hidden Size: 32, LR: 0.0001, Batch Size: 16, Loss: 0.3298\n",
      "Epoch [30/50], Hidden Size: 32, LR: 0.0001, Batch Size: 16, Loss: 0.3330\n",
      "Epoch [31/50], Hidden Size: 32, LR: 0.0001, Batch Size: 16, Loss: 0.3294\n",
      "Epoch [32/50], Hidden Size: 32, LR: 0.0001, Batch Size: 16, Loss: 0.3323\n",
      "Epoch [33/50], Hidden Size: 32, LR: 0.0001, Batch Size: 16, Loss: 0.3332\n",
      "Epoch [34/50], Hidden Size: 32, LR: 0.0001, Batch Size: 16, Loss: 0.3309\n",
      "Epoch [35/50], Hidden Size: 32, LR: 0.0001, Batch Size: 16, Loss: 0.3343\n",
      "Epoch [36/50], Hidden Size: 32, LR: 0.0001, Batch Size: 16, Loss: 0.3338\n",
      "Epoch [37/50], Hidden Size: 32, LR: 0.0001, Batch Size: 16, Loss: 0.3339\n",
      "Epoch [38/50], Hidden Size: 32, LR: 0.0001, Batch Size: 16, Loss: 0.3283\n",
      "Epoch [39/50], Hidden Size: 32, LR: 0.0001, Batch Size: 16, Loss: 0.3320\n",
      "Epoch [40/50], Hidden Size: 32, LR: 0.0001, Batch Size: 16, Loss: 0.3339\n",
      "Epoch [41/50], Hidden Size: 32, LR: 0.0001, Batch Size: 16, Loss: 0.3334\n",
      "Epoch [42/50], Hidden Size: 32, LR: 0.0001, Batch Size: 16, Loss: 0.3330\n",
      "Epoch [43/50], Hidden Size: 32, LR: 0.0001, Batch Size: 16, Loss: 0.3338\n",
      "Epoch [44/50], Hidden Size: 32, LR: 0.0001, Batch Size: 16, Loss: 0.3302\n",
      "Epoch [45/50], Hidden Size: 32, LR: 0.0001, Batch Size: 16, Loss: 0.3310\n",
      "Epoch [46/50], Hidden Size: 32, LR: 0.0001, Batch Size: 16, Loss: 0.3345\n",
      "Epoch [47/50], Hidden Size: 32, LR: 0.0001, Batch Size: 16, Loss: 0.3313\n",
      "Epoch [48/50], Hidden Size: 32, LR: 0.0001, Batch Size: 16, Loss: 0.3333\n",
      "Epoch [49/50], Hidden Size: 32, LR: 0.0001, Batch Size: 16, Loss: 0.3293\n",
      "Epoch [50/50], Hidden Size: 32, LR: 0.0001, Batch Size: 16, Loss: 0.3335\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\10374\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torchvision\\models\\_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "c:\\Users\\10374\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torchvision\\models\\_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ResNet50_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet50_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/50], Hidden Size: 32, LR: 0.0001, Batch Size: 32, Loss: 0.9998\n",
      "Epoch [2/50], Hidden Size: 32, LR: 0.0001, Batch Size: 32, Loss: 0.9991\n",
      "Epoch [3/50], Hidden Size: 32, LR: 0.0001, Batch Size: 32, Loss: 0.9980\n",
      "Epoch [4/50], Hidden Size: 32, LR: 0.0001, Batch Size: 32, Loss: 0.9964\n",
      "Epoch [5/50], Hidden Size: 32, LR: 0.0001, Batch Size: 32, Loss: 0.9942\n",
      "Epoch [6/50], Hidden Size: 32, LR: 0.0001, Batch Size: 32, Loss: 0.9912\n",
      "Epoch [7/50], Hidden Size: 32, LR: 0.0001, Batch Size: 32, Loss: 0.9871\n",
      "Epoch [8/50], Hidden Size: 32, LR: 0.0001, Batch Size: 32, Loss: 0.9815\n",
      "Epoch [9/50], Hidden Size: 32, LR: 0.0001, Batch Size: 32, Loss: 0.9736\n",
      "Epoch [10/50], Hidden Size: 32, LR: 0.0001, Batch Size: 32, Loss: 0.9633\n",
      "Epoch [11/50], Hidden Size: 32, LR: 0.0001, Batch Size: 32, Loss: 0.9506\n",
      "Epoch [12/50], Hidden Size: 32, LR: 0.0001, Batch Size: 32, Loss: 0.9368\n",
      "Epoch [13/50], Hidden Size: 32, LR: 0.0001, Batch Size: 32, Loss: 0.9208\n",
      "Epoch [14/50], Hidden Size: 32, LR: 0.0001, Batch Size: 32, Loss: 0.9035\n",
      "Epoch [15/50], Hidden Size: 32, LR: 0.0001, Batch Size: 32, Loss: 0.8842\n",
      "Epoch [16/50], Hidden Size: 32, LR: 0.0001, Batch Size: 32, Loss: 0.8616\n",
      "Epoch [17/50], Hidden Size: 32, LR: 0.0001, Batch Size: 32, Loss: 0.8380\n",
      "Epoch [18/50], Hidden Size: 32, LR: 0.0001, Batch Size: 32, Loss: 0.8107\n",
      "Epoch [19/50], Hidden Size: 32, LR: 0.0001, Batch Size: 32, Loss: 0.7818\n",
      "Epoch [20/50], Hidden Size: 32, LR: 0.0001, Batch Size: 32, Loss: 0.7494\n",
      "Epoch [21/50], Hidden Size: 32, LR: 0.0001, Batch Size: 32, Loss: 0.7139\n",
      "Epoch [22/50], Hidden Size: 32, LR: 0.0001, Batch Size: 32, Loss: 0.6784\n",
      "Epoch [23/50], Hidden Size: 32, LR: 0.0001, Batch Size: 32, Loss: 0.6387\n",
      "Epoch [24/50], Hidden Size: 32, LR: 0.0001, Batch Size: 32, Loss: 0.5951\n",
      "Epoch [25/50], Hidden Size: 32, LR: 0.0001, Batch Size: 32, Loss: 0.5528\n",
      "Epoch [26/50], Hidden Size: 32, LR: 0.0001, Batch Size: 32, Loss: 0.5109\n",
      "Epoch [27/50], Hidden Size: 32, LR: 0.0001, Batch Size: 32, Loss: 0.4624\n",
      "Epoch [28/50], Hidden Size: 32, LR: 0.0001, Batch Size: 32, Loss: 0.4260\n",
      "Epoch [29/50], Hidden Size: 32, LR: 0.0001, Batch Size: 32, Loss: 0.3908\n",
      "Epoch [30/50], Hidden Size: 32, LR: 0.0001, Batch Size: 32, Loss: 0.3638\n",
      "Epoch [31/50], Hidden Size: 32, LR: 0.0001, Batch Size: 32, Loss: 0.3470\n",
      "Epoch [32/50], Hidden Size: 32, LR: 0.0001, Batch Size: 32, Loss: 0.3394\n",
      "Epoch [33/50], Hidden Size: 32, LR: 0.0001, Batch Size: 32, Loss: 0.3328\n",
      "Epoch [34/50], Hidden Size: 32, LR: 0.0001, Batch Size: 32, Loss: 0.3327\n",
      "Epoch [35/50], Hidden Size: 32, LR: 0.0001, Batch Size: 32, Loss: 0.3318\n",
      "Epoch [36/50], Hidden Size: 32, LR: 0.0001, Batch Size: 32, Loss: 0.3337\n",
      "Epoch [37/50], Hidden Size: 32, LR: 0.0001, Batch Size: 32, Loss: 0.3333\n",
      "Epoch [38/50], Hidden Size: 32, LR: 0.0001, Batch Size: 32, Loss: 0.3337\n",
      "Epoch [39/50], Hidden Size: 32, LR: 0.0001, Batch Size: 32, Loss: 0.3338\n",
      "Epoch [40/50], Hidden Size: 32, LR: 0.0001, Batch Size: 32, Loss: 0.3330\n",
      "Epoch [41/50], Hidden Size: 32, LR: 0.0001, Batch Size: 32, Loss: 0.3325\n",
      "Epoch [42/50], Hidden Size: 32, LR: 0.0001, Batch Size: 32, Loss: 0.3343\n",
      "Epoch [43/50], Hidden Size: 32, LR: 0.0001, Batch Size: 32, Loss: 0.3331\n",
      "Epoch [44/50], Hidden Size: 32, LR: 0.0001, Batch Size: 32, Loss: 0.3338\n",
      "Epoch [45/50], Hidden Size: 32, LR: 0.0001, Batch Size: 32, Loss: 0.3347\n",
      "Epoch [46/50], Hidden Size: 32, LR: 0.0001, Batch Size: 32, Loss: 0.3336\n",
      "Epoch [47/50], Hidden Size: 32, LR: 0.0001, Batch Size: 32, Loss: 0.3330\n",
      "Epoch [48/50], Hidden Size: 32, LR: 0.0001, Batch Size: 32, Loss: 0.3351\n",
      "Epoch [49/50], Hidden Size: 32, LR: 0.0001, Batch Size: 32, Loss: 0.3345\n",
      "Epoch [50/50], Hidden Size: 32, LR: 0.0001, Batch Size: 32, Loss: 0.3339\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\10374\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torchvision\\models\\_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "c:\\Users\\10374\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torchvision\\models\\_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ResNet50_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet50_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/50], Hidden Size: 32, LR: 0.0001, Batch Size: 64, Loss: 0.9996\n",
      "Epoch [2/50], Hidden Size: 32, LR: 0.0001, Batch Size: 64, Loss: 0.9989\n",
      "Epoch [3/50], Hidden Size: 32, LR: 0.0001, Batch Size: 64, Loss: 0.9979\n",
      "Epoch [4/50], Hidden Size: 32, LR: 0.0001, Batch Size: 64, Loss: 0.9968\n",
      "Epoch [5/50], Hidden Size: 32, LR: 0.0001, Batch Size: 64, Loss: 0.9956\n",
      "Epoch [6/50], Hidden Size: 32, LR: 0.0001, Batch Size: 64, Loss: 0.9939\n",
      "Epoch [7/50], Hidden Size: 32, LR: 0.0001, Batch Size: 64, Loss: 0.9921\n",
      "Epoch [8/50], Hidden Size: 32, LR: 0.0001, Batch Size: 64, Loss: 0.9897\n",
      "Epoch [9/50], Hidden Size: 32, LR: 0.0001, Batch Size: 64, Loss: 0.9870\n",
      "Epoch [10/50], Hidden Size: 32, LR: 0.0001, Batch Size: 64, Loss: 0.9838\n",
      "Epoch [11/50], Hidden Size: 32, LR: 0.0001, Batch Size: 64, Loss: 0.9795\n",
      "Epoch [12/50], Hidden Size: 32, LR: 0.0001, Batch Size: 64, Loss: 0.9749\n",
      "Epoch [13/50], Hidden Size: 32, LR: 0.0001, Batch Size: 64, Loss: 0.9684\n",
      "Epoch [14/50], Hidden Size: 32, LR: 0.0001, Batch Size: 64, Loss: 0.9612\n",
      "Epoch [15/50], Hidden Size: 32, LR: 0.0001, Batch Size: 64, Loss: 0.9528\n",
      "Epoch [16/50], Hidden Size: 32, LR: 0.0001, Batch Size: 64, Loss: 0.9431\n",
      "Epoch [17/50], Hidden Size: 32, LR: 0.0001, Batch Size: 64, Loss: 0.9326\n",
      "Epoch [18/50], Hidden Size: 32, LR: 0.0001, Batch Size: 64, Loss: 0.9199\n",
      "Epoch [19/50], Hidden Size: 32, LR: 0.0001, Batch Size: 64, Loss: 0.9082\n",
      "Epoch [20/50], Hidden Size: 32, LR: 0.0001, Batch Size: 64, Loss: 0.8942\n",
      "Epoch [21/50], Hidden Size: 32, LR: 0.0001, Batch Size: 64, Loss: 0.8797\n",
      "Epoch [22/50], Hidden Size: 32, LR: 0.0001, Batch Size: 64, Loss: 0.8652\n",
      "Epoch [23/50], Hidden Size: 32, LR: 0.0001, Batch Size: 64, Loss: 0.8508\n",
      "Epoch [24/50], Hidden Size: 32, LR: 0.0001, Batch Size: 64, Loss: 0.8347\n",
      "Epoch [25/50], Hidden Size: 32, LR: 0.0001, Batch Size: 64, Loss: 0.8147\n",
      "Epoch [26/50], Hidden Size: 32, LR: 0.0001, Batch Size: 64, Loss: 0.7945\n",
      "Epoch [27/50], Hidden Size: 32, LR: 0.0001, Batch Size: 64, Loss: 0.7747\n",
      "Epoch [28/50], Hidden Size: 32, LR: 0.0001, Batch Size: 64, Loss: 0.7513\n",
      "Epoch [29/50], Hidden Size: 32, LR: 0.0001, Batch Size: 64, Loss: 0.7261\n",
      "Epoch [30/50], Hidden Size: 32, LR: 0.0001, Batch Size: 64, Loss: 0.7021\n",
      "Epoch [31/50], Hidden Size: 32, LR: 0.0001, Batch Size: 64, Loss: 0.6817\n",
      "Epoch [32/50], Hidden Size: 32, LR: 0.0001, Batch Size: 64, Loss: 0.6543\n",
      "Epoch [33/50], Hidden Size: 32, LR: 0.0001, Batch Size: 64, Loss: 0.6277\n",
      "Epoch [34/50], Hidden Size: 32, LR: 0.0001, Batch Size: 64, Loss: 0.5986\n",
      "Epoch [35/50], Hidden Size: 32, LR: 0.0001, Batch Size: 64, Loss: 0.5687\n",
      "Epoch [36/50], Hidden Size: 32, LR: 0.0001, Batch Size: 64, Loss: 0.5405\n",
      "Epoch [37/50], Hidden Size: 32, LR: 0.0001, Batch Size: 64, Loss: 0.5010\n",
      "Epoch [38/50], Hidden Size: 32, LR: 0.0001, Batch Size: 64, Loss: 0.4891\n",
      "Epoch [39/50], Hidden Size: 32, LR: 0.0001, Batch Size: 64, Loss: 0.4578\n",
      "Epoch [40/50], Hidden Size: 32, LR: 0.0001, Batch Size: 64, Loss: 0.4277\n",
      "Epoch [41/50], Hidden Size: 32, LR: 0.0001, Batch Size: 64, Loss: 0.4022\n",
      "Epoch [42/50], Hidden Size: 32, LR: 0.0001, Batch Size: 64, Loss: 0.3878\n",
      "Epoch [43/50], Hidden Size: 32, LR: 0.0001, Batch Size: 64, Loss: 0.3615\n",
      "Epoch [44/50], Hidden Size: 32, LR: 0.0001, Batch Size: 64, Loss: 0.3503\n",
      "Epoch [45/50], Hidden Size: 32, LR: 0.0001, Batch Size: 64, Loss: 0.3377\n",
      "Epoch [46/50], Hidden Size: 32, LR: 0.0001, Batch Size: 64, Loss: 0.3341\n",
      "Epoch [47/50], Hidden Size: 32, LR: 0.0001, Batch Size: 64, Loss: 0.3290\n",
      "Epoch [48/50], Hidden Size: 32, LR: 0.0001, Batch Size: 64, Loss: 0.3334\n",
      "Epoch [49/50], Hidden Size: 32, LR: 0.0001, Batch Size: 64, Loss: 0.3350\n",
      "Epoch [50/50], Hidden Size: 32, LR: 0.0001, Batch Size: 64, Loss: 0.3321\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\10374\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torchvision\\models\\_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "c:\\Users\\10374\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torchvision\\models\\_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ResNet50_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet50_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/50], Hidden Size: 64, LR: 0.001, Batch Size: 16, Loss: 0.9580\n",
      "Epoch [2/50], Hidden Size: 64, LR: 0.001, Batch Size: 16, Loss: 0.5862\n",
      "Epoch [3/50], Hidden Size: 64, LR: 0.001, Batch Size: 16, Loss: 0.3398\n",
      "Epoch [4/50], Hidden Size: 64, LR: 0.001, Batch Size: 16, Loss: 0.3335\n",
      "Epoch [5/50], Hidden Size: 64, LR: 0.001, Batch Size: 16, Loss: 0.3340\n",
      "Epoch [6/50], Hidden Size: 64, LR: 0.001, Batch Size: 16, Loss: 0.3374\n",
      "Epoch [7/50], Hidden Size: 64, LR: 0.001, Batch Size: 16, Loss: 0.3356\n",
      "Epoch [8/50], Hidden Size: 64, LR: 0.001, Batch Size: 16, Loss: 0.3354\n",
      "Epoch [9/50], Hidden Size: 64, LR: 0.001, Batch Size: 16, Loss: 0.3376\n",
      "Epoch [10/50], Hidden Size: 64, LR: 0.001, Batch Size: 16, Loss: 0.3355\n",
      "Epoch [11/50], Hidden Size: 64, LR: 0.001, Batch Size: 16, Loss: 0.3430\n",
      "Epoch [12/50], Hidden Size: 64, LR: 0.001, Batch Size: 16, Loss: 0.3347\n",
      "Epoch [13/50], Hidden Size: 64, LR: 0.001, Batch Size: 16, Loss: 0.3348\n",
      "Epoch [14/50], Hidden Size: 64, LR: 0.001, Batch Size: 16, Loss: 0.3340\n",
      "Epoch [15/50], Hidden Size: 64, LR: 0.001, Batch Size: 16, Loss: 0.3330\n",
      "Epoch [16/50], Hidden Size: 64, LR: 0.001, Batch Size: 16, Loss: 0.3312\n",
      "Epoch [17/50], Hidden Size: 64, LR: 0.001, Batch Size: 16, Loss: 0.3363\n",
      "Epoch [18/50], Hidden Size: 64, LR: 0.001, Batch Size: 16, Loss: 0.3332\n",
      "Epoch [19/50], Hidden Size: 64, LR: 0.001, Batch Size: 16, Loss: 0.3335\n",
      "Epoch [20/50], Hidden Size: 64, LR: 0.001, Batch Size: 16, Loss: 0.3348\n",
      "Epoch [21/50], Hidden Size: 64, LR: 0.001, Batch Size: 16, Loss: 0.3350\n",
      "Epoch [22/50], Hidden Size: 64, LR: 0.001, Batch Size: 16, Loss: 0.3373\n",
      "Epoch [23/50], Hidden Size: 64, LR: 0.001, Batch Size: 16, Loss: 0.3308\n",
      "Epoch [24/50], Hidden Size: 64, LR: 0.001, Batch Size: 16, Loss: 0.3357\n",
      "Epoch [25/50], Hidden Size: 64, LR: 0.001, Batch Size: 16, Loss: 0.3371\n",
      "Epoch [26/50], Hidden Size: 64, LR: 0.001, Batch Size: 16, Loss: 0.3375\n",
      "Epoch [27/50], Hidden Size: 64, LR: 0.001, Batch Size: 16, Loss: 0.3349\n",
      "Epoch [28/50], Hidden Size: 64, LR: 0.001, Batch Size: 16, Loss: 0.3341\n",
      "Epoch [29/50], Hidden Size: 64, LR: 0.001, Batch Size: 16, Loss: 0.3340\n",
      "Epoch [30/50], Hidden Size: 64, LR: 0.001, Batch Size: 16, Loss: 0.3353\n",
      "Epoch [31/50], Hidden Size: 64, LR: 0.001, Batch Size: 16, Loss: 0.3376\n",
      "Epoch [32/50], Hidden Size: 64, LR: 0.001, Batch Size: 16, Loss: 0.3327\n",
      "Epoch [33/50], Hidden Size: 64, LR: 0.001, Batch Size: 16, Loss: 0.3339\n",
      "Epoch [34/50], Hidden Size: 64, LR: 0.001, Batch Size: 16, Loss: 0.3306\n",
      "Epoch [35/50], Hidden Size: 64, LR: 0.001, Batch Size: 16, Loss: 0.3487\n",
      "Epoch [36/50], Hidden Size: 64, LR: 0.001, Batch Size: 16, Loss: 0.3440\n",
      "Epoch [37/50], Hidden Size: 64, LR: 0.001, Batch Size: 16, Loss: 0.3335\n",
      "Epoch [38/50], Hidden Size: 64, LR: 0.001, Batch Size: 16, Loss: 0.3343\n",
      "Epoch [39/50], Hidden Size: 64, LR: 0.001, Batch Size: 16, Loss: 0.3364\n",
      "Epoch [40/50], Hidden Size: 64, LR: 0.001, Batch Size: 16, Loss: 0.3365\n",
      "Epoch [41/50], Hidden Size: 64, LR: 0.001, Batch Size: 16, Loss: 0.3336\n",
      "Epoch [42/50], Hidden Size: 64, LR: 0.001, Batch Size: 16, Loss: 0.3359\n",
      "Epoch [43/50], Hidden Size: 64, LR: 0.001, Batch Size: 16, Loss: 0.3358\n",
      "Epoch [44/50], Hidden Size: 64, LR: 0.001, Batch Size: 16, Loss: 0.3321\n",
      "Epoch [45/50], Hidden Size: 64, LR: 0.001, Batch Size: 16, Loss: 0.3323\n",
      "Epoch [46/50], Hidden Size: 64, LR: 0.001, Batch Size: 16, Loss: 0.3334\n",
      "Epoch [47/50], Hidden Size: 64, LR: 0.001, Batch Size: 16, Loss: 0.3352\n",
      "Epoch [48/50], Hidden Size: 64, LR: 0.001, Batch Size: 16, Loss: 0.3379\n",
      "Epoch [49/50], Hidden Size: 64, LR: 0.001, Batch Size: 16, Loss: 0.3397\n",
      "Epoch [50/50], Hidden Size: 64, LR: 0.001, Batch Size: 16, Loss: 0.3341\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\10374\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torchvision\\models\\_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "c:\\Users\\10374\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torchvision\\models\\_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ResNet50_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet50_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/50], Hidden Size: 64, LR: 0.001, Batch Size: 32, Loss: 0.9905\n",
      "Epoch [2/50], Hidden Size: 64, LR: 0.001, Batch Size: 32, Loss: 0.9090\n",
      "Epoch [3/50], Hidden Size: 64, LR: 0.001, Batch Size: 32, Loss: 0.6806\n",
      "Epoch [4/50], Hidden Size: 64, LR: 0.001, Batch Size: 32, Loss: 0.3858\n",
      "Epoch [5/50], Hidden Size: 64, LR: 0.001, Batch Size: 32, Loss: 0.3401\n",
      "Epoch [6/50], Hidden Size: 64, LR: 0.001, Batch Size: 32, Loss: 0.3366\n",
      "Epoch [7/50], Hidden Size: 64, LR: 0.001, Batch Size: 32, Loss: 0.3358\n",
      "Epoch [8/50], Hidden Size: 64, LR: 0.001, Batch Size: 32, Loss: 0.3366\n",
      "Epoch [9/50], Hidden Size: 64, LR: 0.001, Batch Size: 32, Loss: 0.3351\n",
      "Epoch [10/50], Hidden Size: 64, LR: 0.001, Batch Size: 32, Loss: 0.3345\n",
      "Epoch [11/50], Hidden Size: 64, LR: 0.001, Batch Size: 32, Loss: 0.3357\n",
      "Epoch [12/50], Hidden Size: 64, LR: 0.001, Batch Size: 32, Loss: 0.3341\n",
      "Epoch [13/50], Hidden Size: 64, LR: 0.001, Batch Size: 32, Loss: 0.3332\n",
      "Epoch [14/50], Hidden Size: 64, LR: 0.001, Batch Size: 32, Loss: 0.3359\n",
      "Epoch [15/50], Hidden Size: 64, LR: 0.001, Batch Size: 32, Loss: 0.3364\n",
      "Epoch [16/50], Hidden Size: 64, LR: 0.001, Batch Size: 32, Loss: 0.3373\n",
      "Epoch [17/50], Hidden Size: 64, LR: 0.001, Batch Size: 32, Loss: 0.3409\n",
      "Epoch [18/50], Hidden Size: 64, LR: 0.001, Batch Size: 32, Loss: 0.3394\n",
      "Epoch [19/50], Hidden Size: 64, LR: 0.001, Batch Size: 32, Loss: 0.3348\n",
      "Epoch [20/50], Hidden Size: 64, LR: 0.001, Batch Size: 32, Loss: 0.3334\n",
      "Epoch [21/50], Hidden Size: 64, LR: 0.001, Batch Size: 32, Loss: 0.3333\n",
      "Epoch [22/50], Hidden Size: 64, LR: 0.001, Batch Size: 32, Loss: 0.3334\n",
      "Epoch [23/50], Hidden Size: 64, LR: 0.001, Batch Size: 32, Loss: 0.3324\n",
      "Epoch [24/50], Hidden Size: 64, LR: 0.001, Batch Size: 32, Loss: 0.3348\n",
      "Epoch [25/50], Hidden Size: 64, LR: 0.001, Batch Size: 32, Loss: 0.3350\n",
      "Epoch [26/50], Hidden Size: 64, LR: 0.001, Batch Size: 32, Loss: 0.3344\n",
      "Epoch [27/50], Hidden Size: 64, LR: 0.001, Batch Size: 32, Loss: 0.3385\n",
      "Epoch [28/50], Hidden Size: 64, LR: 0.001, Batch Size: 32, Loss: 0.3374\n",
      "Epoch [29/50], Hidden Size: 64, LR: 0.001, Batch Size: 32, Loss: 0.3341\n",
      "Epoch [30/50], Hidden Size: 64, LR: 0.001, Batch Size: 32, Loss: 0.3322\n",
      "Epoch [31/50], Hidden Size: 64, LR: 0.001, Batch Size: 32, Loss: 0.3343\n",
      "Epoch [32/50], Hidden Size: 64, LR: 0.001, Batch Size: 32, Loss: 0.3343\n",
      "Epoch [33/50], Hidden Size: 64, LR: 0.001, Batch Size: 32, Loss: 0.3352\n",
      "Epoch [34/50], Hidden Size: 64, LR: 0.001, Batch Size: 32, Loss: 0.3337\n",
      "Epoch [35/50], Hidden Size: 64, LR: 0.001, Batch Size: 32, Loss: 0.3339\n",
      "Epoch [36/50], Hidden Size: 64, LR: 0.001, Batch Size: 32, Loss: 0.3364\n",
      "Epoch [37/50], Hidden Size: 64, LR: 0.001, Batch Size: 32, Loss: 0.3362\n",
      "Epoch [38/50], Hidden Size: 64, LR: 0.001, Batch Size: 32, Loss: 0.3364\n",
      "Epoch [39/50], Hidden Size: 64, LR: 0.001, Batch Size: 32, Loss: 0.3344\n",
      "Epoch [40/50], Hidden Size: 64, LR: 0.001, Batch Size: 32, Loss: 0.3350\n",
      "Epoch [41/50], Hidden Size: 64, LR: 0.001, Batch Size: 32, Loss: 0.3342\n",
      "Epoch [42/50], Hidden Size: 64, LR: 0.001, Batch Size: 32, Loss: 0.3343\n",
      "Epoch [43/50], Hidden Size: 64, LR: 0.001, Batch Size: 32, Loss: 0.3342\n",
      "Epoch [44/50], Hidden Size: 64, LR: 0.001, Batch Size: 32, Loss: 0.3328\n",
      "Epoch [45/50], Hidden Size: 64, LR: 0.001, Batch Size: 32, Loss: 0.3340\n",
      "Epoch [46/50], Hidden Size: 64, LR: 0.001, Batch Size: 32, Loss: 0.3344\n",
      "Epoch [47/50], Hidden Size: 64, LR: 0.001, Batch Size: 32, Loss: 0.3351\n",
      "Epoch [48/50], Hidden Size: 64, LR: 0.001, Batch Size: 32, Loss: 0.3374\n",
      "Epoch [49/50], Hidden Size: 64, LR: 0.001, Batch Size: 32, Loss: 0.3335\n",
      "Epoch [50/50], Hidden Size: 64, LR: 0.001, Batch Size: 32, Loss: 0.3377\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\10374\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torchvision\\models\\_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "c:\\Users\\10374\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torchvision\\models\\_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ResNet50_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet50_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/50], Hidden Size: 64, LR: 0.001, Batch Size: 64, Loss: 0.9970\n",
      "Epoch [2/50], Hidden Size: 64, LR: 0.001, Batch Size: 64, Loss: 0.9809\n",
      "Epoch [3/50], Hidden Size: 64, LR: 0.001, Batch Size: 64, Loss: 0.9338\n",
      "Epoch [4/50], Hidden Size: 64, LR: 0.001, Batch Size: 64, Loss: 0.8464\n",
      "Epoch [5/50], Hidden Size: 64, LR: 0.001, Batch Size: 64, Loss: 0.7101\n",
      "Epoch [6/50], Hidden Size: 64, LR: 0.001, Batch Size: 64, Loss: 0.5124\n",
      "Epoch [7/50], Hidden Size: 64, LR: 0.001, Batch Size: 64, Loss: 0.3601\n",
      "Epoch [8/50], Hidden Size: 64, LR: 0.001, Batch Size: 64, Loss: 0.3637\n",
      "Epoch [9/50], Hidden Size: 64, LR: 0.001, Batch Size: 64, Loss: 0.3390\n",
      "Epoch [10/50], Hidden Size: 64, LR: 0.001, Batch Size: 64, Loss: 0.3381\n",
      "Epoch [11/50], Hidden Size: 64, LR: 0.001, Batch Size: 64, Loss: 0.3371\n",
      "Epoch [12/50], Hidden Size: 64, LR: 0.001, Batch Size: 64, Loss: 0.3384\n",
      "Epoch [13/50], Hidden Size: 64, LR: 0.001, Batch Size: 64, Loss: 0.3378\n",
      "Epoch [14/50], Hidden Size: 64, LR: 0.001, Batch Size: 64, Loss: 0.3336\n",
      "Epoch [15/50], Hidden Size: 64, LR: 0.001, Batch Size: 64, Loss: 0.3406\n",
      "Epoch [16/50], Hidden Size: 64, LR: 0.001, Batch Size: 64, Loss: 0.3255\n",
      "Epoch [17/50], Hidden Size: 64, LR: 0.001, Batch Size: 64, Loss: 0.3358\n",
      "Epoch [18/50], Hidden Size: 64, LR: 0.001, Batch Size: 64, Loss: 0.3339\n",
      "Epoch [19/50], Hidden Size: 64, LR: 0.001, Batch Size: 64, Loss: 0.3373\n",
      "Epoch [20/50], Hidden Size: 64, LR: 0.001, Batch Size: 64, Loss: 0.3349\n",
      "Epoch [21/50], Hidden Size: 64, LR: 0.001, Batch Size: 64, Loss: 0.3274\n",
      "Epoch [22/50], Hidden Size: 64, LR: 0.001, Batch Size: 64, Loss: 0.3315\n",
      "Epoch [23/50], Hidden Size: 64, LR: 0.001, Batch Size: 64, Loss: 0.3296\n",
      "Epoch [24/50], Hidden Size: 64, LR: 0.001, Batch Size: 64, Loss: 0.3362\n",
      "Epoch [25/50], Hidden Size: 64, LR: 0.001, Batch Size: 64, Loss: 0.3358\n",
      "Epoch [26/50], Hidden Size: 64, LR: 0.001, Batch Size: 64, Loss: 0.3394\n",
      "Epoch [27/50], Hidden Size: 64, LR: 0.001, Batch Size: 64, Loss: 0.3360\n",
      "Epoch [28/50], Hidden Size: 64, LR: 0.001, Batch Size: 64, Loss: 0.3358\n",
      "Epoch [29/50], Hidden Size: 64, LR: 0.001, Batch Size: 64, Loss: 0.3383\n",
      "Epoch [30/50], Hidden Size: 64, LR: 0.001, Batch Size: 64, Loss: 0.3300\n",
      "Epoch [31/50], Hidden Size: 64, LR: 0.001, Batch Size: 64, Loss: 0.3413\n",
      "Epoch [32/50], Hidden Size: 64, LR: 0.001, Batch Size: 64, Loss: 0.3289\n",
      "Epoch [33/50], Hidden Size: 64, LR: 0.001, Batch Size: 64, Loss: 0.3349\n",
      "Epoch [34/50], Hidden Size: 64, LR: 0.001, Batch Size: 64, Loss: 0.3324\n",
      "Epoch [35/50], Hidden Size: 64, LR: 0.001, Batch Size: 64, Loss: 0.3336\n",
      "Epoch [36/50], Hidden Size: 64, LR: 0.001, Batch Size: 64, Loss: 0.3339\n",
      "Epoch [37/50], Hidden Size: 64, LR: 0.001, Batch Size: 64, Loss: 0.3354\n",
      "Epoch [38/50], Hidden Size: 64, LR: 0.001, Batch Size: 64, Loss: 0.3375\n",
      "Epoch [39/50], Hidden Size: 64, LR: 0.001, Batch Size: 64, Loss: 0.3371\n",
      "Epoch [40/50], Hidden Size: 64, LR: 0.001, Batch Size: 64, Loss: 0.3388\n",
      "Epoch [41/50], Hidden Size: 64, LR: 0.001, Batch Size: 64, Loss: 0.3385\n",
      "Epoch [42/50], Hidden Size: 64, LR: 0.001, Batch Size: 64, Loss: 0.3361\n",
      "Epoch [43/50], Hidden Size: 64, LR: 0.001, Batch Size: 64, Loss: 0.3312\n",
      "Epoch [44/50], Hidden Size: 64, LR: 0.001, Batch Size: 64, Loss: 0.3346\n",
      "Epoch [45/50], Hidden Size: 64, LR: 0.001, Batch Size: 64, Loss: 0.3341\n",
      "Epoch [46/50], Hidden Size: 64, LR: 0.001, Batch Size: 64, Loss: 0.3352\n",
      "Epoch [47/50], Hidden Size: 64, LR: 0.001, Batch Size: 64, Loss: 0.3345\n",
      "Epoch [48/50], Hidden Size: 64, LR: 0.001, Batch Size: 64, Loss: 0.3389\n",
      "Epoch [49/50], Hidden Size: 64, LR: 0.001, Batch Size: 64, Loss: 0.3386\n",
      "Epoch [50/50], Hidden Size: 64, LR: 0.001, Batch Size: 64, Loss: 0.3402\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\10374\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torchvision\\models\\_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "c:\\Users\\10374\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torchvision\\models\\_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ResNet50_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet50_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/50], Hidden Size: 64, LR: 0.0001, Batch Size: 16, Loss: 0.9986\n",
      "Epoch [2/50], Hidden Size: 64, LR: 0.0001, Batch Size: 16, Loss: 0.9944\n",
      "Epoch [3/50], Hidden Size: 64, LR: 0.0001, Batch Size: 16, Loss: 0.9861\n",
      "Epoch [4/50], Hidden Size: 64, LR: 0.0001, Batch Size: 16, Loss: 0.9702\n",
      "Epoch [5/50], Hidden Size: 64, LR: 0.0001, Batch Size: 16, Loss: 0.9390\n",
      "Epoch [6/50], Hidden Size: 64, LR: 0.0001, Batch Size: 16, Loss: 0.8898\n",
      "Epoch [7/50], Hidden Size: 64, LR: 0.0001, Batch Size: 16, Loss: 0.8257\n",
      "Epoch [8/50], Hidden Size: 64, LR: 0.0001, Batch Size: 16, Loss: 0.7440\n",
      "Epoch [9/50], Hidden Size: 64, LR: 0.0001, Batch Size: 16, Loss: 0.6412\n",
      "Epoch [10/50], Hidden Size: 64, LR: 0.0001, Batch Size: 16, Loss: 0.5299\n",
      "Epoch [11/50], Hidden Size: 64, LR: 0.0001, Batch Size: 16, Loss: 0.4187\n",
      "Epoch [12/50], Hidden Size: 64, LR: 0.0001, Batch Size: 16, Loss: 0.3482\n",
      "Epoch [13/50], Hidden Size: 64, LR: 0.0001, Batch Size: 16, Loss: 0.3329\n",
      "Epoch [14/50], Hidden Size: 64, LR: 0.0001, Batch Size: 16, Loss: 0.3338\n",
      "Epoch [15/50], Hidden Size: 64, LR: 0.0001, Batch Size: 16, Loss: 0.3291\n",
      "Epoch [16/50], Hidden Size: 64, LR: 0.0001, Batch Size: 16, Loss: 0.3333\n",
      "Epoch [17/50], Hidden Size: 64, LR: 0.0001, Batch Size: 16, Loss: 0.3330\n",
      "Epoch [18/50], Hidden Size: 64, LR: 0.0001, Batch Size: 16, Loss: 0.3322\n",
      "Epoch [19/50], Hidden Size: 64, LR: 0.0001, Batch Size: 16, Loss: 0.3334\n",
      "Epoch [20/50], Hidden Size: 64, LR: 0.0001, Batch Size: 16, Loss: 0.3347\n",
      "Epoch [21/50], Hidden Size: 64, LR: 0.0001, Batch Size: 16, Loss: 0.3322\n",
      "Epoch [22/50], Hidden Size: 64, LR: 0.0001, Batch Size: 16, Loss: 0.3309\n",
      "Epoch [23/50], Hidden Size: 64, LR: 0.0001, Batch Size: 16, Loss: 0.3326\n",
      "Epoch [24/50], Hidden Size: 64, LR: 0.0001, Batch Size: 16, Loss: 0.3318\n",
      "Epoch [25/50], Hidden Size: 64, LR: 0.0001, Batch Size: 16, Loss: 0.3310\n",
      "Epoch [26/50], Hidden Size: 64, LR: 0.0001, Batch Size: 16, Loss: 0.3304\n",
      "Epoch [27/50], Hidden Size: 64, LR: 0.0001, Batch Size: 16, Loss: 0.3315\n",
      "Epoch [28/50], Hidden Size: 64, LR: 0.0001, Batch Size: 16, Loss: 0.3345\n",
      "Epoch [29/50], Hidden Size: 64, LR: 0.0001, Batch Size: 16, Loss: 0.3319\n",
      "Epoch [30/50], Hidden Size: 64, LR: 0.0001, Batch Size: 16, Loss: 0.3302\n",
      "Epoch [31/50], Hidden Size: 64, LR: 0.0001, Batch Size: 16, Loss: 0.3348\n",
      "Epoch [32/50], Hidden Size: 64, LR: 0.0001, Batch Size: 16, Loss: 0.3344\n",
      "Epoch [33/50], Hidden Size: 64, LR: 0.0001, Batch Size: 16, Loss: 0.3325\n",
      "Epoch [34/50], Hidden Size: 64, LR: 0.0001, Batch Size: 16, Loss: 0.3312\n",
      "Epoch [35/50], Hidden Size: 64, LR: 0.0001, Batch Size: 16, Loss: 0.3335\n",
      "Epoch [36/50], Hidden Size: 64, LR: 0.0001, Batch Size: 16, Loss: 0.3311\n",
      "Epoch [37/50], Hidden Size: 64, LR: 0.0001, Batch Size: 16, Loss: 0.3341\n",
      "Epoch [38/50], Hidden Size: 64, LR: 0.0001, Batch Size: 16, Loss: 0.3325\n",
      "Epoch [39/50], Hidden Size: 64, LR: 0.0001, Batch Size: 16, Loss: 0.3348\n",
      "Epoch [40/50], Hidden Size: 64, LR: 0.0001, Batch Size: 16, Loss: 0.3300\n",
      "Epoch [41/50], Hidden Size: 64, LR: 0.0001, Batch Size: 16, Loss: 0.3310\n",
      "Epoch [42/50], Hidden Size: 64, LR: 0.0001, Batch Size: 16, Loss: 0.3323\n",
      "Epoch [43/50], Hidden Size: 64, LR: 0.0001, Batch Size: 16, Loss: 0.3306\n",
      "Epoch [44/50], Hidden Size: 64, LR: 0.0001, Batch Size: 16, Loss: 0.3325\n",
      "Epoch [45/50], Hidden Size: 64, LR: 0.0001, Batch Size: 16, Loss: 0.3333\n",
      "Epoch [46/50], Hidden Size: 64, LR: 0.0001, Batch Size: 16, Loss: 0.3325\n",
      "Epoch [47/50], Hidden Size: 64, LR: 0.0001, Batch Size: 16, Loss: 0.3326\n",
      "Epoch [48/50], Hidden Size: 64, LR: 0.0001, Batch Size: 16, Loss: 0.3330\n",
      "Epoch [49/50], Hidden Size: 64, LR: 0.0001, Batch Size: 16, Loss: 0.3331\n",
      "Epoch [50/50], Hidden Size: 64, LR: 0.0001, Batch Size: 16, Loss: 0.3324\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\10374\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torchvision\\models\\_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "c:\\Users\\10374\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torchvision\\models\\_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ResNet50_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet50_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/50], Hidden Size: 64, LR: 0.0001, Batch Size: 32, Loss: 0.9996\n",
      "Epoch [2/50], Hidden Size: 64, LR: 0.0001, Batch Size: 32, Loss: 0.9987\n",
      "Epoch [3/50], Hidden Size: 64, LR: 0.0001, Batch Size: 32, Loss: 0.9974\n",
      "Epoch [4/50], Hidden Size: 64, LR: 0.0001, Batch Size: 32, Loss: 0.9956\n",
      "Epoch [5/50], Hidden Size: 64, LR: 0.0001, Batch Size: 32, Loss: 0.9929\n",
      "Epoch [6/50], Hidden Size: 64, LR: 0.0001, Batch Size: 32, Loss: 0.9891\n",
      "Epoch [7/50], Hidden Size: 64, LR: 0.0001, Batch Size: 32, Loss: 0.9834\n",
      "Epoch [8/50], Hidden Size: 64, LR: 0.0001, Batch Size: 32, Loss: 0.9754\n",
      "Epoch [9/50], Hidden Size: 64, LR: 0.0001, Batch Size: 32, Loss: 0.9638\n",
      "Epoch [10/50], Hidden Size: 64, LR: 0.0001, Batch Size: 32, Loss: 0.9485\n",
      "Epoch [11/50], Hidden Size: 64, LR: 0.0001, Batch Size: 32, Loss: 0.9295\n",
      "Epoch [12/50], Hidden Size: 64, LR: 0.0001, Batch Size: 32, Loss: 0.9086\n",
      "Epoch [13/50], Hidden Size: 64, LR: 0.0001, Batch Size: 32, Loss: 0.8837\n",
      "Epoch [14/50], Hidden Size: 64, LR: 0.0001, Batch Size: 32, Loss: 0.8552\n",
      "Epoch [15/50], Hidden Size: 64, LR: 0.0001, Batch Size: 32, Loss: 0.8225\n",
      "Epoch [16/50], Hidden Size: 64, LR: 0.0001, Batch Size: 32, Loss: 0.7857\n",
      "Epoch [17/50], Hidden Size: 64, LR: 0.0001, Batch Size: 32, Loss: 0.7470\n",
      "Epoch [18/50], Hidden Size: 64, LR: 0.0001, Batch Size: 32, Loss: 0.7040\n",
      "Epoch [19/50], Hidden Size: 64, LR: 0.0001, Batch Size: 32, Loss: 0.6543\n",
      "Epoch [20/50], Hidden Size: 64, LR: 0.0001, Batch Size: 32, Loss: 0.6069\n",
      "Epoch [21/50], Hidden Size: 64, LR: 0.0001, Batch Size: 32, Loss: 0.5512\n",
      "Epoch [22/50], Hidden Size: 64, LR: 0.0001, Batch Size: 32, Loss: 0.5045\n",
      "Epoch [23/50], Hidden Size: 64, LR: 0.0001, Batch Size: 32, Loss: 0.4568\n",
      "Epoch [24/50], Hidden Size: 64, LR: 0.0001, Batch Size: 32, Loss: 0.4147\n",
      "Epoch [25/50], Hidden Size: 64, LR: 0.0001, Batch Size: 32, Loss: 0.3830\n",
      "Epoch [26/50], Hidden Size: 64, LR: 0.0001, Batch Size: 32, Loss: 0.3580\n",
      "Epoch [27/50], Hidden Size: 64, LR: 0.0001, Batch Size: 32, Loss: 0.3459\n",
      "Epoch [28/50], Hidden Size: 64, LR: 0.0001, Batch Size: 32, Loss: 0.3406\n",
      "Epoch [29/50], Hidden Size: 64, LR: 0.0001, Batch Size: 32, Loss: 0.3383\n",
      "Epoch [30/50], Hidden Size: 64, LR: 0.0001, Batch Size: 32, Loss: 0.3385\n",
      "Epoch [31/50], Hidden Size: 64, LR: 0.0001, Batch Size: 32, Loss: 0.3366\n",
      "Epoch [32/50], Hidden Size: 64, LR: 0.0001, Batch Size: 32, Loss: 0.3353\n",
      "Epoch [33/50], Hidden Size: 64, LR: 0.0001, Batch Size: 32, Loss: 0.3334\n",
      "Epoch [34/50], Hidden Size: 64, LR: 0.0001, Batch Size: 32, Loss: 0.3325\n",
      "Epoch [35/50], Hidden Size: 64, LR: 0.0001, Batch Size: 32, Loss: 0.3344\n",
      "Epoch [36/50], Hidden Size: 64, LR: 0.0001, Batch Size: 32, Loss: 0.3347\n",
      "Epoch [37/50], Hidden Size: 64, LR: 0.0001, Batch Size: 32, Loss: 0.3357\n",
      "Epoch [38/50], Hidden Size: 64, LR: 0.0001, Batch Size: 32, Loss: 0.3351\n",
      "Epoch [39/50], Hidden Size: 64, LR: 0.0001, Batch Size: 32, Loss: 0.3340\n",
      "Epoch [40/50], Hidden Size: 64, LR: 0.0001, Batch Size: 32, Loss: 0.3349\n",
      "Epoch [41/50], Hidden Size: 64, LR: 0.0001, Batch Size: 32, Loss: 0.3350\n",
      "Epoch [42/50], Hidden Size: 64, LR: 0.0001, Batch Size: 32, Loss: 0.3337\n",
      "Epoch [43/50], Hidden Size: 64, LR: 0.0001, Batch Size: 32, Loss: 0.3341\n",
      "Epoch [44/50], Hidden Size: 64, LR: 0.0001, Batch Size: 32, Loss: 0.3352\n",
      "Epoch [45/50], Hidden Size: 64, LR: 0.0001, Batch Size: 32, Loss: 0.3358\n",
      "Epoch [46/50], Hidden Size: 64, LR: 0.0001, Batch Size: 32, Loss: 0.3349\n",
      "Epoch [47/50], Hidden Size: 64, LR: 0.0001, Batch Size: 32, Loss: 0.3338\n",
      "Epoch [48/50], Hidden Size: 64, LR: 0.0001, Batch Size: 32, Loss: 0.3340\n",
      "Epoch [49/50], Hidden Size: 64, LR: 0.0001, Batch Size: 32, Loss: 0.3320\n",
      "Epoch [50/50], Hidden Size: 64, LR: 0.0001, Batch Size: 32, Loss: 0.3315\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\10374\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torchvision\\models\\_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "c:\\Users\\10374\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torchvision\\models\\_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ResNet50_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet50_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/50], Hidden Size: 64, LR: 0.0001, Batch Size: 64, Loss: 0.9998\n",
      "Epoch [2/50], Hidden Size: 64, LR: 0.0001, Batch Size: 64, Loss: 0.9992\n",
      "Epoch [3/50], Hidden Size: 64, LR: 0.0001, Batch Size: 64, Loss: 0.9984\n",
      "Epoch [4/50], Hidden Size: 64, LR: 0.0001, Batch Size: 64, Loss: 0.9974\n",
      "Epoch [5/50], Hidden Size: 64, LR: 0.0001, Batch Size: 64, Loss: 0.9960\n",
      "Epoch [6/50], Hidden Size: 64, LR: 0.0001, Batch Size: 64, Loss: 0.9944\n",
      "Epoch [7/50], Hidden Size: 64, LR: 0.0001, Batch Size: 64, Loss: 0.9924\n",
      "Epoch [8/50], Hidden Size: 64, LR: 0.0001, Batch Size: 64, Loss: 0.9898\n",
      "Epoch [9/50], Hidden Size: 64, LR: 0.0001, Batch Size: 64, Loss: 0.9867\n",
      "Epoch [10/50], Hidden Size: 64, LR: 0.0001, Batch Size: 64, Loss: 0.9830\n",
      "Epoch [11/50], Hidden Size: 64, LR: 0.0001, Batch Size: 64, Loss: 0.9784\n",
      "Epoch [12/50], Hidden Size: 64, LR: 0.0001, Batch Size: 64, Loss: 0.9738\n",
      "Epoch [13/50], Hidden Size: 64, LR: 0.0001, Batch Size: 64, Loss: 0.9669\n",
      "Epoch [14/50], Hidden Size: 64, LR: 0.0001, Batch Size: 64, Loss: 0.9597\n",
      "Epoch [15/50], Hidden Size: 64, LR: 0.0001, Batch Size: 64, Loss: 0.9508\n",
      "Epoch [16/50], Hidden Size: 64, LR: 0.0001, Batch Size: 64, Loss: 0.9415\n",
      "Epoch [17/50], Hidden Size: 64, LR: 0.0001, Batch Size: 64, Loss: 0.9319\n",
      "Epoch [18/50], Hidden Size: 64, LR: 0.0001, Batch Size: 64, Loss: 0.9191\n",
      "Epoch [19/50], Hidden Size: 64, LR: 0.0001, Batch Size: 64, Loss: 0.9081\n",
      "Epoch [20/50], Hidden Size: 64, LR: 0.0001, Batch Size: 64, Loss: 0.8931\n",
      "Epoch [21/50], Hidden Size: 64, LR: 0.0001, Batch Size: 64, Loss: 0.8790\n",
      "Epoch [22/50], Hidden Size: 64, LR: 0.0001, Batch Size: 64, Loss: 0.8628\n",
      "Epoch [23/50], Hidden Size: 64, LR: 0.0001, Batch Size: 64, Loss: 0.8454\n",
      "Epoch [24/50], Hidden Size: 64, LR: 0.0001, Batch Size: 64, Loss: 0.8283\n",
      "Epoch [25/50], Hidden Size: 64, LR: 0.0001, Batch Size: 64, Loss: 0.8065\n",
      "Epoch [26/50], Hidden Size: 64, LR: 0.0001, Batch Size: 64, Loss: 0.7875\n",
      "Epoch [27/50], Hidden Size: 64, LR: 0.0001, Batch Size: 64, Loss: 0.7669\n",
      "Epoch [28/50], Hidden Size: 64, LR: 0.0001, Batch Size: 64, Loss: 0.7400\n",
      "Epoch [29/50], Hidden Size: 64, LR: 0.0001, Batch Size: 64, Loss: 0.7212\n",
      "Epoch [30/50], Hidden Size: 64, LR: 0.0001, Batch Size: 64, Loss: 0.6926\n",
      "Epoch [31/50], Hidden Size: 64, LR: 0.0001, Batch Size: 64, Loss: 0.6650\n",
      "Epoch [32/50], Hidden Size: 64, LR: 0.0001, Batch Size: 64, Loss: 0.6336\n",
      "Epoch [33/50], Hidden Size: 64, LR: 0.0001, Batch Size: 64, Loss: 0.6066\n",
      "Epoch [34/50], Hidden Size: 64, LR: 0.0001, Batch Size: 64, Loss: 0.5740\n",
      "Epoch [35/50], Hidden Size: 64, LR: 0.0001, Batch Size: 64, Loss: 0.5481\n",
      "Epoch [36/50], Hidden Size: 64, LR: 0.0001, Batch Size: 64, Loss: 0.5124\n",
      "Epoch [37/50], Hidden Size: 64, LR: 0.0001, Batch Size: 64, Loss: 0.4815\n",
      "Epoch [38/50], Hidden Size: 64, LR: 0.0001, Batch Size: 64, Loss: 0.4562\n",
      "Epoch [39/50], Hidden Size: 64, LR: 0.0001, Batch Size: 64, Loss: 0.4253\n",
      "Epoch [40/50], Hidden Size: 64, LR: 0.0001, Batch Size: 64, Loss: 0.4023\n",
      "Epoch [41/50], Hidden Size: 64, LR: 0.0001, Batch Size: 64, Loss: 0.3804\n",
      "Epoch [42/50], Hidden Size: 64, LR: 0.0001, Batch Size: 64, Loss: 0.3601\n",
      "Epoch [43/50], Hidden Size: 64, LR: 0.0001, Batch Size: 64, Loss: 0.3515\n",
      "Epoch [44/50], Hidden Size: 64, LR: 0.0001, Batch Size: 64, Loss: 0.3451\n",
      "Epoch [45/50], Hidden Size: 64, LR: 0.0001, Batch Size: 64, Loss: 0.3377\n",
      "Epoch [46/50], Hidden Size: 64, LR: 0.0001, Batch Size: 64, Loss: 0.3294\n",
      "Epoch [47/50], Hidden Size: 64, LR: 0.0001, Batch Size: 64, Loss: 0.3403\n",
      "Epoch [48/50], Hidden Size: 64, LR: 0.0001, Batch Size: 64, Loss: 0.3286\n",
      "Epoch [49/50], Hidden Size: 64, LR: 0.0001, Batch Size: 64, Loss: 0.3307\n",
      "Epoch [50/50], Hidden Size: 64, LR: 0.0001, Batch Size: 64, Loss: 0.3349\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\10374\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torchvision\\models\\_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "c:\\Users\\10374\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torchvision\\models\\_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ResNet50_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet50_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/50], Hidden Size: 128, LR: 0.001, Batch Size: 16, Loss: 0.9494\n",
      "Epoch [2/50], Hidden Size: 128, LR: 0.001, Batch Size: 16, Loss: 0.5364\n",
      "Epoch [3/50], Hidden Size: 128, LR: 0.001, Batch Size: 16, Loss: 0.3468\n",
      "Epoch [4/50], Hidden Size: 128, LR: 0.001, Batch Size: 16, Loss: 0.3352\n",
      "Epoch [5/50], Hidden Size: 128, LR: 0.001, Batch Size: 16, Loss: 0.3327\n",
      "Epoch [6/50], Hidden Size: 128, LR: 0.001, Batch Size: 16, Loss: 0.3303\n",
      "Epoch [7/50], Hidden Size: 128, LR: 0.001, Batch Size: 16, Loss: 0.3367\n",
      "Epoch [8/50], Hidden Size: 128, LR: 0.001, Batch Size: 16, Loss: 0.3338\n",
      "Epoch [9/50], Hidden Size: 128, LR: 0.001, Batch Size: 16, Loss: 0.3343\n",
      "Epoch [10/50], Hidden Size: 128, LR: 0.001, Batch Size: 16, Loss: 0.3335\n",
      "Epoch [11/50], Hidden Size: 128, LR: 0.001, Batch Size: 16, Loss: 0.3346\n",
      "Epoch [12/50], Hidden Size: 128, LR: 0.001, Batch Size: 16, Loss: 0.3299\n",
      "Epoch [13/50], Hidden Size: 128, LR: 0.001, Batch Size: 16, Loss: 0.3330\n",
      "Epoch [14/50], Hidden Size: 128, LR: 0.001, Batch Size: 16, Loss: 0.3329\n",
      "Epoch [15/50], Hidden Size: 128, LR: 0.001, Batch Size: 16, Loss: 0.3350\n",
      "Epoch [16/50], Hidden Size: 128, LR: 0.001, Batch Size: 16, Loss: 0.3342\n",
      "Epoch [17/50], Hidden Size: 128, LR: 0.001, Batch Size: 16, Loss: 0.3314\n",
      "Epoch [18/50], Hidden Size: 128, LR: 0.001, Batch Size: 16, Loss: 0.3315\n",
      "Epoch [19/50], Hidden Size: 128, LR: 0.001, Batch Size: 16, Loss: 0.3366\n",
      "Epoch [20/50], Hidden Size: 128, LR: 0.001, Batch Size: 16, Loss: 0.3320\n",
      "Epoch [21/50], Hidden Size: 128, LR: 0.001, Batch Size: 16, Loss: 0.3370\n",
      "Epoch [22/50], Hidden Size: 128, LR: 0.001, Batch Size: 16, Loss: 0.3356\n",
      "Epoch [23/50], Hidden Size: 128, LR: 0.001, Batch Size: 16, Loss: 0.3377\n",
      "Epoch [24/50], Hidden Size: 128, LR: 0.001, Batch Size: 16, Loss: 0.3345\n",
      "Epoch [25/50], Hidden Size: 128, LR: 0.001, Batch Size: 16, Loss: 0.3308\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[29], line 44\u001b[0m\n\u001b[0;32m     41\u001b[0m label \u001b[38;5;241m=\u001b[39m batch[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlabel\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[0;32m     43\u001b[0m \u001b[38;5;66;03m# Forward pass\u001b[39;00m\n\u001b[1;32m---> 44\u001b[0m outputs \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43msequential_data\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mimg_b\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mimg_l\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnumeric_data\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mviome_sequential\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     45\u001b[0m loss \u001b[38;5;241m=\u001b[39m criterion(outputs, label)\n\u001b[0;32m     47\u001b[0m \u001b[38;5;66;03m# Backward pass and optimization\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\10374\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1736\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1734\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1735\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1736\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\10374\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1747\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1742\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1743\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1744\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1745\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1746\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1747\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1749\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1750\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "Cell \u001b[1;32mIn[26], line 48\u001b[0m, in \u001b[0;36mCombinedModel.forward\u001b[1;34m(self, sequential_data, img_b, img_l, numeric_data, viome_sequential)\u001b[0m\n\u001b[0;32m     46\u001b[0m \u001b[38;5;66;03m# Process image data\u001b[39;00m\n\u001b[0;32m     47\u001b[0m img_b_out \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mimg_net(img_b)\n\u001b[1;32m---> 48\u001b[0m img_l_out \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mimg_net\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimg_l\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     50\u001b[0m \u001b[38;5;66;03m# Process numeric data\u001b[39;00m\n\u001b[0;32m     51\u001b[0m num_out \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnum_net(numeric_data)\n",
      "File \u001b[1;32mc:\\Users\\10374\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1736\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1734\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1735\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1736\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\10374\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1747\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1742\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1743\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1744\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1745\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1746\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1747\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1749\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1750\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[1;32mc:\\Users\\10374\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torchvision\\models\\resnet.py:285\u001b[0m, in \u001b[0;36mResNet.forward\u001b[1;34m(self, x)\u001b[0m\n\u001b[0;32m    284\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, x: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[1;32m--> 285\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_forward_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\10374\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torchvision\\models\\resnet.py:273\u001b[0m, in \u001b[0;36mResNet._forward_impl\u001b[1;34m(self, x)\u001b[0m\n\u001b[0;32m    270\u001b[0m x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrelu(x)\n\u001b[0;32m    271\u001b[0m x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmaxpool(x)\n\u001b[1;32m--> 273\u001b[0m x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlayer1\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    274\u001b[0m x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlayer2(x)\n\u001b[0;32m    275\u001b[0m x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlayer3(x)\n",
      "File \u001b[1;32mc:\\Users\\10374\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1736\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1734\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1735\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1736\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\10374\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1747\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1742\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1743\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1744\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1745\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1746\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1747\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1749\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1750\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[1;32mc:\\Users\\10374\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\nn\\modules\\container.py:250\u001b[0m, in \u001b[0;36mSequential.forward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m    248\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m):\n\u001b[0;32m    249\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m:\n\u001b[1;32m--> 250\u001b[0m         \u001b[38;5;28minput\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[43mmodule\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m    251\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28minput\u001b[39m\n",
      "File \u001b[1;32mc:\\Users\\10374\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1736\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1734\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1735\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1736\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\10374\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1747\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1742\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1743\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1744\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1745\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1746\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1747\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1749\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1750\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[1;32mc:\\Users\\10374\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torchvision\\models\\resnet.py:158\u001b[0m, in \u001b[0;36mBottleneck.forward\u001b[1;34m(self, x)\u001b[0m\n\u001b[0;32m    155\u001b[0m out \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbn3(out)\n\u001b[0;32m    157\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdownsample \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m--> 158\u001b[0m     identity \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdownsample\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    160\u001b[0m out \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m identity\n\u001b[0;32m    161\u001b[0m out \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrelu(out)\n",
      "File \u001b[1;32mc:\\Users\\10374\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1736\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1734\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1735\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1736\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\10374\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1747\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1742\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1743\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1744\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1745\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1746\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1747\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1749\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1750\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[1;32mc:\\Users\\10374\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\nn\\modules\\container.py:250\u001b[0m, in \u001b[0;36mSequential.forward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m    248\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m):\n\u001b[0;32m    249\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m:\n\u001b[1;32m--> 250\u001b[0m         \u001b[38;5;28minput\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[43mmodule\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m    251\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28minput\u001b[39m\n",
      "File \u001b[1;32mc:\\Users\\10374\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1736\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1734\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1735\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1736\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\10374\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1747\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1742\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1743\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1744\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1745\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1746\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1747\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1749\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1750\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[1;32mc:\\Users\\10374\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\nn\\modules\\batchnorm.py:193\u001b[0m, in \u001b[0;36m_BatchNorm.forward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m    186\u001b[0m     bn_training \u001b[38;5;241m=\u001b[39m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrunning_mean \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m) \u001b[38;5;129;01mand\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrunning_var \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[0;32m    188\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    189\u001b[0m \u001b[38;5;124;03mBuffers are only updated if they are to be tracked and we are in training mode. Thus they only need to be\u001b[39;00m\n\u001b[0;32m    190\u001b[0m \u001b[38;5;124;03mpassed when the update should occur (i.e. in training mode when they are tracked), or when buffer stats are\u001b[39;00m\n\u001b[0;32m    191\u001b[0m \u001b[38;5;124;03mused for normalization (i.e. in eval mode when buffers are not None).\u001b[39;00m\n\u001b[0;32m    192\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m--> 193\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbatch_norm\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    194\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m    195\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;66;43;03m# If buffers are not to be tracked, ensure that they won't be updated\u001b[39;49;00m\n\u001b[0;32m    196\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrunning_mean\u001b[49m\n\u001b[0;32m    197\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mnot\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtraining\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrack_running_stats\u001b[49m\n\u001b[0;32m    198\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m    199\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrunning_var\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mnot\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtraining\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrack_running_stats\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m    200\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    201\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbias\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    202\u001b[0m \u001b[43m    \u001b[49m\u001b[43mbn_training\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    203\u001b[0m \u001b[43m    \u001b[49m\u001b[43mexponential_average_factor\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    204\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43meps\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    205\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\10374\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\nn\\functional.py:2812\u001b[0m, in \u001b[0;36mbatch_norm\u001b[1;34m(input, running_mean, running_var, weight, bias, training, momentum, eps)\u001b[0m\n\u001b[0;32m   2809\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m training:\n\u001b[0;32m   2810\u001b[0m     _verify_batch_size(\u001b[38;5;28minput\u001b[39m\u001b[38;5;241m.\u001b[39msize())\n\u001b[1;32m-> 2812\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbatch_norm\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   2813\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2814\u001b[0m \u001b[43m    \u001b[49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2815\u001b[0m \u001b[43m    \u001b[49m\u001b[43mbias\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2816\u001b[0m \u001b[43m    \u001b[49m\u001b[43mrunning_mean\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2817\u001b[0m \u001b[43m    \u001b[49m\u001b[43mrunning_var\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2818\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtraining\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2819\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmomentum\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2820\u001b[0m \u001b[43m    \u001b[49m\u001b[43meps\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2821\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackends\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcudnn\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43menabled\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2822\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Hyperparameters\n",
    "hidden_sizes = [32, 64, 128]\n",
    "learning_rates = [1e-3, 1e-4]\n",
    "batch_sizes = [16, 32, 64]\n",
    "\n",
    "best_loss = float('inf')\n",
    "best_model = None\n",
    "\n",
    "# Iterate over different combinations of hyperparameters to find the best model\n",
    "for hidden_size in hidden_sizes:\n",
    "    for lr in learning_rates:\n",
    "        for batch_size in batch_sizes:\n",
    "            # Create the dataset loader for training and testing\n",
    "            train_loader = DataLoader(dataset_train, batch_size=batch_size, shuffle=True)\n",
    "            test_loader = DataLoader(dataset_test, batch_size=batch_size, shuffle=False)\n",
    "            \n",
    "            model = CombinedModel(hidden_size=hidden_size)\n",
    "            criterion = RMSRELoss()  \n",
    "            optimizer = optim.Adam(model.parameters(), lr=lr)\n",
    "            \n",
    "            # Learning rate scheduler\n",
    "            scheduler = optim.lr_scheduler.StepLR(optimizer, step_size=50, gamma=0.1)\n",
    "            \n",
    "            # Set up the device\n",
    "            device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "            model = model.to(device)\n",
    "            criterion = criterion.to(device)\n",
    "            \n",
    "            # Training process\n",
    "            num_epochs = 50\n",
    "            for epoch in range(num_epochs):\n",
    "                model.train()\n",
    "                running_loss = 0.0\n",
    "                for batch in train_loader:\n",
    "                    # Move data to device\n",
    "                    sequential_data = batch['sequential_data'].to(device)\n",
    "                    img_b = batch['img_b'].to(device)\n",
    "                    img_l = batch['img_l'].to(device)\n",
    "                    numeric_data = batch['numeric_data'].to(device)\n",
    "                    viome_sequential = batch['viome_sequential'].to(device)\n",
    "                    label = batch['label'].to(device)\n",
    "                    \n",
    "                    # Forward pass\n",
    "                    outputs = model(sequential_data, img_b, img_l, numeric_data, viome_sequential)\n",
    "                    loss = criterion(outputs, label)\n",
    "                    \n",
    "                    # Backward pass and optimization\n",
    "                    optimizer.zero_grad()\n",
    "                    loss.backward()\n",
    "                    optimizer.step()\n",
    "                    \n",
    "                    running_loss += loss.item()\n",
    "                scheduler.step()\n",
    "                \n",
    "                avg_loss = running_loss / len(train_loader)\n",
    "                print(f\"Epoch [{epoch+1}/{num_epochs}], Hidden Size: {hidden_size}, LR: {lr}, Batch Size: {batch_size}, Loss: {avg_loss:.4f}\")\n",
    "            \n",
    "            # Save the best model\n",
    "            if avg_loss < best_loss:\n",
    "                best_loss = avg_loss\n",
    "                best_model = model\n",
    "\n",
    "# Use the best trained model for predictions\n",
    "best_model.eval()  # Set the model to evaluation mode\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predictions saved to 'submission.csv' in Kaggle format.\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "# Use the best trained model for predictions\n",
    "best_model.eval()  # Set the model to evaluation mode\n",
    "test_loss = 0.0\n",
    "all_outputs = []\n",
    "all_labels = []\n",
    "\n",
    "with torch.no_grad():  # Disable gradient calculation\n",
    "    for batch in test_loader:\n",
    "        sequential_data = batch['sequential_data'].to(device)\n",
    "        img_b = batch['img_b'].to(device)\n",
    "        img_l = batch['img_l'].to(device)\n",
    "        numeric_data = batch['numeric_data'].to(device)\n",
    "        viome_sequential = batch['viome_sequential'].to(device)\n",
    "        \n",
    "        # Forward pass\n",
    "        outputs = model(sequential_data, img_b, img_l, numeric_data, viome_sequential)\n",
    "        \n",
    "        all_outputs += ([i[0] for i in outputs.tolist()])\n",
    "        \n",
    "test_ids = np.arange(len(all_outputs)) \n",
    "submission = pd.DataFrame({\"row_id\": test_ids, \"label\": all_outputs})\n",
    "submission.to_csv(\"submission.csv\", index=False)\n",
    "print(\"Predictions saved to 'submission.csv' in Kaggle format.\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
