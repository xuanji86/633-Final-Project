{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import numpy as np\n",
    "import ast\n",
    "from PIL import Image\n",
    "from datetime import datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cgm_test = pd.read_csv('633FinalData/cgm_test.csv')\n",
    "cgm_train = pd.read_csv('633FinalData/cgm_train.csv')\n",
    "img_train = pd.read_csv('633FinalData/img_train.csv')\n",
    "img_test = pd.read_csv('633FinalData/img_test.csv')\n",
    "cgm_test['cgm'] = cgm_test['CGM Data'].apply(lambda x: ast.literal_eval(x))\n",
    "cgm_train['cgm'] = cgm_train['CGM Data'].apply(lambda x: ast.literal_eval(x))\n",
    "def get_img(img):\n",
    "    # Convert the string representation of the image to a list\n",
    "    img = ast.literal_eval(img)\n",
    "    \n",
    "    # Convert the list to a NumPy array\n",
    "    img = np.array(img)\n",
    "    \n",
    "    # Convert the NumPy array to a PIL Image\n",
    "    img = Image.fromarray(np.uint8(img))\n",
    "    \n",
    "    # Resize the image\n",
    "    img = img.resize((64,64))\n",
    "    \n",
    "    # Convert the resized image back to a NumPy array\n",
    "    img = np.array(img)\n",
    "    \n",
    "    if len(img.shape) == 2:\n",
    "        img = np.stack((img,)*3, axis=-1)\n",
    "    return img\n",
    "\n",
    "img_train['img_b'] = img_train['Image Before Breakfast'].apply(get_img)\n",
    "img_train['img_l'] = img_train['Image Before Lunch'].apply(get_img)\n",
    "img_test['img_b'] = img_test['Image Before Breakfast'].apply(get_img)\n",
    "img_test['img_l'] = img_test['Image Before Lunch'].apply(get_img)\n",
    "\n",
    "def to_step(t):\n",
    "    date_obj = datetime.strptime(t, '%Y-%m-%d %H:%M:%S')\n",
    "    return (date_obj.hour*60 + date_obj.minute)//5\n",
    "\n",
    "def cgm_to_steps(cgm):\n",
    "    steps = [0 for _ in range(288)]\n",
    "    for t,value in cgm:\n",
    "        steps[to_step(t)] = value\n",
    "    return steps\n",
    "\n",
    "def time_to_step(t1,t2):\n",
    "    if t1 == '{}' or t2 == '{}':\n",
    "        return [0 for _ in range(288)]\n",
    "    steps = [0 for _ in range(288)]\n",
    "    steps[to_step(t1)] = 1\n",
    "    steps[to_step(t2)] = 1\n",
    "    return steps\n",
    "\n",
    "cgm_train = cgm_train.dropna()\n",
    "\n",
    "cgm_train = cgm_train[~cgm_train.apply(lambda row: row.astype(str).str.contains('{}').any(), axis=1)]\n",
    "\n",
    "cgm_test['cgm_sequential'] = cgm_test['cgm'].apply(cgm_to_steps)\n",
    "cgm_train['cgm_sequential'] = cgm_train['cgm'].apply(cgm_to_steps)\n",
    "cgm_train['when_to_eat'] = cgm_train[['Breakfast Time', 'Lunch Time']].apply(lambda x: time_to_step(x['Breakfast Time'], x['Lunch Time']), axis=1)\n",
    "cgm_test['when_to_eat'] = cgm_test[['Breakfast Time', 'Lunch Time']].apply(lambda x: time_to_step(x['Breakfast Time'], x['Lunch Time']), axis=1)\n",
    "viome_test = pd.read_csv('633FinalData/demo_viome_test.csv')\n",
    "viome_train = pd.read_csv('633FinalData/demo_viome_train.csv')\n",
    "viome_test= pd.get_dummies(viome_test, columns=['Race'])\n",
    "viome_train= pd.get_dummies(viome_train, columns=['Race'])\n",
    "viome_test['viome_sequential'] = viome_test['Viome'].apply(lambda x :[float(x) for x in x.split(',')])\n",
    "viome_train['viome_sequential'] = viome_train['Viome'].apply(lambda x :[float(x) for x in x.split(',')])\n",
    "combined_train = pd.merge(cgm_train, img_train, on=['Subject ID', 'Day'])\n",
    "combined_train = pd.merge(combined_train, viome_train, on=['Subject ID'])\n",
    "combined_test = pd.merge(cgm_test, img_test, on=['Subject ID', 'Day'])\n",
    "combined_test = pd.merge(combined_test, viome_test, on=['Subject ID'])\n",
    "to_drop_train = ['Subject ID','Day','Breakfast Time','Lunch Time','CGM Data','Image Before Breakfast','Image Before Lunch','Viome']\n",
    "combined_train = combined_train.drop(to_drop_train, axis=1)\n",
    "to_drop_test = ['Subject ID','Day','Breakfast Time','Lunch Time','CGM Data','Image Before Breakfast','Image Before Lunch','Viome']\n",
    "to_drop_test = ['Subject ID','Day','Breakfast Time','Lunch Time','CGM Data','Image Before Breakfast','Image Before Lunch','Viome']\n",
    "combined_test = combined_test.drop(to_drop_train, axis=1)\n",
    "label_train = pd.read_csv('633FinalData/label_train.csv')\n",
    "label_train = label_train['Lunch Calories']\n",
    "combined_train['label'] = label_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomDataset(Dataset):\n",
    "    def __init__(self, dataframe):\n",
    "        self.dataframe = dataframe\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.dataframe)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        row = self.dataframe.iloc[idx]\n",
    "\n",
    "        # Small epsilon value to avoid division by zero\n",
    "        epsilon = 1e-6\n",
    "\n",
    "        # Extract sequential data\n",
    "        cgm_sequential = np.array(row['cgm_sequential'], dtype=float)\n",
    "        when_to_eat = np.array(row['when_to_eat'], dtype=float)\n",
    "\n",
    "        # Normalize sequential data\n",
    "        cgm_sequential = (cgm_sequential - np.mean(cgm_sequential)) / (np.std(cgm_sequential) + epsilon)\n",
    "        when_to_eat = (when_to_eat - np.mean(when_to_eat)) / (np.std(when_to_eat) + epsilon)\n",
    "\n",
    "        # Stack cgm_sequential and when_to_eat to create a sequential data array with 2 features\n",
    "        sequential_data = np.stack((cgm_sequential, when_to_eat), axis=-1)\n",
    "\n",
    "        # Extract image data\n",
    "        img_b = np.array(row['img_b'], dtype=float)\n",
    "        img_l = np.array(row['img_l'], dtype=float)\n",
    "\n",
    "        # Normalize image data\n",
    "        img_b = (img_b - np.mean(img_b)) / (np.std(img_b) + epsilon)\n",
    "        img_l = (img_l - np.mean(img_l)) / (np.std(img_l) + epsilon)\n",
    "\n",
    "        # Ensure img_b has 3 channels\n",
    "        if img_b.ndim == 2:  # Grayscale image\n",
    "            img_b = np.stack((img_b, img_b, img_b), axis=-1)\n",
    "\n",
    "        # Ensure img_l has 3 channels\n",
    "        if img_l.ndim == 2:  # Grayscale image\n",
    "            img_l = np.stack((img_l, img_l, img_l), axis=-1)\n",
    "\n",
    "        # Convert image data to PIL Image and then to tensor\n",
    "        img_b = Image.fromarray(np.uint8(img_b))\n",
    "        img_l = Image.fromarray(np.uint8(img_l))\n",
    "        img_b = torch.tensor(np.array(img_b), dtype=torch.float32).permute(2, 0, 1)  # Convert to CxHxW\n",
    "        img_l = torch.tensor(np.array(img_l), dtype=torch.float32).permute(2, 0, 1)  # Convert to CxHxW\n",
    "\n",
    "        # Extract numeric data\n",
    "        numeric_data = row[['Age', 'Gender', 'Weight', 'Height', 'Diabetes Status', 'A1C',\n",
    "                            'Baseline Fasting Glucose', 'Insulin', 'Triglycerides', 'Cholesterol',\n",
    "                            'HDL', 'Non-HDL', 'LDL', 'VLDL', 'CHO/HDL Ratio', 'HOMA-IR', 'BMI',\n",
    "                            'Race_African American', 'Race_Hispanic/Latino', 'Race_White']].values.astype(np.float32)\n",
    "\n",
    "        # Normalize numeric data\n",
    "        numeric_data = (numeric_data - np.mean(numeric_data)) / (np.std(numeric_data) + epsilon)\n",
    "        numeric_data = torch.tensor(numeric_data, dtype=torch.float32)\n",
    "\n",
    "        # Extract and reshape viome_sequential\n",
    "        viome_sequential = np.array(row['viome_sequential'], dtype=float).reshape(27, 1)\n",
    "\n",
    "        # Normalize viome_sequential\n",
    "        viome_sequential = (viome_sequential - np.mean(viome_sequential)) / (np.std(viome_sequential) + epsilon)\n",
    "\n",
    "        label_train = row['label']\n",
    "\n",
    "        return {\n",
    "            'sequential_data': torch.tensor(sequential_data, dtype=torch.float32),\n",
    "            'viome_sequential': torch.tensor(viome_sequential, dtype=torch.float32),\n",
    "            'img_b': img_b,\n",
    "            'img_l': img_l,\n",
    "            'numeric_data': numeric_data,\n",
    "            'label': label_train\n",
    "        }\n",
    "        \n",
    "class CustomTestDataset(Dataset):\n",
    "    def __init__(self, dataframe):\n",
    "        self.dataframe = dataframe\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.dataframe)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        row = self.dataframe.iloc[idx]\n",
    "\n",
    "        # Small epsilon value to avoid division by zero\n",
    "        epsilon = 1e-6\n",
    "\n",
    "        # Extract sequential data\n",
    "        cgm_sequential = np.array(row['cgm_sequential'], dtype=float)\n",
    "        when_to_eat = np.array(row['when_to_eat'], dtype=float)\n",
    "\n",
    "        # Normalize sequential data\n",
    "        cgm_sequential = (cgm_sequential - np.mean(cgm_sequential)) / (np.std(cgm_sequential) + epsilon)\n",
    "        when_to_eat = (when_to_eat - np.mean(when_to_eat)) / (np.std(when_to_eat) + epsilon)\n",
    "\n",
    "        # Stack cgm_sequential and when_to_eat to create a sequential data array with 2 features\n",
    "        sequential_data = np.stack((cgm_sequential, when_to_eat), axis=-1)\n",
    "\n",
    "        # Extract image data\n",
    "        img_b = np.array(row['img_b'], dtype=float)\n",
    "        img_l = np.array(row['img_l'], dtype=float)\n",
    "\n",
    "        # Normalize image data\n",
    "        img_b = (img_b - np.mean(img_b)) / (np.std(img_b) + epsilon)\n",
    "        img_l = (img_l - np.mean(img_l)) / (np.std(img_l) + epsilon)\n",
    "\n",
    "        # Ensure img_b has 3 channels\n",
    "        if img_b.ndim == 2:  # Grayscale image\n",
    "            img_b = np.stack((img_b, img_b, img_b), axis=-1)\n",
    "\n",
    "        # Ensure img_l has 3 channels\n",
    "        if img_l.ndim == 2:  # Grayscale image\n",
    "            img_l = np.stack((img_l, img_l, img_l), axis=-1)\n",
    "\n",
    "        # Convert image data to PIL Image and then to tensor\n",
    "        img_b = Image.fromarray(np.uint8(img_b))\n",
    "        img_l = Image.fromarray(np.uint8(img_l))\n",
    "        img_b = torch.tensor(np.array(img_b), dtype=torch.float32).permute(2, 0, 1)  # Convert to CxHxW\n",
    "        img_l = torch.tensor(np.array(img_l), dtype=torch.float32).permute(2, 0, 1)  # Convert to CxHxW\n",
    "\n",
    "        # Extract numeric data\n",
    "        numeric_data = row[['Age', 'Gender', 'Weight', 'Height', 'Diabetes Status', 'A1C',\n",
    "                            'Baseline Fasting Glucose', 'Insulin', 'Triglycerides', 'Cholesterol',\n",
    "                            'HDL', 'Non-HDL', 'LDL', 'VLDL', 'CHO/HDL Ratio', 'HOMA-IR', 'BMI',\n",
    "                            'Race_African American', 'Race_Hispanic/Latino', 'Race_White']].values.astype(np.float32)\n",
    "\n",
    "        # Normalize numeric data\n",
    "        numeric_data = (numeric_data - np.mean(numeric_data)) / (np.std(numeric_data) + epsilon)\n",
    "        numeric_data = torch.tensor(numeric_data, dtype=torch.float32)\n",
    "\n",
    "        # Extract and reshape viome_sequential\n",
    "        viome_sequential = np.array(row['viome_sequential'], dtype=float).reshape(27, 1)\n",
    "\n",
    "        # Normalize viome_sequential\n",
    "        viome_sequential = (viome_sequential - np.mean(viome_sequential)) / (np.std(viome_sequential) + epsilon)\n",
    "\n",
    "\n",
    "        return {\n",
    "            'sequential_data': torch.tensor(sequential_data, dtype=torch.float32),\n",
    "            'viome_sequential': torch.tensor(viome_sequential, dtype=torch.float32),\n",
    "            'img_b': img_b,\n",
    "            'img_l': img_l,\n",
    "            'numeric_data': numeric_data,\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample 0:\n",
      "  sequential_data shape: torch.Size([288, 2])\n",
      "  viome_sequential shape: torch.Size([27, 1])\n",
      "  img_b shape: torch.Size([3, 64, 64])\n",
      "  img_l shape: torch.Size([3, 64, 64])\n",
      "  numeric_data shape: torch.Size([20])\n",
      "  label: 830\n"
     ]
    }
   ],
   "source": [
    "dataset_train = CustomDataset(combined_train)\n",
    "dataset_test = CustomTestDataset(combined_test)\n",
    "combined_train['img_b'].apply(lambda x: x.shape).unique()\n",
    "\n",
    "for i in range(1):\n",
    "    sample = dataset_train[i]\n",
    "    print(f\"Sample {i}:\")\n",
    "    print(f\"  sequential_data shape: {sample['sequential_data'].shape}\")\n",
    "    print(f\"  viome_sequential shape: {sample['viome_sequential'].shape}\")\n",
    "    print(f\"  img_b shape: {sample['img_b'].shape}\")\n",
    "    print(f\"  img_l shape: {sample['img_l'].shape}\")\n",
    "    print(f\"  numeric_data shape: {sample['numeric_data'].shape}\")\n",
    "    print(f\"  label: {sample['label']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader\n",
    "import torchvision.models as models\n",
    "\n",
    "class CombinedModel(nn.Module):\n",
    "    def __init__(self, hidden_size=64):\n",
    "        super(CombinedModel, self).__init__()\n",
    "        \n",
    "        # Sequential data sub-network\n",
    "        self.seq_net = nn.LSTM(input_size=2, hidden_size=hidden_size, num_layers=1, batch_first=True)\n",
    "        self.seq_fc = nn.Linear(hidden_size, 32)\n",
    "        \n",
    "        # Viome sequential data sub-network\n",
    "        self.viome_net = nn.LSTM(input_size=1, hidden_size=hidden_size, num_layers=1, batch_first=True)\n",
    "        self.viome_fc = nn.Linear(hidden_size, 32)\n",
    "        \n",
    "        # Image data sub-network using pre-trained ResNet\n",
    "        self.img_net = models.resnet50(pretrained=False)\n",
    "        self.img_net.fc = nn.Linear(self.img_net.fc.in_features, 32)  # Modify the final layer\n",
    "        \n",
    "        # Numeric data sub-network\n",
    "        self.num_net = nn.Sequential(\n",
    "            nn.Linear(20, 32),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(32, 16)\n",
    "        )\n",
    "        \n",
    "        # Combined fully connected layers\n",
    "        self.fc = nn.Sequential(\n",
    "            nn.Linear(32 + 32 + 32 + 32 + 16, 64),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(64, 1)  # Assuming a regression task\n",
    "        )\n",
    "    \n",
    "    def forward(self, sequential_data, img_b, img_l, numeric_data, viome_sequential):\n",
    "        # Process sequential data\n",
    "        seq_out, _ = self.seq_net(sequential_data)\n",
    "        seq_out = self.seq_fc(seq_out[:, -1, :])  # Take the last output of the LSTM and pass through a linear layer\n",
    "        \n",
    "        # Process viome sequential data\n",
    "        viome_out, _ = self.viome_net(viome_sequential)\n",
    "        viome_out = self.viome_fc(viome_out[:, -1, :])  # Take the last output of the LSTM and pass through a linear layer\n",
    "        \n",
    "        # Process image data\n",
    "        img_b_out = self.img_net(img_b)\n",
    "        img_l_out = self.img_net(img_l)\n",
    "        \n",
    "        # Process numeric data\n",
    "        num_out = self.num_net(numeric_data)\n",
    "        \n",
    "        # Concatenate all outputs\n",
    "        combined = torch.cat((seq_out, viome_out, img_b_out, img_l_out, num_out), dim=1)\n",
    "        \n",
    "        # Pass through fully connected layers\n",
    "        out = self.fc(combined)\n",
    "        \n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\10374\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torchvision\\models\\_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "c:\\Users\\10374\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torchvision\\models\\_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=None`.\n",
      "  warnings.warn(msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/50], Loss: 0.9707751936382718\n",
      "Epoch [2/50], Loss: 0.7751023570696512\n",
      "Epoch [3/50], Loss: 0.4197426471445296\n",
      "Epoch [4/50], Loss: 0.3628566960493724\n",
      "Epoch [5/50], Loss: 0.3345205585161845\n",
      "Epoch [6/50], Loss: 0.33391284611490035\n",
      "Epoch [7/50], Loss: 0.33558954464064705\n",
      "Epoch [8/50], Loss: 0.33615383836958146\n",
      "Epoch [9/50], Loss: 0.33505045705371433\n",
      "Epoch [10/50], Loss: 0.3338976403077443\n",
      "Epoch [11/50], Loss: 0.3345744444264306\n",
      "Epoch [12/50], Loss: 0.33175089293056065\n",
      "Epoch [13/50], Loss: 0.3352155453628964\n",
      "Epoch [14/50], Loss: 0.33526430527369183\n",
      "Epoch [15/50], Loss: 0.33334535029199386\n",
      "Epoch [16/50], Loss: 0.33565447065565324\n",
      "Epoch [17/50], Loss: 0.33446647061241996\n",
      "Epoch [18/50], Loss: 0.3377770417266422\n",
      "Epoch [19/50], Loss: 0.334703716966841\n",
      "Epoch [20/50], Loss: 0.3352616760465834\n",
      "Epoch [21/50], Loss: 0.33306145999166703\n",
      "Epoch [22/50], Loss: 0.3350561824109819\n",
      "Epoch [23/50], Loss: 0.3344064868158764\n",
      "Epoch [24/50], Loss: 0.3353710373242696\n",
      "Epoch [25/50], Loss: 0.33563068840238786\n",
      "Epoch [26/50], Loss: 0.33465727501445347\n",
      "Epoch [27/50], Loss: 0.33421414759423995\n",
      "Epoch [28/50], Loss: 0.3356625801987118\n",
      "Epoch [29/50], Loss: 0.33511369427045185\n",
      "Epoch [30/50], Loss: 0.33376584119266933\n",
      "Epoch [31/50], Loss: 0.3337814443641239\n",
      "Epoch [32/50], Loss: 0.3318455119927724\n",
      "Epoch [33/50], Loss: 0.3345963789357079\n",
      "Epoch [34/50], Loss: 0.33587263690100777\n",
      "Epoch [35/50], Loss: 0.33735570311546326\n",
      "Epoch [36/50], Loss: 0.33578206764327156\n",
      "Epoch [37/50], Loss: 0.34005839625994366\n",
      "Epoch [38/50], Loss: 0.3389720320701599\n",
      "Epoch [39/50], Loss: 0.3364027639230092\n",
      "Epoch [40/50], Loss: 0.3330273429552714\n",
      "Epoch [41/50], Loss: 0.33460620045661926\n",
      "Epoch [42/50], Loss: 0.33454564213752747\n",
      "Epoch [43/50], Loss: 0.3351793156729804\n",
      "Epoch [44/50], Loss: 0.33519452148013645\n",
      "Epoch [45/50], Loss: 0.3345024618837569\n",
      "Epoch [46/50], Loss: 0.33412591947449577\n",
      "Epoch [47/50], Loss: 0.33453044295310974\n",
      "Epoch [48/50], Loss: 0.3342668049865299\n",
      "Epoch [49/50], Loss: 0.33516688148180646\n",
      "Epoch [50/50], Loss: 0.33741329113642377\n",
      "Training complete.\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "epoch_losses = []\n",
    "\n",
    "class RMSRELoss(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(RMSRELoss, self).__init__()\n",
    "\n",
    "    def forward(self, y_pred, y_true):\n",
    "        epsilon = 1e-6  # Small value to avoid division by zero\n",
    "        relative_error = (y_pred - y_true) / (y_true + epsilon)\n",
    "        return torch.sqrt(torch.mean(relative_error ** 2))\n",
    "    \n",
    "train_loader = DataLoader(dataset_train, batch_size=32, shuffle=True)\n",
    "test_loader = DataLoader(dataset_test, batch_size=32, shuffle=False)\n",
    "\n",
    "model = CombinedModel()\n",
    "criterion = RMSRELoss()  \n",
    "optimizer = optim.Adam(model.parameters(), lr=1e-3)  # Reduced learning rate\n",
    "\n",
    "# Learning rate scheduler\n",
    "scheduler = optim.lr_scheduler.StepLR(optimizer, step_size=50, gamma=0.1)\n",
    "device = torch.device(\"cuda\")\n",
    "\n",
    "model = model.to(device)\n",
    "criterion = criterion.to(device)\n",
    "\n",
    "num_epochs = 50\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "    for batch in train_loader:\n",
    "        sequential_data = batch['sequential_data'].to(device)\n",
    "        img_b = batch['img_b'].to(device)\n",
    "        img_l = batch['img_l'].to(device)\n",
    "        numeric_data = batch['numeric_data'].to(device)\n",
    "        viome_sequential = batch['viome_sequential'].to(device)\n",
    "        label = batch['label'].to(device)\n",
    "        \n",
    "        # Forward pass\n",
    "        outputs = model(sequential_data, img_b, img_l, numeric_data, viome_sequential)\n",
    "        loss = criterion(outputs, label)\n",
    "        \n",
    "        # Backward pass and optimization\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        running_loss += loss.item()\n",
    "    \n",
    "\n",
    "    epoch_loss = running_loss / len(train_loader)\n",
    "    epoch_losses.append(epoch_loss)\n",
    "    print(f\"Epoch [{epoch+1}/{num_epochs}], Loss: {epoch_loss}\")\n",
    "\n",
    "    scheduler.step()\n",
    "\n",
    "print(\"Training complete.\")\n",
    "\n",
    "plt.plot(range(1, num_epochs + 1), epoch_losses, marker='o')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.title('Training Loss over Epochs')\n",
    "plt.grid(True)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predictions saved to 'submission.csv' in Kaggle format.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "model.eval()  # Set the model to evaluation mode\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = model.to(device)\n",
    "test_loss = 0.0\n",
    "all_outputs = []\n",
    "all_labels = []\n",
    "\n",
    "with torch.no_grad():  # Disable gradient calculation\n",
    "    for batch in test_loader:\n",
    "        sequential_data = batch['sequential_data'].to(device)\n",
    "        img_b = batch['img_b'].to(device)\n",
    "        img_l = batch['img_l'].to(device)\n",
    "        numeric_data = batch['numeric_data'].to(device)\n",
    "        viome_sequential = batch['viome_sequential'].to(device)\n",
    "        \n",
    "        # Forward pass\n",
    "        outputs = model(sequential_data, img_b, img_l, numeric_data, viome_sequential)\n",
    "        \n",
    "        all_outputs += ([i[0] for i in outputs.tolist()])\n",
    "        \n",
    "test_ids = np.arange(len(all_outputs))  \n",
    "submission = pd.DataFrame({\"row_id\": test_ids, \"label\": all_outputs})\n",
    "submission.to_csv(\"submission.csv\", index=False)\n",
    "print(\"Predictions saved to 'submission.csv' in Kaggle format.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\10374\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torchvision\\models\\_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "c:\\Users\\10374\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torchvision\\models\\_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=None`.\n",
      "  warnings.warn(msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/50], Hidden Size: 32, LR: 0.001, Batch Size: 16, Loss: 0.9222\n",
      "Epoch [2/50], Hidden Size: 32, LR: 0.001, Batch Size: 16, Loss: 0.4521\n",
      "Epoch [3/50], Hidden Size: 32, LR: 0.001, Batch Size: 16, Loss: 0.3379\n",
      "Epoch [4/50], Hidden Size: 32, LR: 0.001, Batch Size: 16, Loss: 0.3320\n",
      "Epoch [5/50], Hidden Size: 32, LR: 0.001, Batch Size: 16, Loss: 0.3355\n",
      "Epoch [6/50], Hidden Size: 32, LR: 0.001, Batch Size: 16, Loss: 0.3353\n",
      "Epoch [7/50], Hidden Size: 32, LR: 0.001, Batch Size: 16, Loss: 0.3343\n",
      "Epoch [8/50], Hidden Size: 32, LR: 0.001, Batch Size: 16, Loss: 0.3314\n",
      "Epoch [9/50], Hidden Size: 32, LR: 0.001, Batch Size: 16, Loss: 0.3312\n",
      "Epoch [10/50], Hidden Size: 32, LR: 0.001, Batch Size: 16, Loss: 0.3356\n",
      "Epoch [11/50], Hidden Size: 32, LR: 0.001, Batch Size: 16, Loss: 0.3353\n",
      "Epoch [12/50], Hidden Size: 32, LR: 0.001, Batch Size: 16, Loss: 0.3386\n",
      "Epoch [13/50], Hidden Size: 32, LR: 0.001, Batch Size: 16, Loss: 0.3473\n",
      "Epoch [14/50], Hidden Size: 32, LR: 0.001, Batch Size: 16, Loss: 0.3319\n",
      "Epoch [15/50], Hidden Size: 32, LR: 0.001, Batch Size: 16, Loss: 0.3395\n",
      "Epoch [16/50], Hidden Size: 32, LR: 0.001, Batch Size: 16, Loss: 0.3420\n",
      "Epoch [17/50], Hidden Size: 32, LR: 0.001, Batch Size: 16, Loss: 0.3348\n",
      "Epoch [18/50], Hidden Size: 32, LR: 0.001, Batch Size: 16, Loss: 0.3328\n",
      "Epoch [19/50], Hidden Size: 32, LR: 0.001, Batch Size: 16, Loss: 0.3379\n",
      "Epoch [20/50], Hidden Size: 32, LR: 0.001, Batch Size: 16, Loss: 0.3323\n",
      "Epoch [21/50], Hidden Size: 32, LR: 0.001, Batch Size: 16, Loss: 0.3326\n",
      "Epoch [22/50], Hidden Size: 32, LR: 0.001, Batch Size: 16, Loss: 0.3536\n",
      "Epoch [23/50], Hidden Size: 32, LR: 0.001, Batch Size: 16, Loss: 0.3424\n",
      "Epoch [24/50], Hidden Size: 32, LR: 0.001, Batch Size: 16, Loss: 0.3360\n",
      "Epoch [25/50], Hidden Size: 32, LR: 0.001, Batch Size: 16, Loss: 0.3314\n",
      "Epoch [26/50], Hidden Size: 32, LR: 0.001, Batch Size: 16, Loss: 0.3336\n",
      "Epoch [27/50], Hidden Size: 32, LR: 0.001, Batch Size: 16, Loss: 0.3345\n",
      "Epoch [28/50], Hidden Size: 32, LR: 0.001, Batch Size: 16, Loss: 0.3319\n",
      "Epoch [29/50], Hidden Size: 32, LR: 0.001, Batch Size: 16, Loss: 0.3354\n",
      "Epoch [30/50], Hidden Size: 32, LR: 0.001, Batch Size: 16, Loss: 0.3331\n",
      "Epoch [31/50], Hidden Size: 32, LR: 0.001, Batch Size: 16, Loss: 0.3353\n",
      "Epoch [32/50], Hidden Size: 32, LR: 0.001, Batch Size: 16, Loss: 0.3384\n",
      "Epoch [33/50], Hidden Size: 32, LR: 0.001, Batch Size: 16, Loss: 0.3355\n",
      "Epoch [34/50], Hidden Size: 32, LR: 0.001, Batch Size: 16, Loss: 0.3370\n",
      "Epoch [35/50], Hidden Size: 32, LR: 0.001, Batch Size: 16, Loss: 0.3320\n",
      "Epoch [36/50], Hidden Size: 32, LR: 0.001, Batch Size: 16, Loss: 0.3337\n",
      "Epoch [37/50], Hidden Size: 32, LR: 0.001, Batch Size: 16, Loss: 0.3351\n",
      "Epoch [38/50], Hidden Size: 32, LR: 0.001, Batch Size: 16, Loss: 0.3285\n",
      "Epoch [39/50], Hidden Size: 32, LR: 0.001, Batch Size: 16, Loss: 0.3386\n",
      "Epoch [40/50], Hidden Size: 32, LR: 0.001, Batch Size: 16, Loss: 0.3333\n",
      "Epoch [41/50], Hidden Size: 32, LR: 0.001, Batch Size: 16, Loss: 0.3336\n",
      "Epoch [42/50], Hidden Size: 32, LR: 0.001, Batch Size: 16, Loss: 0.3347\n",
      "Epoch [43/50], Hidden Size: 32, LR: 0.001, Batch Size: 16, Loss: 0.3393\n",
      "Epoch [44/50], Hidden Size: 32, LR: 0.001, Batch Size: 16, Loss: 0.3305\n",
      "Epoch [45/50], Hidden Size: 32, LR: 0.001, Batch Size: 16, Loss: 0.3364\n",
      "Epoch [46/50], Hidden Size: 32, LR: 0.001, Batch Size: 16, Loss: 0.3332\n",
      "Epoch [47/50], Hidden Size: 32, LR: 0.001, Batch Size: 16, Loss: 0.3391\n",
      "Epoch [48/50], Hidden Size: 32, LR: 0.001, Batch Size: 16, Loss: 0.3331\n",
      "Epoch [49/50], Hidden Size: 32, LR: 0.001, Batch Size: 16, Loss: 0.3308\n",
      "Epoch [50/50], Hidden Size: 32, LR: 0.001, Batch Size: 16, Loss: 0.3357\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\10374\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torchvision\\models\\_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "c:\\Users\\10374\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torchvision\\models\\_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=None`.\n",
      "  warnings.warn(msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/50], Hidden Size: 32, LR: 0.001, Batch Size: 32, Loss: 0.9606\n",
      "Epoch [2/50], Hidden Size: 32, LR: 0.001, Batch Size: 32, Loss: 0.7038\n",
      "Epoch [3/50], Hidden Size: 32, LR: 0.001, Batch Size: 32, Loss: 0.3803\n",
      "Epoch [4/50], Hidden Size: 32, LR: 0.001, Batch Size: 32, Loss: 0.3452\n",
      "Epoch [5/50], Hidden Size: 32, LR: 0.001, Batch Size: 32, Loss: 0.3354\n",
      "Epoch [6/50], Hidden Size: 32, LR: 0.001, Batch Size: 32, Loss: 0.3336\n",
      "Epoch [7/50], Hidden Size: 32, LR: 0.001, Batch Size: 32, Loss: 0.3340\n",
      "Epoch [8/50], Hidden Size: 32, LR: 0.001, Batch Size: 32, Loss: 0.3333\n",
      "Epoch [9/50], Hidden Size: 32, LR: 0.001, Batch Size: 32, Loss: 0.3387\n",
      "Epoch [10/50], Hidden Size: 32, LR: 0.001, Batch Size: 32, Loss: 0.3349\n",
      "Epoch [11/50], Hidden Size: 32, LR: 0.001, Batch Size: 32, Loss: 0.3347\n",
      "Epoch [12/50], Hidden Size: 32, LR: 0.001, Batch Size: 32, Loss: 0.3358\n",
      "Epoch [13/50], Hidden Size: 32, LR: 0.001, Batch Size: 32, Loss: 0.3355\n",
      "Epoch [14/50], Hidden Size: 32, LR: 0.001, Batch Size: 32, Loss: 0.3344\n",
      "Epoch [15/50], Hidden Size: 32, LR: 0.001, Batch Size: 32, Loss: 0.3355\n",
      "Epoch [16/50], Hidden Size: 32, LR: 0.001, Batch Size: 32, Loss: 0.3347\n",
      "Epoch [17/50], Hidden Size: 32, LR: 0.001, Batch Size: 32, Loss: 0.3329\n",
      "Epoch [18/50], Hidden Size: 32, LR: 0.001, Batch Size: 32, Loss: 0.3368\n",
      "Epoch [19/50], Hidden Size: 32, LR: 0.001, Batch Size: 32, Loss: 0.3362\n",
      "Epoch [20/50], Hidden Size: 32, LR: 0.001, Batch Size: 32, Loss: 0.3342\n",
      "Epoch [21/50], Hidden Size: 32, LR: 0.001, Batch Size: 32, Loss: 0.3345\n",
      "Epoch [22/50], Hidden Size: 32, LR: 0.001, Batch Size: 32, Loss: 0.3357\n",
      "Epoch [23/50], Hidden Size: 32, LR: 0.001, Batch Size: 32, Loss: 0.3323\n",
      "Epoch [24/50], Hidden Size: 32, LR: 0.001, Batch Size: 32, Loss: 0.3347\n",
      "Epoch [25/50], Hidden Size: 32, LR: 0.001, Batch Size: 32, Loss: 0.3352\n",
      "Epoch [26/50], Hidden Size: 32, LR: 0.001, Batch Size: 32, Loss: 0.3320\n",
      "Epoch [27/50], Hidden Size: 32, LR: 0.001, Batch Size: 32, Loss: 0.3399\n",
      "Epoch [28/50], Hidden Size: 32, LR: 0.001, Batch Size: 32, Loss: 0.3358\n",
      "Epoch [29/50], Hidden Size: 32, LR: 0.001, Batch Size: 32, Loss: 0.3358\n",
      "Epoch [30/50], Hidden Size: 32, LR: 0.001, Batch Size: 32, Loss: 0.3355\n",
      "Epoch [31/50], Hidden Size: 32, LR: 0.001, Batch Size: 32, Loss: 0.3355\n",
      "Epoch [32/50], Hidden Size: 32, LR: 0.001, Batch Size: 32, Loss: 0.3365\n",
      "Epoch [33/50], Hidden Size: 32, LR: 0.001, Batch Size: 32, Loss: 0.3341\n",
      "Epoch [34/50], Hidden Size: 32, LR: 0.001, Batch Size: 32, Loss: 0.3360\n",
      "Epoch [35/50], Hidden Size: 32, LR: 0.001, Batch Size: 32, Loss: 0.3329\n",
      "Epoch [36/50], Hidden Size: 32, LR: 0.001, Batch Size: 32, Loss: 0.3341\n",
      "Epoch [37/50], Hidden Size: 32, LR: 0.001, Batch Size: 32, Loss: 0.3352\n",
      "Epoch [38/50], Hidden Size: 32, LR: 0.001, Batch Size: 32, Loss: 0.3368\n",
      "Epoch [39/50], Hidden Size: 32, LR: 0.001, Batch Size: 32, Loss: 0.3355\n",
      "Epoch [40/50], Hidden Size: 32, LR: 0.001, Batch Size: 32, Loss: 0.3350\n",
      "Epoch [41/50], Hidden Size: 32, LR: 0.001, Batch Size: 32, Loss: 0.3351\n",
      "Epoch [42/50], Hidden Size: 32, LR: 0.001, Batch Size: 32, Loss: 0.3344\n",
      "Epoch [43/50], Hidden Size: 32, LR: 0.001, Batch Size: 32, Loss: 0.3359\n",
      "Epoch [44/50], Hidden Size: 32, LR: 0.001, Batch Size: 32, Loss: 0.3335\n",
      "Epoch [45/50], Hidden Size: 32, LR: 0.001, Batch Size: 32, Loss: 0.3344\n",
      "Epoch [46/50], Hidden Size: 32, LR: 0.001, Batch Size: 32, Loss: 0.3383\n",
      "Epoch [47/50], Hidden Size: 32, LR: 0.001, Batch Size: 32, Loss: 0.3371\n",
      "Epoch [48/50], Hidden Size: 32, LR: 0.001, Batch Size: 32, Loss: 0.3343\n",
      "Epoch [49/50], Hidden Size: 32, LR: 0.001, Batch Size: 32, Loss: 0.3344\n",
      "Epoch [50/50], Hidden Size: 32, LR: 0.001, Batch Size: 32, Loss: 0.3374\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\10374\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torchvision\\models\\_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "c:\\Users\\10374\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torchvision\\models\\_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=None`.\n",
      "  warnings.warn(msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/50], Hidden Size: 32, LR: 0.001, Batch Size: 64, Loss: 0.9941\n",
      "Epoch [2/50], Hidden Size: 32, LR: 0.001, Batch Size: 64, Loss: 0.9525\n",
      "Epoch [3/50], Hidden Size: 32, LR: 0.001, Batch Size: 64, Loss: 0.8561\n",
      "Epoch [4/50], Hidden Size: 32, LR: 0.001, Batch Size: 64, Loss: 0.6962\n",
      "Epoch [5/50], Hidden Size: 32, LR: 0.001, Batch Size: 64, Loss: 0.4681\n",
      "Epoch [6/50], Hidden Size: 32, LR: 0.001, Batch Size: 64, Loss: 0.3543\n",
      "Epoch [7/50], Hidden Size: 32, LR: 0.001, Batch Size: 64, Loss: 0.3626\n",
      "Epoch [8/50], Hidden Size: 32, LR: 0.001, Batch Size: 64, Loss: 0.3457\n",
      "Epoch [9/50], Hidden Size: 32, LR: 0.001, Batch Size: 64, Loss: 0.3332\n",
      "Epoch [10/50], Hidden Size: 32, LR: 0.001, Batch Size: 64, Loss: 0.3326\n",
      "Epoch [11/50], Hidden Size: 32, LR: 0.001, Batch Size: 64, Loss: 0.3329\n",
      "Epoch [12/50], Hidden Size: 32, LR: 0.001, Batch Size: 64, Loss: 0.3283\n",
      "Epoch [13/50], Hidden Size: 32, LR: 0.001, Batch Size: 64, Loss: 0.3295\n",
      "Epoch [14/50], Hidden Size: 32, LR: 0.001, Batch Size: 64, Loss: 0.3376\n",
      "Epoch [15/50], Hidden Size: 32, LR: 0.001, Batch Size: 64, Loss: 0.3413\n",
      "Epoch [16/50], Hidden Size: 32, LR: 0.001, Batch Size: 64, Loss: 0.3341\n",
      "Epoch [17/50], Hidden Size: 32, LR: 0.001, Batch Size: 64, Loss: 0.3323\n",
      "Epoch [18/50], Hidden Size: 32, LR: 0.001, Batch Size: 64, Loss: 0.3292\n",
      "Epoch [19/50], Hidden Size: 32, LR: 0.001, Batch Size: 64, Loss: 0.3305\n",
      "Epoch [20/50], Hidden Size: 32, LR: 0.001, Batch Size: 64, Loss: 0.3368\n",
      "Epoch [21/50], Hidden Size: 32, LR: 0.001, Batch Size: 64, Loss: 0.3320\n",
      "Epoch [22/50], Hidden Size: 32, LR: 0.001, Batch Size: 64, Loss: 0.3265\n",
      "Epoch [23/50], Hidden Size: 32, LR: 0.001, Batch Size: 64, Loss: 0.3422\n",
      "Epoch [24/50], Hidden Size: 32, LR: 0.001, Batch Size: 64, Loss: 0.3371\n",
      "Epoch [25/50], Hidden Size: 32, LR: 0.001, Batch Size: 64, Loss: 0.3366\n",
      "Epoch [26/50], Hidden Size: 32, LR: 0.001, Batch Size: 64, Loss: 0.3360\n",
      "Epoch [27/50], Hidden Size: 32, LR: 0.001, Batch Size: 64, Loss: 0.3357\n",
      "Epoch [28/50], Hidden Size: 32, LR: 0.001, Batch Size: 64, Loss: 0.3393\n",
      "Epoch [29/50], Hidden Size: 32, LR: 0.001, Batch Size: 64, Loss: 0.3292\n",
      "Epoch [30/50], Hidden Size: 32, LR: 0.001, Batch Size: 64, Loss: 0.3334\n",
      "Epoch [31/50], Hidden Size: 32, LR: 0.001, Batch Size: 64, Loss: 0.3292\n",
      "Epoch [32/50], Hidden Size: 32, LR: 0.001, Batch Size: 64, Loss: 0.3377\n",
      "Epoch [33/50], Hidden Size: 32, LR: 0.001, Batch Size: 64, Loss: 0.3275\n",
      "Epoch [34/50], Hidden Size: 32, LR: 0.001, Batch Size: 64, Loss: 0.3258\n",
      "Epoch [35/50], Hidden Size: 32, LR: 0.001, Batch Size: 64, Loss: 0.3324\n",
      "Epoch [36/50], Hidden Size: 32, LR: 0.001, Batch Size: 64, Loss: 0.3393\n",
      "Epoch [37/50], Hidden Size: 32, LR: 0.001, Batch Size: 64, Loss: 0.3418\n",
      "Epoch [38/50], Hidden Size: 32, LR: 0.001, Batch Size: 64, Loss: 0.3280\n",
      "Epoch [39/50], Hidden Size: 32, LR: 0.001, Batch Size: 64, Loss: 0.3350\n",
      "Epoch [40/50], Hidden Size: 32, LR: 0.001, Batch Size: 64, Loss: 0.3372\n",
      "Epoch [41/50], Hidden Size: 32, LR: 0.001, Batch Size: 64, Loss: 0.3364\n",
      "Epoch [42/50], Hidden Size: 32, LR: 0.001, Batch Size: 64, Loss: 0.3291\n",
      "Epoch [43/50], Hidden Size: 32, LR: 0.001, Batch Size: 64, Loss: 0.3365\n",
      "Epoch [44/50], Hidden Size: 32, LR: 0.001, Batch Size: 64, Loss: 0.3394\n",
      "Epoch [45/50], Hidden Size: 32, LR: 0.001, Batch Size: 64, Loss: 0.3305\n",
      "Epoch [46/50], Hidden Size: 32, LR: 0.001, Batch Size: 64, Loss: 0.3337\n",
      "Epoch [47/50], Hidden Size: 32, LR: 0.001, Batch Size: 64, Loss: 0.3331\n",
      "Epoch [48/50], Hidden Size: 32, LR: 0.001, Batch Size: 64, Loss: 0.3376\n",
      "Epoch [49/50], Hidden Size: 32, LR: 0.001, Batch Size: 64, Loss: 0.3395\n",
      "Epoch [50/50], Hidden Size: 32, LR: 0.001, Batch Size: 64, Loss: 0.3345\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\10374\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torchvision\\models\\_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "c:\\Users\\10374\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torchvision\\models\\_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=None`.\n",
      "  warnings.warn(msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/50], Hidden Size: 32, LR: 0.0001, Batch Size: 16, Loss: 0.9978\n",
      "Epoch [2/50], Hidden Size: 32, LR: 0.0001, Batch Size: 16, Loss: 0.9885\n",
      "Epoch [3/50], Hidden Size: 32, LR: 0.0001, Batch Size: 16, Loss: 0.9674\n",
      "Epoch [4/50], Hidden Size: 32, LR: 0.0001, Batch Size: 16, Loss: 0.9276\n",
      "Epoch [5/50], Hidden Size: 32, LR: 0.0001, Batch Size: 16, Loss: 0.8711\n",
      "Epoch [6/50], Hidden Size: 32, LR: 0.0001, Batch Size: 16, Loss: 0.7948\n",
      "Epoch [7/50], Hidden Size: 32, LR: 0.0001, Batch Size: 16, Loss: 0.7010\n",
      "Epoch [8/50], Hidden Size: 32, LR: 0.0001, Batch Size: 16, Loss: 0.5892\n",
      "Epoch [9/50], Hidden Size: 32, LR: 0.0001, Batch Size: 16, Loss: 0.4734\n",
      "Epoch [10/50], Hidden Size: 32, LR: 0.0001, Batch Size: 16, Loss: 0.3713\n",
      "Epoch [11/50], Hidden Size: 32, LR: 0.0001, Batch Size: 16, Loss: 0.3352\n",
      "Epoch [12/50], Hidden Size: 32, LR: 0.0001, Batch Size: 16, Loss: 0.3304\n",
      "Epoch [13/50], Hidden Size: 32, LR: 0.0001, Batch Size: 16, Loss: 0.3317\n",
      "Epoch [14/50], Hidden Size: 32, LR: 0.0001, Batch Size: 16, Loss: 0.3310\n",
      "Epoch [15/50], Hidden Size: 32, LR: 0.0001, Batch Size: 16, Loss: 0.3334\n",
      "Epoch [16/50], Hidden Size: 32, LR: 0.0001, Batch Size: 16, Loss: 0.3318\n",
      "Epoch [17/50], Hidden Size: 32, LR: 0.0001, Batch Size: 16, Loss: 0.3321\n",
      "Epoch [18/50], Hidden Size: 32, LR: 0.0001, Batch Size: 16, Loss: 0.3345\n",
      "Epoch [19/50], Hidden Size: 32, LR: 0.0001, Batch Size: 16, Loss: 0.3341\n",
      "Epoch [20/50], Hidden Size: 32, LR: 0.0001, Batch Size: 16, Loss: 0.3321\n",
      "Epoch [21/50], Hidden Size: 32, LR: 0.0001, Batch Size: 16, Loss: 0.3305\n",
      "Epoch [22/50], Hidden Size: 32, LR: 0.0001, Batch Size: 16, Loss: 0.3322\n",
      "Epoch [23/50], Hidden Size: 32, LR: 0.0001, Batch Size: 16, Loss: 0.3330\n",
      "Epoch [24/50], Hidden Size: 32, LR: 0.0001, Batch Size: 16, Loss: 0.3275\n",
      "Epoch [25/50], Hidden Size: 32, LR: 0.0001, Batch Size: 16, Loss: 0.3323\n",
      "Epoch [26/50], Hidden Size: 32, LR: 0.0001, Batch Size: 16, Loss: 0.3328\n",
      "Epoch [27/50], Hidden Size: 32, LR: 0.0001, Batch Size: 16, Loss: 0.3315\n",
      "Epoch [28/50], Hidden Size: 32, LR: 0.0001, Batch Size: 16, Loss: 0.3303\n",
      "Epoch [29/50], Hidden Size: 32, LR: 0.0001, Batch Size: 16, Loss: 0.3313\n",
      "Epoch [30/50], Hidden Size: 32, LR: 0.0001, Batch Size: 16, Loss: 0.3294\n",
      "Epoch [31/50], Hidden Size: 32, LR: 0.0001, Batch Size: 16, Loss: 0.3317\n",
      "Epoch [32/50], Hidden Size: 32, LR: 0.0001, Batch Size: 16, Loss: 0.3329\n",
      "Epoch [33/50], Hidden Size: 32, LR: 0.0001, Batch Size: 16, Loss: 0.3304\n",
      "Epoch [34/50], Hidden Size: 32, LR: 0.0001, Batch Size: 16, Loss: 0.3301\n",
      "Epoch [35/50], Hidden Size: 32, LR: 0.0001, Batch Size: 16, Loss: 0.3317\n",
      "Epoch [36/50], Hidden Size: 32, LR: 0.0001, Batch Size: 16, Loss: 0.3315\n",
      "Epoch [37/50], Hidden Size: 32, LR: 0.0001, Batch Size: 16, Loss: 0.3332\n",
      "Epoch [38/50], Hidden Size: 32, LR: 0.0001, Batch Size: 16, Loss: 0.3309\n",
      "Epoch [39/50], Hidden Size: 32, LR: 0.0001, Batch Size: 16, Loss: 0.3292\n",
      "Epoch [40/50], Hidden Size: 32, LR: 0.0001, Batch Size: 16, Loss: 0.3313\n",
      "Epoch [41/50], Hidden Size: 32, LR: 0.0001, Batch Size: 16, Loss: 0.3322\n",
      "Epoch [42/50], Hidden Size: 32, LR: 0.0001, Batch Size: 16, Loss: 0.3325\n",
      "Epoch [43/50], Hidden Size: 32, LR: 0.0001, Batch Size: 16, Loss: 0.3343\n",
      "Epoch [44/50], Hidden Size: 32, LR: 0.0001, Batch Size: 16, Loss: 0.3354\n",
      "Epoch [45/50], Hidden Size: 32, LR: 0.0001, Batch Size: 16, Loss: 0.3340\n",
      "Epoch [46/50], Hidden Size: 32, LR: 0.0001, Batch Size: 16, Loss: 0.3288\n",
      "Epoch [47/50], Hidden Size: 32, LR: 0.0001, Batch Size: 16, Loss: 0.3288\n",
      "Epoch [48/50], Hidden Size: 32, LR: 0.0001, Batch Size: 16, Loss: 0.3311\n",
      "Epoch [49/50], Hidden Size: 32, LR: 0.0001, Batch Size: 16, Loss: 0.3321\n",
      "Epoch [50/50], Hidden Size: 32, LR: 0.0001, Batch Size: 16, Loss: 0.3334\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\10374\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torchvision\\models\\_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "c:\\Users\\10374\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torchvision\\models\\_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=None`.\n",
      "  warnings.warn(msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/50], Hidden Size: 32, LR: 0.0001, Batch Size: 32, Loss: 0.9989\n",
      "Epoch [2/50], Hidden Size: 32, LR: 0.0001, Batch Size: 32, Loss: 0.9959\n",
      "Epoch [3/50], Hidden Size: 32, LR: 0.0001, Batch Size: 32, Loss: 0.9910\n",
      "Epoch [4/50], Hidden Size: 32, LR: 0.0001, Batch Size: 32, Loss: 0.9829\n",
      "Epoch [5/50], Hidden Size: 32, LR: 0.0001, Batch Size: 32, Loss: 0.9698\n",
      "Epoch [6/50], Hidden Size: 32, LR: 0.0001, Batch Size: 32, Loss: 0.9510\n",
      "Epoch [7/50], Hidden Size: 32, LR: 0.0001, Batch Size: 32, Loss: 0.9269\n",
      "Epoch [8/50], Hidden Size: 32, LR: 0.0001, Batch Size: 32, Loss: 0.8982\n",
      "Epoch [9/50], Hidden Size: 32, LR: 0.0001, Batch Size: 32, Loss: 0.8629\n",
      "Epoch [10/50], Hidden Size: 32, LR: 0.0001, Batch Size: 32, Loss: 0.8230\n",
      "Epoch [11/50], Hidden Size: 32, LR: 0.0001, Batch Size: 32, Loss: 0.7770\n",
      "Epoch [12/50], Hidden Size: 32, LR: 0.0001, Batch Size: 32, Loss: 0.7239\n",
      "Epoch [13/50], Hidden Size: 32, LR: 0.0001, Batch Size: 32, Loss: 0.6633\n",
      "Epoch [14/50], Hidden Size: 32, LR: 0.0001, Batch Size: 32, Loss: 0.6020\n",
      "Epoch [15/50], Hidden Size: 32, LR: 0.0001, Batch Size: 32, Loss: 0.5361\n",
      "Epoch [16/50], Hidden Size: 32, LR: 0.0001, Batch Size: 32, Loss: 0.4688\n",
      "Epoch [17/50], Hidden Size: 32, LR: 0.0001, Batch Size: 32, Loss: 0.4104\n",
      "Epoch [18/50], Hidden Size: 32, LR: 0.0001, Batch Size: 32, Loss: 0.3641\n",
      "Epoch [19/50], Hidden Size: 32, LR: 0.0001, Batch Size: 32, Loss: 0.3415\n",
      "Epoch [20/50], Hidden Size: 32, LR: 0.0001, Batch Size: 32, Loss: 0.3351\n",
      "Epoch [21/50], Hidden Size: 32, LR: 0.0001, Batch Size: 32, Loss: 0.3339\n",
      "Epoch [22/50], Hidden Size: 32, LR: 0.0001, Batch Size: 32, Loss: 0.3320\n",
      "Epoch [23/50], Hidden Size: 32, LR: 0.0001, Batch Size: 32, Loss: 0.3353\n",
      "Epoch [24/50], Hidden Size: 32, LR: 0.0001, Batch Size: 32, Loss: 0.3338\n",
      "Epoch [25/50], Hidden Size: 32, LR: 0.0001, Batch Size: 32, Loss: 0.3337\n",
      "Epoch [26/50], Hidden Size: 32, LR: 0.0001, Batch Size: 32, Loss: 0.3344\n",
      "Epoch [27/50], Hidden Size: 32, LR: 0.0001, Batch Size: 32, Loss: 0.3344\n",
      "Epoch [28/50], Hidden Size: 32, LR: 0.0001, Batch Size: 32, Loss: 0.3334\n",
      "Epoch [29/50], Hidden Size: 32, LR: 0.0001, Batch Size: 32, Loss: 0.3336\n",
      "Epoch [30/50], Hidden Size: 32, LR: 0.0001, Batch Size: 32, Loss: 0.3330\n",
      "Epoch [31/50], Hidden Size: 32, LR: 0.0001, Batch Size: 32, Loss: 0.3334\n",
      "Epoch [32/50], Hidden Size: 32, LR: 0.0001, Batch Size: 32, Loss: 0.3342\n",
      "Epoch [33/50], Hidden Size: 32, LR: 0.0001, Batch Size: 32, Loss: 0.3337\n",
      "Epoch [34/50], Hidden Size: 32, LR: 0.0001, Batch Size: 32, Loss: 0.3344\n",
      "Epoch [35/50], Hidden Size: 32, LR: 0.0001, Batch Size: 32, Loss: 0.3348\n",
      "Epoch [36/50], Hidden Size: 32, LR: 0.0001, Batch Size: 32, Loss: 0.3335\n",
      "Epoch [37/50], Hidden Size: 32, LR: 0.0001, Batch Size: 32, Loss: 0.3339\n",
      "Epoch [38/50], Hidden Size: 32, LR: 0.0001, Batch Size: 32, Loss: 0.3335\n",
      "Epoch [39/50], Hidden Size: 32, LR: 0.0001, Batch Size: 32, Loss: 0.3335\n",
      "Epoch [40/50], Hidden Size: 32, LR: 0.0001, Batch Size: 32, Loss: 0.3336\n",
      "Epoch [41/50], Hidden Size: 32, LR: 0.0001, Batch Size: 32, Loss: 0.3331\n",
      "Epoch [42/50], Hidden Size: 32, LR: 0.0001, Batch Size: 32, Loss: 0.3345\n",
      "Epoch [43/50], Hidden Size: 32, LR: 0.0001, Batch Size: 32, Loss: 0.3345\n",
      "Epoch [44/50], Hidden Size: 32, LR: 0.0001, Batch Size: 32, Loss: 0.3353\n",
      "Epoch [45/50], Hidden Size: 32, LR: 0.0001, Batch Size: 32, Loss: 0.3335\n",
      "Epoch [46/50], Hidden Size: 32, LR: 0.0001, Batch Size: 32, Loss: 0.3318\n",
      "Epoch [47/50], Hidden Size: 32, LR: 0.0001, Batch Size: 32, Loss: 0.3339\n",
      "Epoch [48/50], Hidden Size: 32, LR: 0.0001, Batch Size: 32, Loss: 0.3345\n",
      "Epoch [49/50], Hidden Size: 32, LR: 0.0001, Batch Size: 32, Loss: 0.3348\n",
      "Epoch [50/50], Hidden Size: 32, LR: 0.0001, Batch Size: 32, Loss: 0.3338\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\10374\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torchvision\\models\\_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "c:\\Users\\10374\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torchvision\\models\\_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=None`.\n",
      "  warnings.warn(msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/50], Hidden Size: 32, LR: 0.0001, Batch Size: 64, Loss: 0.9996\n",
      "Epoch [2/50], Hidden Size: 32, LR: 0.0001, Batch Size: 64, Loss: 0.9984\n",
      "Epoch [3/50], Hidden Size: 32, LR: 0.0001, Batch Size: 64, Loss: 0.9967\n",
      "Epoch [4/50], Hidden Size: 32, LR: 0.0001, Batch Size: 64, Loss: 0.9947\n",
      "Epoch [5/50], Hidden Size: 32, LR: 0.0001, Batch Size: 64, Loss: 0.9922\n",
      "Epoch [6/50], Hidden Size: 32, LR: 0.0001, Batch Size: 64, Loss: 0.9883\n",
      "Epoch [7/50], Hidden Size: 32, LR: 0.0001, Batch Size: 64, Loss: 0.9835\n",
      "Epoch [8/50], Hidden Size: 32, LR: 0.0001, Batch Size: 64, Loss: 0.9775\n",
      "Epoch [9/50], Hidden Size: 32, LR: 0.0001, Batch Size: 64, Loss: 0.9700\n",
      "Epoch [10/50], Hidden Size: 32, LR: 0.0001, Batch Size: 64, Loss: 0.9622\n",
      "Epoch [11/50], Hidden Size: 32, LR: 0.0001, Batch Size: 64, Loss: 0.9522\n",
      "Epoch [12/50], Hidden Size: 32, LR: 0.0001, Batch Size: 64, Loss: 0.9425\n",
      "Epoch [13/50], Hidden Size: 32, LR: 0.0001, Batch Size: 64, Loss: 0.9293\n",
      "Epoch [14/50], Hidden Size: 32, LR: 0.0001, Batch Size: 64, Loss: 0.9172\n",
      "Epoch [15/50], Hidden Size: 32, LR: 0.0001, Batch Size: 64, Loss: 0.9033\n",
      "Epoch [16/50], Hidden Size: 32, LR: 0.0001, Batch Size: 64, Loss: 0.8867\n",
      "Epoch [17/50], Hidden Size: 32, LR: 0.0001, Batch Size: 64, Loss: 0.8707\n",
      "Epoch [18/50], Hidden Size: 32, LR: 0.0001, Batch Size: 64, Loss: 0.8536\n",
      "Epoch [19/50], Hidden Size: 32, LR: 0.0001, Batch Size: 64, Loss: 0.8324\n",
      "Epoch [20/50], Hidden Size: 32, LR: 0.0001, Batch Size: 64, Loss: 0.8087\n",
      "Epoch [21/50], Hidden Size: 32, LR: 0.0001, Batch Size: 64, Loss: 0.7892\n",
      "Epoch [22/50], Hidden Size: 32, LR: 0.0001, Batch Size: 64, Loss: 0.7627\n",
      "Epoch [23/50], Hidden Size: 32, LR: 0.0001, Batch Size: 64, Loss: 0.7345\n",
      "Epoch [24/50], Hidden Size: 32, LR: 0.0001, Batch Size: 64, Loss: 0.7034\n",
      "Epoch [25/50], Hidden Size: 32, LR: 0.0001, Batch Size: 64, Loss: 0.6734\n",
      "Epoch [26/50], Hidden Size: 32, LR: 0.0001, Batch Size: 64, Loss: 0.6410\n",
      "Epoch [27/50], Hidden Size: 32, LR: 0.0001, Batch Size: 64, Loss: 0.6092\n",
      "Epoch [28/50], Hidden Size: 32, LR: 0.0001, Batch Size: 64, Loss: 0.5705\n",
      "Epoch [29/50], Hidden Size: 32, LR: 0.0001, Batch Size: 64, Loss: 0.5417\n",
      "Epoch [30/50], Hidden Size: 32, LR: 0.0001, Batch Size: 64, Loss: 0.5047\n",
      "Epoch [31/50], Hidden Size: 32, LR: 0.0001, Batch Size: 64, Loss: 0.4739\n",
      "Epoch [32/50], Hidden Size: 32, LR: 0.0001, Batch Size: 64, Loss: 0.4467\n",
      "Epoch [33/50], Hidden Size: 32, LR: 0.0001, Batch Size: 64, Loss: 0.4097\n",
      "Epoch [34/50], Hidden Size: 32, LR: 0.0001, Batch Size: 64, Loss: 0.3937\n",
      "Epoch [35/50], Hidden Size: 32, LR: 0.0001, Batch Size: 64, Loss: 0.3575\n",
      "Epoch [36/50], Hidden Size: 32, LR: 0.0001, Batch Size: 64, Loss: 0.3450\n",
      "Epoch [37/50], Hidden Size: 32, LR: 0.0001, Batch Size: 64, Loss: 0.3343\n",
      "Epoch [38/50], Hidden Size: 32, LR: 0.0001, Batch Size: 64, Loss: 0.3371\n",
      "Epoch [39/50], Hidden Size: 32, LR: 0.0001, Batch Size: 64, Loss: 0.3371\n",
      "Epoch [40/50], Hidden Size: 32, LR: 0.0001, Batch Size: 64, Loss: 0.3376\n",
      "Epoch [41/50], Hidden Size: 32, LR: 0.0001, Batch Size: 64, Loss: 0.3401\n",
      "Epoch [42/50], Hidden Size: 32, LR: 0.0001, Batch Size: 64, Loss: 0.3332\n",
      "Epoch [43/50], Hidden Size: 32, LR: 0.0001, Batch Size: 64, Loss: 0.3312\n",
      "Epoch [44/50], Hidden Size: 32, LR: 0.0001, Batch Size: 64, Loss: 0.3353\n",
      "Epoch [45/50], Hidden Size: 32, LR: 0.0001, Batch Size: 64, Loss: 0.3274\n",
      "Epoch [46/50], Hidden Size: 32, LR: 0.0001, Batch Size: 64, Loss: 0.3356\n",
      "Epoch [47/50], Hidden Size: 32, LR: 0.0001, Batch Size: 64, Loss: 0.3419\n",
      "Epoch [48/50], Hidden Size: 32, LR: 0.0001, Batch Size: 64, Loss: 0.3310\n",
      "Epoch [49/50], Hidden Size: 32, LR: 0.0001, Batch Size: 64, Loss: 0.3316\n",
      "Epoch [50/50], Hidden Size: 32, LR: 0.0001, Batch Size: 64, Loss: 0.3376\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\10374\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torchvision\\models\\_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "c:\\Users\\10374\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torchvision\\models\\_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=None`.\n",
      "  warnings.warn(msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/50], Hidden Size: 64, LR: 0.001, Batch Size: 16, Loss: 0.9390\n",
      "Epoch [2/50], Hidden Size: 64, LR: 0.001, Batch Size: 16, Loss: 0.4934\n",
      "Epoch [3/50], Hidden Size: 64, LR: 0.001, Batch Size: 16, Loss: 0.3472\n",
      "Epoch [4/50], Hidden Size: 64, LR: 0.001, Batch Size: 16, Loss: 0.3361\n",
      "Epoch [5/50], Hidden Size: 64, LR: 0.001, Batch Size: 16, Loss: 0.3359\n",
      "Epoch [6/50], Hidden Size: 64, LR: 0.001, Batch Size: 16, Loss: 0.3303\n",
      "Epoch [7/50], Hidden Size: 64, LR: 0.001, Batch Size: 16, Loss: 0.3345\n",
      "Epoch [8/50], Hidden Size: 64, LR: 0.001, Batch Size: 16, Loss: 0.3361\n",
      "Epoch [9/50], Hidden Size: 64, LR: 0.001, Batch Size: 16, Loss: 0.3379\n",
      "Epoch [10/50], Hidden Size: 64, LR: 0.001, Batch Size: 16, Loss: 0.3355\n",
      "Epoch [11/50], Hidden Size: 64, LR: 0.001, Batch Size: 16, Loss: 0.3352\n",
      "Epoch [12/50], Hidden Size: 64, LR: 0.001, Batch Size: 16, Loss: 0.3319\n",
      "Epoch [13/50], Hidden Size: 64, LR: 0.001, Batch Size: 16, Loss: 0.3336\n",
      "Epoch [14/50], Hidden Size: 64, LR: 0.001, Batch Size: 16, Loss: 0.3379\n",
      "Epoch [15/50], Hidden Size: 64, LR: 0.001, Batch Size: 16, Loss: 0.3329\n",
      "Epoch [16/50], Hidden Size: 64, LR: 0.001, Batch Size: 16, Loss: 0.3319\n",
      "Epoch [17/50], Hidden Size: 64, LR: 0.001, Batch Size: 16, Loss: 0.3376\n",
      "Epoch [18/50], Hidden Size: 64, LR: 0.001, Batch Size: 16, Loss: 0.3336\n",
      "Epoch [19/50], Hidden Size: 64, LR: 0.001, Batch Size: 16, Loss: 0.3333\n",
      "Epoch [20/50], Hidden Size: 64, LR: 0.001, Batch Size: 16, Loss: 0.3348\n",
      "Epoch [21/50], Hidden Size: 64, LR: 0.001, Batch Size: 16, Loss: 0.3366\n",
      "Epoch [22/50], Hidden Size: 64, LR: 0.001, Batch Size: 16, Loss: 0.3342\n",
      "Epoch [23/50], Hidden Size: 64, LR: 0.001, Batch Size: 16, Loss: 0.3336\n",
      "Epoch [24/50], Hidden Size: 64, LR: 0.001, Batch Size: 16, Loss: 0.3362\n",
      "Epoch [25/50], Hidden Size: 64, LR: 0.001, Batch Size: 16, Loss: 0.3366\n",
      "Epoch [26/50], Hidden Size: 64, LR: 0.001, Batch Size: 16, Loss: 0.3336\n",
      "Epoch [27/50], Hidden Size: 64, LR: 0.001, Batch Size: 16, Loss: 0.3357\n",
      "Epoch [28/50], Hidden Size: 64, LR: 0.001, Batch Size: 16, Loss: 0.3415\n",
      "Epoch [29/50], Hidden Size: 64, LR: 0.001, Batch Size: 16, Loss: 0.3387\n",
      "Epoch [30/50], Hidden Size: 64, LR: 0.001, Batch Size: 16, Loss: 0.3326\n",
      "Epoch [31/50], Hidden Size: 64, LR: 0.001, Batch Size: 16, Loss: 0.3325\n",
      "Epoch [32/50], Hidden Size: 64, LR: 0.001, Batch Size: 16, Loss: 0.3318\n",
      "Epoch [33/50], Hidden Size: 64, LR: 0.001, Batch Size: 16, Loss: 0.3425\n",
      "Epoch [34/50], Hidden Size: 64, LR: 0.001, Batch Size: 16, Loss: 0.3383\n",
      "Epoch [35/50], Hidden Size: 64, LR: 0.001, Batch Size: 16, Loss: 0.3340\n",
      "Epoch [36/50], Hidden Size: 64, LR: 0.001, Batch Size: 16, Loss: 0.3343\n",
      "Epoch [37/50], Hidden Size: 64, LR: 0.001, Batch Size: 16, Loss: 0.3338\n",
      "Epoch [38/50], Hidden Size: 64, LR: 0.001, Batch Size: 16, Loss: 0.3342\n",
      "Epoch [39/50], Hidden Size: 64, LR: 0.001, Batch Size: 16, Loss: 0.3340\n",
      "Epoch [40/50], Hidden Size: 64, LR: 0.001, Batch Size: 16, Loss: 0.3347\n",
      "Epoch [41/50], Hidden Size: 64, LR: 0.001, Batch Size: 16, Loss: 0.3360\n",
      "Epoch [42/50], Hidden Size: 64, LR: 0.001, Batch Size: 16, Loss: 0.3319\n",
      "Epoch [43/50], Hidden Size: 64, LR: 0.001, Batch Size: 16, Loss: 0.3356\n",
      "Epoch [44/50], Hidden Size: 64, LR: 0.001, Batch Size: 16, Loss: 0.3332\n",
      "Epoch [45/50], Hidden Size: 64, LR: 0.001, Batch Size: 16, Loss: 0.3358\n",
      "Epoch [46/50], Hidden Size: 64, LR: 0.001, Batch Size: 16, Loss: 0.3375\n",
      "Epoch [47/50], Hidden Size: 64, LR: 0.001, Batch Size: 16, Loss: 0.3341\n",
      "Epoch [48/50], Hidden Size: 64, LR: 0.001, Batch Size: 16, Loss: 0.3387\n",
      "Epoch [49/50], Hidden Size: 64, LR: 0.001, Batch Size: 16, Loss: 0.3340\n",
      "Epoch [50/50], Hidden Size: 64, LR: 0.001, Batch Size: 16, Loss: 0.3350\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\10374\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torchvision\\models\\_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "c:\\Users\\10374\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torchvision\\models\\_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=None`.\n",
      "  warnings.warn(msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/50], Hidden Size: 64, LR: 0.001, Batch Size: 32, Loss: 0.9739\n",
      "Epoch [2/50], Hidden Size: 64, LR: 0.001, Batch Size: 32, Loss: 0.7999\n",
      "Epoch [3/50], Hidden Size: 64, LR: 0.001, Batch Size: 32, Loss: 0.4388\n",
      "Epoch [4/50], Hidden Size: 64, LR: 0.001, Batch Size: 32, Loss: 0.3716\n",
      "Epoch [5/50], Hidden Size: 64, LR: 0.001, Batch Size: 32, Loss: 0.3346\n",
      "Epoch [6/50], Hidden Size: 64, LR: 0.001, Batch Size: 32, Loss: 0.3337\n",
      "Epoch [7/50], Hidden Size: 64, LR: 0.001, Batch Size: 32, Loss: 0.3337\n",
      "Epoch [8/50], Hidden Size: 64, LR: 0.001, Batch Size: 32, Loss: 0.3351\n",
      "Epoch [9/50], Hidden Size: 64, LR: 0.001, Batch Size: 32, Loss: 0.3353\n",
      "Epoch [10/50], Hidden Size: 64, LR: 0.001, Batch Size: 32, Loss: 0.3346\n",
      "Epoch [11/50], Hidden Size: 64, LR: 0.001, Batch Size: 32, Loss: 0.3348\n",
      "Epoch [12/50], Hidden Size: 64, LR: 0.001, Batch Size: 32, Loss: 0.3329\n",
      "Epoch [13/50], Hidden Size: 64, LR: 0.001, Batch Size: 32, Loss: 0.3343\n",
      "Epoch [14/50], Hidden Size: 64, LR: 0.001, Batch Size: 32, Loss: 0.3342\n",
      "Epoch [15/50], Hidden Size: 64, LR: 0.001, Batch Size: 32, Loss: 0.3347\n",
      "Epoch [16/50], Hidden Size: 64, LR: 0.001, Batch Size: 32, Loss: 0.3315\n",
      "Epoch [17/50], Hidden Size: 64, LR: 0.001, Batch Size: 32, Loss: 0.3341\n",
      "Epoch [18/50], Hidden Size: 64, LR: 0.001, Batch Size: 32, Loss: 0.3336\n",
      "Epoch [19/50], Hidden Size: 64, LR: 0.001, Batch Size: 32, Loss: 0.3351\n",
      "Epoch [20/50], Hidden Size: 64, LR: 0.001, Batch Size: 32, Loss: 0.3339\n",
      "Epoch [21/50], Hidden Size: 64, LR: 0.001, Batch Size: 32, Loss: 0.3347\n",
      "Epoch [22/50], Hidden Size: 64, LR: 0.001, Batch Size: 32, Loss: 0.3316\n",
      "Epoch [23/50], Hidden Size: 64, LR: 0.001, Batch Size: 32, Loss: 0.3340\n",
      "Epoch [24/50], Hidden Size: 64, LR: 0.001, Batch Size: 32, Loss: 0.3334\n",
      "Epoch [25/50], Hidden Size: 64, LR: 0.001, Batch Size: 32, Loss: 0.3346\n",
      "Epoch [26/50], Hidden Size: 64, LR: 0.001, Batch Size: 32, Loss: 0.3348\n",
      "Epoch [27/50], Hidden Size: 64, LR: 0.001, Batch Size: 32, Loss: 0.3352\n",
      "Epoch [28/50], Hidden Size: 64, LR: 0.001, Batch Size: 32, Loss: 0.3385\n",
      "Epoch [29/50], Hidden Size: 64, LR: 0.001, Batch Size: 32, Loss: 0.3352\n",
      "Epoch [30/50], Hidden Size: 64, LR: 0.001, Batch Size: 32, Loss: 0.3354\n",
      "Epoch [31/50], Hidden Size: 64, LR: 0.001, Batch Size: 32, Loss: 0.3348\n",
      "Epoch [32/50], Hidden Size: 64, LR: 0.001, Batch Size: 32, Loss: 0.3344\n",
      "Epoch [33/50], Hidden Size: 64, LR: 0.001, Batch Size: 32, Loss: 0.3339\n",
      "Epoch [34/50], Hidden Size: 64, LR: 0.001, Batch Size: 32, Loss: 0.3347\n",
      "Epoch [35/50], Hidden Size: 64, LR: 0.001, Batch Size: 32, Loss: 0.3356\n",
      "Epoch [36/50], Hidden Size: 64, LR: 0.001, Batch Size: 32, Loss: 0.3336\n",
      "Epoch [37/50], Hidden Size: 64, LR: 0.001, Batch Size: 32, Loss: 0.3359\n",
      "Epoch [38/50], Hidden Size: 64, LR: 0.001, Batch Size: 32, Loss: 0.3334\n",
      "Epoch [39/50], Hidden Size: 64, LR: 0.001, Batch Size: 32, Loss: 0.3343\n",
      "Epoch [40/50], Hidden Size: 64, LR: 0.001, Batch Size: 32, Loss: 0.3367\n",
      "Epoch [41/50], Hidden Size: 64, LR: 0.001, Batch Size: 32, Loss: 0.3382\n",
      "Epoch [42/50], Hidden Size: 64, LR: 0.001, Batch Size: 32, Loss: 0.3383\n",
      "Epoch [43/50], Hidden Size: 64, LR: 0.001, Batch Size: 32, Loss: 0.3334\n",
      "Epoch [44/50], Hidden Size: 64, LR: 0.001, Batch Size: 32, Loss: 0.3329\n",
      "Epoch [45/50], Hidden Size: 64, LR: 0.001, Batch Size: 32, Loss: 0.3337\n",
      "Epoch [46/50], Hidden Size: 64, LR: 0.001, Batch Size: 32, Loss: 0.3360\n",
      "Epoch [47/50], Hidden Size: 64, LR: 0.001, Batch Size: 32, Loss: 0.3358\n",
      "Epoch [48/50], Hidden Size: 64, LR: 0.001, Batch Size: 32, Loss: 0.3370\n",
      "Epoch [49/50], Hidden Size: 64, LR: 0.001, Batch Size: 32, Loss: 0.3339\n",
      "Epoch [50/50], Hidden Size: 64, LR: 0.001, Batch Size: 32, Loss: 0.3360\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\10374\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torchvision\\models\\_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "c:\\Users\\10374\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torchvision\\models\\_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=None`.\n",
      "  warnings.warn(msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/50], Hidden Size: 64, LR: 0.001, Batch Size: 64, Loss: 0.9931\n",
      "Epoch [2/50], Hidden Size: 64, LR: 0.001, Batch Size: 64, Loss: 0.9493\n",
      "Epoch [3/50], Hidden Size: 64, LR: 0.001, Batch Size: 64, Loss: 0.8533\n",
      "Epoch [4/50], Hidden Size: 64, LR: 0.001, Batch Size: 64, Loss: 0.6944\n",
      "Epoch [5/50], Hidden Size: 64, LR: 0.001, Batch Size: 64, Loss: 0.4712\n",
      "Epoch [6/50], Hidden Size: 64, LR: 0.001, Batch Size: 64, Loss: 0.3578\n",
      "Epoch [7/50], Hidden Size: 64, LR: 0.001, Batch Size: 64, Loss: 0.3724\n",
      "Epoch [8/50], Hidden Size: 64, LR: 0.001, Batch Size: 64, Loss: 0.3385\n",
      "Epoch [9/50], Hidden Size: 64, LR: 0.001, Batch Size: 64, Loss: 0.3372\n",
      "Epoch [10/50], Hidden Size: 64, LR: 0.001, Batch Size: 64, Loss: 0.3371\n",
      "Epoch [11/50], Hidden Size: 64, LR: 0.001, Batch Size: 64, Loss: 0.3374\n",
      "Epoch [12/50], Hidden Size: 64, LR: 0.001, Batch Size: 64, Loss: 0.3393\n",
      "Epoch [13/50], Hidden Size: 64, LR: 0.001, Batch Size: 64, Loss: 0.3280\n",
      "Epoch [14/50], Hidden Size: 64, LR: 0.001, Batch Size: 64, Loss: 0.3341\n",
      "Epoch [15/50], Hidden Size: 64, LR: 0.001, Batch Size: 64, Loss: 0.3371\n",
      "Epoch [16/50], Hidden Size: 64, LR: 0.001, Batch Size: 64, Loss: 0.3274\n",
      "Epoch [17/50], Hidden Size: 64, LR: 0.001, Batch Size: 64, Loss: 0.3319\n",
      "Epoch [18/50], Hidden Size: 64, LR: 0.001, Batch Size: 64, Loss: 0.3343\n",
      "Epoch [19/50], Hidden Size: 64, LR: 0.001, Batch Size: 64, Loss: 0.3343\n",
      "Epoch [20/50], Hidden Size: 64, LR: 0.001, Batch Size: 64, Loss: 0.3373\n",
      "Epoch [21/50], Hidden Size: 64, LR: 0.001, Batch Size: 64, Loss: 0.3307\n",
      "Epoch [22/50], Hidden Size: 64, LR: 0.001, Batch Size: 64, Loss: 0.3356\n",
      "Epoch [23/50], Hidden Size: 64, LR: 0.001, Batch Size: 64, Loss: 0.3364\n",
      "Epoch [24/50], Hidden Size: 64, LR: 0.001, Batch Size: 64, Loss: 0.3353\n",
      "Epoch [25/50], Hidden Size: 64, LR: 0.001, Batch Size: 64, Loss: 0.3393\n",
      "Epoch [26/50], Hidden Size: 64, LR: 0.001, Batch Size: 64, Loss: 0.3421\n",
      "Epoch [27/50], Hidden Size: 64, LR: 0.001, Batch Size: 64, Loss: 0.3317\n",
      "Epoch [28/50], Hidden Size: 64, LR: 0.001, Batch Size: 64, Loss: 0.3377\n",
      "Epoch [29/50], Hidden Size: 64, LR: 0.001, Batch Size: 64, Loss: 0.3365\n",
      "Epoch [30/50], Hidden Size: 64, LR: 0.001, Batch Size: 64, Loss: 0.3314\n",
      "Epoch [31/50], Hidden Size: 64, LR: 0.001, Batch Size: 64, Loss: 0.3426\n",
      "Epoch [32/50], Hidden Size: 64, LR: 0.001, Batch Size: 64, Loss: 0.3371\n",
      "Epoch [33/50], Hidden Size: 64, LR: 0.001, Batch Size: 64, Loss: 0.3385\n",
      "Epoch [34/50], Hidden Size: 64, LR: 0.001, Batch Size: 64, Loss: 0.3340\n",
      "Epoch [35/50], Hidden Size: 64, LR: 0.001, Batch Size: 64, Loss: 0.3360\n",
      "Epoch [36/50], Hidden Size: 64, LR: 0.001, Batch Size: 64, Loss: 0.3345\n",
      "Epoch [37/50], Hidden Size: 64, LR: 0.001, Batch Size: 64, Loss: 0.3432\n",
      "Epoch [38/50], Hidden Size: 64, LR: 0.001, Batch Size: 64, Loss: 0.3389\n",
      "Epoch [39/50], Hidden Size: 64, LR: 0.001, Batch Size: 64, Loss: 0.3313\n",
      "Epoch [40/50], Hidden Size: 64, LR: 0.001, Batch Size: 64, Loss: 0.3391\n",
      "Epoch [41/50], Hidden Size: 64, LR: 0.001, Batch Size: 64, Loss: 0.3344\n",
      "Epoch [42/50], Hidden Size: 64, LR: 0.001, Batch Size: 64, Loss: 0.3299\n",
      "Epoch [43/50], Hidden Size: 64, LR: 0.001, Batch Size: 64, Loss: 0.3291\n",
      "Epoch [44/50], Hidden Size: 64, LR: 0.001, Batch Size: 64, Loss: 0.3339\n",
      "Epoch [45/50], Hidden Size: 64, LR: 0.001, Batch Size: 64, Loss: 0.3356\n",
      "Epoch [46/50], Hidden Size: 64, LR: 0.001, Batch Size: 64, Loss: 0.3275\n",
      "Epoch [47/50], Hidden Size: 64, LR: 0.001, Batch Size: 64, Loss: 0.3325\n",
      "Epoch [48/50], Hidden Size: 64, LR: 0.001, Batch Size: 64, Loss: 0.3380\n",
      "Epoch [49/50], Hidden Size: 64, LR: 0.001, Batch Size: 64, Loss: 0.3347\n",
      "Epoch [50/50], Hidden Size: 64, LR: 0.001, Batch Size: 64, Loss: 0.3368\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\10374\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torchvision\\models\\_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "c:\\Users\\10374\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torchvision\\models\\_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=None`.\n",
      "  warnings.warn(msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/50], Hidden Size: 64, LR: 0.0001, Batch Size: 16, Loss: 0.9978\n",
      "Epoch [2/50], Hidden Size: 64, LR: 0.0001, Batch Size: 16, Loss: 0.9896\n",
      "Epoch [3/50], Hidden Size: 64, LR: 0.0001, Batch Size: 16, Loss: 0.9681\n",
      "Epoch [4/50], Hidden Size: 64, LR: 0.0001, Batch Size: 16, Loss: 0.9271\n",
      "Epoch [5/50], Hidden Size: 64, LR: 0.0001, Batch Size: 16, Loss: 0.8665\n",
      "Epoch [6/50], Hidden Size: 64, LR: 0.0001, Batch Size: 16, Loss: 0.7866\n",
      "Epoch [7/50], Hidden Size: 64, LR: 0.0001, Batch Size: 16, Loss: 0.6877\n",
      "Epoch [8/50], Hidden Size: 64, LR: 0.0001, Batch Size: 16, Loss: 0.5682\n",
      "Epoch [9/50], Hidden Size: 64, LR: 0.0001, Batch Size: 16, Loss: 0.4423\n",
      "Epoch [10/50], Hidden Size: 64, LR: 0.0001, Batch Size: 16, Loss: 0.3537\n",
      "Epoch [11/50], Hidden Size: 64, LR: 0.0001, Batch Size: 16, Loss: 0.3327\n",
      "Epoch [12/50], Hidden Size: 64, LR: 0.0001, Batch Size: 16, Loss: 0.3298\n",
      "Epoch [13/50], Hidden Size: 64, LR: 0.0001, Batch Size: 16, Loss: 0.3310\n",
      "Epoch [14/50], Hidden Size: 64, LR: 0.0001, Batch Size: 16, Loss: 0.3336\n",
      "Epoch [15/50], Hidden Size: 64, LR: 0.0001, Batch Size: 16, Loss: 0.3297\n",
      "Epoch [16/50], Hidden Size: 64, LR: 0.0001, Batch Size: 16, Loss: 0.3336\n",
      "Epoch [17/50], Hidden Size: 64, LR: 0.0001, Batch Size: 16, Loss: 0.3349\n",
      "Epoch [18/50], Hidden Size: 64, LR: 0.0001, Batch Size: 16, Loss: 0.3303\n",
      "Epoch [19/50], Hidden Size: 64, LR: 0.0001, Batch Size: 16, Loss: 0.3287\n",
      "Epoch [20/50], Hidden Size: 64, LR: 0.0001, Batch Size: 16, Loss: 0.3313\n",
      "Epoch [21/50], Hidden Size: 64, LR: 0.0001, Batch Size: 16, Loss: 0.3311\n",
      "Epoch [22/50], Hidden Size: 64, LR: 0.0001, Batch Size: 16, Loss: 0.3328\n",
      "Epoch [23/50], Hidden Size: 64, LR: 0.0001, Batch Size: 16, Loss: 0.3331\n",
      "Epoch [24/50], Hidden Size: 64, LR: 0.0001, Batch Size: 16, Loss: 0.3324\n",
      "Epoch [25/50], Hidden Size: 64, LR: 0.0001, Batch Size: 16, Loss: 0.3321\n",
      "Epoch [26/50], Hidden Size: 64, LR: 0.0001, Batch Size: 16, Loss: 0.3331\n",
      "Epoch [27/50], Hidden Size: 64, LR: 0.0001, Batch Size: 16, Loss: 0.3328\n",
      "Epoch [28/50], Hidden Size: 64, LR: 0.0001, Batch Size: 16, Loss: 0.3325\n",
      "Epoch [29/50], Hidden Size: 64, LR: 0.0001, Batch Size: 16, Loss: 0.3315\n",
      "Epoch [30/50], Hidden Size: 64, LR: 0.0001, Batch Size: 16, Loss: 0.3307\n",
      "Epoch [31/50], Hidden Size: 64, LR: 0.0001, Batch Size: 16, Loss: 0.3315\n",
      "Epoch [32/50], Hidden Size: 64, LR: 0.0001, Batch Size: 16, Loss: 0.3302\n",
      "Epoch [33/50], Hidden Size: 64, LR: 0.0001, Batch Size: 16, Loss: 0.3329\n",
      "Epoch [34/50], Hidden Size: 64, LR: 0.0001, Batch Size: 16, Loss: 0.3340\n",
      "Epoch [35/50], Hidden Size: 64, LR: 0.0001, Batch Size: 16, Loss: 0.3257\n",
      "Epoch [36/50], Hidden Size: 64, LR: 0.0001, Batch Size: 16, Loss: 0.3348\n",
      "Epoch [37/50], Hidden Size: 64, LR: 0.0001, Batch Size: 16, Loss: 0.3328\n",
      "Epoch [38/50], Hidden Size: 64, LR: 0.0001, Batch Size: 16, Loss: 0.3296\n",
      "Epoch [39/50], Hidden Size: 64, LR: 0.0001, Batch Size: 16, Loss: 0.3316\n",
      "Epoch [40/50], Hidden Size: 64, LR: 0.0001, Batch Size: 16, Loss: 0.3297\n",
      "Epoch [41/50], Hidden Size: 64, LR: 0.0001, Batch Size: 16, Loss: 0.3323\n",
      "Epoch [42/50], Hidden Size: 64, LR: 0.0001, Batch Size: 16, Loss: 0.3325\n",
      "Epoch [43/50], Hidden Size: 64, LR: 0.0001, Batch Size: 16, Loss: 0.3319\n",
      "Epoch [44/50], Hidden Size: 64, LR: 0.0001, Batch Size: 16, Loss: 0.3343\n",
      "Epoch [45/50], Hidden Size: 64, LR: 0.0001, Batch Size: 16, Loss: 0.3305\n",
      "Epoch [46/50], Hidden Size: 64, LR: 0.0001, Batch Size: 16, Loss: 0.3311\n",
      "Epoch [47/50], Hidden Size: 64, LR: 0.0001, Batch Size: 16, Loss: 0.3323\n",
      "Epoch [48/50], Hidden Size: 64, LR: 0.0001, Batch Size: 16, Loss: 0.3336\n",
      "Epoch [49/50], Hidden Size: 64, LR: 0.0001, Batch Size: 16, Loss: 0.3300\n",
      "Epoch [50/50], Hidden Size: 64, LR: 0.0001, Batch Size: 16, Loss: 0.3343\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\10374\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torchvision\\models\\_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "c:\\Users\\10374\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torchvision\\models\\_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=None`.\n",
      "  warnings.warn(msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/50], Hidden Size: 64, LR: 0.0001, Batch Size: 32, Loss: 0.9992\n",
      "Epoch [2/50], Hidden Size: 64, LR: 0.0001, Batch Size: 32, Loss: 0.9967\n",
      "Epoch [3/50], Hidden Size: 64, LR: 0.0001, Batch Size: 32, Loss: 0.9928\n",
      "Epoch [4/50], Hidden Size: 64, LR: 0.0001, Batch Size: 32, Loss: 0.9862\n",
      "Epoch [5/50], Hidden Size: 64, LR: 0.0001, Batch Size: 32, Loss: 0.9756\n",
      "Epoch [6/50], Hidden Size: 64, LR: 0.0001, Batch Size: 32, Loss: 0.9605\n",
      "Epoch [7/50], Hidden Size: 64, LR: 0.0001, Batch Size: 32, Loss: 0.9402\n",
      "Epoch [8/50], Hidden Size: 64, LR: 0.0001, Batch Size: 32, Loss: 0.9162\n",
      "Epoch [9/50], Hidden Size: 64, LR: 0.0001, Batch Size: 32, Loss: 0.8876\n",
      "Epoch [10/50], Hidden Size: 64, LR: 0.0001, Batch Size: 32, Loss: 0.8526\n",
      "Epoch [11/50], Hidden Size: 64, LR: 0.0001, Batch Size: 32, Loss: 0.8131\n",
      "Epoch [12/50], Hidden Size: 64, LR: 0.0001, Batch Size: 32, Loss: 0.7685\n",
      "Epoch [13/50], Hidden Size: 64, LR: 0.0001, Batch Size: 32, Loss: 0.7175\n",
      "Epoch [14/50], Hidden Size: 64, LR: 0.0001, Batch Size: 32, Loss: 0.6610\n",
      "Epoch [15/50], Hidden Size: 64, LR: 0.0001, Batch Size: 32, Loss: 0.5992\n",
      "Epoch [16/50], Hidden Size: 64, LR: 0.0001, Batch Size: 32, Loss: 0.5352\n",
      "Epoch [17/50], Hidden Size: 64, LR: 0.0001, Batch Size: 32, Loss: 0.4710\n",
      "Epoch [18/50], Hidden Size: 64, LR: 0.0001, Batch Size: 32, Loss: 0.4102\n",
      "Epoch [19/50], Hidden Size: 64, LR: 0.0001, Batch Size: 32, Loss: 0.3676\n",
      "Epoch [20/50], Hidden Size: 64, LR: 0.0001, Batch Size: 32, Loss: 0.3433\n",
      "Epoch [21/50], Hidden Size: 64, LR: 0.0001, Batch Size: 32, Loss: 0.3328\n",
      "Epoch [22/50], Hidden Size: 64, LR: 0.0001, Batch Size: 32, Loss: 0.3336\n",
      "Epoch [23/50], Hidden Size: 64, LR: 0.0001, Batch Size: 32, Loss: 0.3351\n",
      "Epoch [24/50], Hidden Size: 64, LR: 0.0001, Batch Size: 32, Loss: 0.3331\n",
      "Epoch [25/50], Hidden Size: 64, LR: 0.0001, Batch Size: 32, Loss: 0.3344\n",
      "Epoch [26/50], Hidden Size: 64, LR: 0.0001, Batch Size: 32, Loss: 0.3331\n",
      "Epoch [27/50], Hidden Size: 64, LR: 0.0001, Batch Size: 32, Loss: 0.3327\n",
      "Epoch [28/50], Hidden Size: 64, LR: 0.0001, Batch Size: 32, Loss: 0.3339\n",
      "Epoch [29/50], Hidden Size: 64, LR: 0.0001, Batch Size: 32, Loss: 0.3323\n",
      "Epoch [30/50], Hidden Size: 64, LR: 0.0001, Batch Size: 32, Loss: 0.3347\n",
      "Epoch [31/50], Hidden Size: 64, LR: 0.0001, Batch Size: 32, Loss: 0.3332\n",
      "Epoch [32/50], Hidden Size: 64, LR: 0.0001, Batch Size: 32, Loss: 0.3309\n",
      "Epoch [33/50], Hidden Size: 64, LR: 0.0001, Batch Size: 32, Loss: 0.3331\n",
      "Epoch [34/50], Hidden Size: 64, LR: 0.0001, Batch Size: 32, Loss: 0.3339\n",
      "Epoch [35/50], Hidden Size: 64, LR: 0.0001, Batch Size: 32, Loss: 0.3323\n",
      "Epoch [36/50], Hidden Size: 64, LR: 0.0001, Batch Size: 32, Loss: 0.3344\n",
      "Epoch [37/50], Hidden Size: 64, LR: 0.0001, Batch Size: 32, Loss: 0.3327\n",
      "Epoch [38/50], Hidden Size: 64, LR: 0.0001, Batch Size: 32, Loss: 0.3333\n",
      "Epoch [39/50], Hidden Size: 64, LR: 0.0001, Batch Size: 32, Loss: 0.3337\n",
      "Epoch [40/50], Hidden Size: 64, LR: 0.0001, Batch Size: 32, Loss: 0.3340\n",
      "Epoch [41/50], Hidden Size: 64, LR: 0.0001, Batch Size: 32, Loss: 0.3339\n",
      "Epoch [42/50], Hidden Size: 64, LR: 0.0001, Batch Size: 32, Loss: 0.3326\n",
      "Epoch [43/50], Hidden Size: 64, LR: 0.0001, Batch Size: 32, Loss: 0.3331\n",
      "Epoch [44/50], Hidden Size: 64, LR: 0.0001, Batch Size: 32, Loss: 0.3322\n",
      "Epoch [45/50], Hidden Size: 64, LR: 0.0001, Batch Size: 32, Loss: 0.3344\n",
      "Epoch [46/50], Hidden Size: 64, LR: 0.0001, Batch Size: 32, Loss: 0.3335\n",
      "Epoch [47/50], Hidden Size: 64, LR: 0.0001, Batch Size: 32, Loss: 0.3321\n",
      "Epoch [48/50], Hidden Size: 64, LR: 0.0001, Batch Size: 32, Loss: 0.3330\n",
      "Epoch [49/50], Hidden Size: 64, LR: 0.0001, Batch Size: 32, Loss: 0.3330\n",
      "Epoch [50/50], Hidden Size: 64, LR: 0.0001, Batch Size: 32, Loss: 0.3340\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\10374\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torchvision\\models\\_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "c:\\Users\\10374\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torchvision\\models\\_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=None`.\n",
      "  warnings.warn(msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/50], Hidden Size: 64, LR: 0.0001, Batch Size: 64, Loss: 0.9996\n",
      "Epoch [2/50], Hidden Size: 64, LR: 0.0001, Batch Size: 64, Loss: 0.9980\n",
      "Epoch [3/50], Hidden Size: 64, LR: 0.0001, Batch Size: 64, Loss: 0.9958\n",
      "Epoch [4/50], Hidden Size: 64, LR: 0.0001, Batch Size: 64, Loss: 0.9932\n",
      "Epoch [5/50], Hidden Size: 64, LR: 0.0001, Batch Size: 64, Loss: 0.9898\n",
      "Epoch [6/50], Hidden Size: 64, LR: 0.0001, Batch Size: 64, Loss: 0.9851\n",
      "Epoch [7/50], Hidden Size: 64, LR: 0.0001, Batch Size: 64, Loss: 0.9785\n",
      "Epoch [8/50], Hidden Size: 64, LR: 0.0001, Batch Size: 64, Loss: 0.9711\n",
      "Epoch [9/50], Hidden Size: 64, LR: 0.0001, Batch Size: 64, Loss: 0.9623\n",
      "Epoch [10/50], Hidden Size: 64, LR: 0.0001, Batch Size: 64, Loss: 0.9512\n",
      "Epoch [11/50], Hidden Size: 64, LR: 0.0001, Batch Size: 64, Loss: 0.9400\n",
      "Epoch [12/50], Hidden Size: 64, LR: 0.0001, Batch Size: 64, Loss: 0.9280\n",
      "Epoch [13/50], Hidden Size: 64, LR: 0.0001, Batch Size: 64, Loss: 0.9106\n",
      "Epoch [14/50], Hidden Size: 64, LR: 0.0001, Batch Size: 64, Loss: 0.8961\n",
      "Epoch [15/50], Hidden Size: 64, LR: 0.0001, Batch Size: 64, Loss: 0.8781\n",
      "Epoch [16/50], Hidden Size: 64, LR: 0.0001, Batch Size: 64, Loss: 0.8566\n",
      "Epoch [17/50], Hidden Size: 64, LR: 0.0001, Batch Size: 64, Loss: 0.8349\n",
      "Epoch [18/50], Hidden Size: 64, LR: 0.0001, Batch Size: 64, Loss: 0.8127\n",
      "Epoch [19/50], Hidden Size: 64, LR: 0.0001, Batch Size: 64, Loss: 0.7849\n",
      "Epoch [20/50], Hidden Size: 64, LR: 0.0001, Batch Size: 64, Loss: 0.7577\n",
      "Epoch [21/50], Hidden Size: 64, LR: 0.0001, Batch Size: 64, Loss: 0.7320\n",
      "Epoch [22/50], Hidden Size: 64, LR: 0.0001, Batch Size: 64, Loss: 0.7000\n",
      "Epoch [23/50], Hidden Size: 64, LR: 0.0001, Batch Size: 64, Loss: 0.6645\n",
      "Epoch [24/50], Hidden Size: 64, LR: 0.0001, Batch Size: 64, Loss: 0.6247\n",
      "Epoch [25/50], Hidden Size: 64, LR: 0.0001, Batch Size: 64, Loss: 0.5955\n",
      "Epoch [26/50], Hidden Size: 64, LR: 0.0001, Batch Size: 64, Loss: 0.5590\n",
      "Epoch [27/50], Hidden Size: 64, LR: 0.0001, Batch Size: 64, Loss: 0.5207\n",
      "Epoch [28/50], Hidden Size: 64, LR: 0.0001, Batch Size: 64, Loss: 0.4813\n",
      "Epoch [29/50], Hidden Size: 64, LR: 0.0001, Batch Size: 64, Loss: 0.4499\n",
      "Epoch [30/50], Hidden Size: 64, LR: 0.0001, Batch Size: 64, Loss: 0.4116\n",
      "Epoch [31/50], Hidden Size: 64, LR: 0.0001, Batch Size: 64, Loss: 0.3807\n",
      "Epoch [32/50], Hidden Size: 64, LR: 0.0001, Batch Size: 64, Loss: 0.3672\n",
      "Epoch [33/50], Hidden Size: 64, LR: 0.0001, Batch Size: 64, Loss: 0.3517\n",
      "Epoch [34/50], Hidden Size: 64, LR: 0.0001, Batch Size: 64, Loss: 0.3392\n",
      "Epoch [35/50], Hidden Size: 64, LR: 0.0001, Batch Size: 64, Loss: 0.3294\n",
      "Epoch [36/50], Hidden Size: 64, LR: 0.0001, Batch Size: 64, Loss: 0.3333\n",
      "Epoch [37/50], Hidden Size: 64, LR: 0.0001, Batch Size: 64, Loss: 0.3350\n",
      "Epoch [38/50], Hidden Size: 64, LR: 0.0001, Batch Size: 64, Loss: 0.3315\n",
      "Epoch [39/50], Hidden Size: 64, LR: 0.0001, Batch Size: 64, Loss: 0.3314\n",
      "Epoch [40/50], Hidden Size: 64, LR: 0.0001, Batch Size: 64, Loss: 0.3359\n",
      "Epoch [41/50], Hidden Size: 64, LR: 0.0001, Batch Size: 64, Loss: 0.3345\n",
      "Epoch [42/50], Hidden Size: 64, LR: 0.0001, Batch Size: 64, Loss: 0.3351\n",
      "Epoch [43/50], Hidden Size: 64, LR: 0.0001, Batch Size: 64, Loss: 0.3293\n",
      "Epoch [44/50], Hidden Size: 64, LR: 0.0001, Batch Size: 64, Loss: 0.3314\n",
      "Epoch [45/50], Hidden Size: 64, LR: 0.0001, Batch Size: 64, Loss: 0.3234\n",
      "Epoch [46/50], Hidden Size: 64, LR: 0.0001, Batch Size: 64, Loss: 0.3234\n",
      "Epoch [47/50], Hidden Size: 64, LR: 0.0001, Batch Size: 64, Loss: 0.3283\n",
      "Epoch [48/50], Hidden Size: 64, LR: 0.0001, Batch Size: 64, Loss: 0.3432\n",
      "Epoch [49/50], Hidden Size: 64, LR: 0.0001, Batch Size: 64, Loss: 0.3257\n",
      "Epoch [50/50], Hidden Size: 64, LR: 0.0001, Batch Size: 64, Loss: 0.3286\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\10374\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torchvision\\models\\_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "c:\\Users\\10374\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torchvision\\models\\_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=None`.\n",
      "  warnings.warn(msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/50], Hidden Size: 128, LR: 0.001, Batch Size: 16, Loss: 0.9020\n",
      "Epoch [2/50], Hidden Size: 128, LR: 0.001, Batch Size: 16, Loss: 0.4277\n",
      "Epoch [3/50], Hidden Size: 128, LR: 0.001, Batch Size: 16, Loss: 0.3361\n",
      "Epoch [4/50], Hidden Size: 128, LR: 0.001, Batch Size: 16, Loss: 0.3342\n",
      "Epoch [5/50], Hidden Size: 128, LR: 0.001, Batch Size: 16, Loss: 0.3357\n",
      "Epoch [6/50], Hidden Size: 128, LR: 0.001, Batch Size: 16, Loss: 0.3313\n",
      "Epoch [7/50], Hidden Size: 128, LR: 0.001, Batch Size: 16, Loss: 0.3368\n",
      "Epoch [8/50], Hidden Size: 128, LR: 0.001, Batch Size: 16, Loss: 0.3339\n",
      "Epoch [9/50], Hidden Size: 128, LR: 0.001, Batch Size: 16, Loss: 0.3381\n",
      "Epoch [10/50], Hidden Size: 128, LR: 0.001, Batch Size: 16, Loss: 0.3337\n",
      "Epoch [11/50], Hidden Size: 128, LR: 0.001, Batch Size: 16, Loss: 0.3332\n",
      "Epoch [12/50], Hidden Size: 128, LR: 0.001, Batch Size: 16, Loss: 0.3312\n",
      "Epoch [13/50], Hidden Size: 128, LR: 0.001, Batch Size: 16, Loss: 0.3345\n",
      "Epoch [14/50], Hidden Size: 128, LR: 0.001, Batch Size: 16, Loss: 0.3362\n",
      "Epoch [15/50], Hidden Size: 128, LR: 0.001, Batch Size: 16, Loss: 0.3309\n",
      "Epoch [16/50], Hidden Size: 128, LR: 0.001, Batch Size: 16, Loss: 0.3346\n",
      "Epoch [17/50], Hidden Size: 128, LR: 0.001, Batch Size: 16, Loss: 0.3374\n",
      "Epoch [18/50], Hidden Size: 128, LR: 0.001, Batch Size: 16, Loss: 0.3473\n",
      "Epoch [19/50], Hidden Size: 128, LR: 0.001, Batch Size: 16, Loss: 0.3323\n",
      "Epoch [20/50], Hidden Size: 128, LR: 0.001, Batch Size: 16, Loss: 0.3331\n",
      "Epoch [21/50], Hidden Size: 128, LR: 0.001, Batch Size: 16, Loss: 0.3311\n",
      "Epoch [22/50], Hidden Size: 128, LR: 0.001, Batch Size: 16, Loss: 0.3398\n",
      "Epoch [23/50], Hidden Size: 128, LR: 0.001, Batch Size: 16, Loss: 0.3355\n",
      "Epoch [24/50], Hidden Size: 128, LR: 0.001, Batch Size: 16, Loss: 0.3346\n",
      "Epoch [25/50], Hidden Size: 128, LR: 0.001, Batch Size: 16, Loss: 0.3353\n",
      "Epoch [26/50], Hidden Size: 128, LR: 0.001, Batch Size: 16, Loss: 0.3360\n",
      "Epoch [27/50], Hidden Size: 128, LR: 0.001, Batch Size: 16, Loss: 0.3403\n",
      "Epoch [28/50], Hidden Size: 128, LR: 0.001, Batch Size: 16, Loss: 0.3331\n",
      "Epoch [29/50], Hidden Size: 128, LR: 0.001, Batch Size: 16, Loss: 0.3338\n",
      "Epoch [30/50], Hidden Size: 128, LR: 0.001, Batch Size: 16, Loss: 0.3342\n",
      "Epoch [31/50], Hidden Size: 128, LR: 0.001, Batch Size: 16, Loss: 0.3339\n",
      "Epoch [32/50], Hidden Size: 128, LR: 0.001, Batch Size: 16, Loss: 0.3389\n",
      "Epoch [33/50], Hidden Size: 128, LR: 0.001, Batch Size: 16, Loss: 0.3407\n",
      "Epoch [34/50], Hidden Size: 128, LR: 0.001, Batch Size: 16, Loss: 0.3351\n",
      "Epoch [35/50], Hidden Size: 128, LR: 0.001, Batch Size: 16, Loss: 0.3317\n",
      "Epoch [36/50], Hidden Size: 128, LR: 0.001, Batch Size: 16, Loss: 0.3334\n",
      "Epoch [37/50], Hidden Size: 128, LR: 0.001, Batch Size: 16, Loss: 0.3336\n",
      "Epoch [38/50], Hidden Size: 128, LR: 0.001, Batch Size: 16, Loss: 0.3350\n",
      "Epoch [39/50], Hidden Size: 128, LR: 0.001, Batch Size: 16, Loss: 0.3407\n",
      "Epoch [40/50], Hidden Size: 128, LR: 0.001, Batch Size: 16, Loss: 0.3400\n",
      "Epoch [41/50], Hidden Size: 128, LR: 0.001, Batch Size: 16, Loss: 0.3339\n",
      "Epoch [42/50], Hidden Size: 128, LR: 0.001, Batch Size: 16, Loss: 0.3351\n",
      "Epoch [43/50], Hidden Size: 128, LR: 0.001, Batch Size: 16, Loss: 0.3328\n",
      "Epoch [44/50], Hidden Size: 128, LR: 0.001, Batch Size: 16, Loss: 0.3332\n",
      "Epoch [45/50], Hidden Size: 128, LR: 0.001, Batch Size: 16, Loss: 0.3346\n",
      "Epoch [46/50], Hidden Size: 128, LR: 0.001, Batch Size: 16, Loss: 0.3333\n",
      "Epoch [47/50], Hidden Size: 128, LR: 0.001, Batch Size: 16, Loss: 0.3342\n",
      "Epoch [48/50], Hidden Size: 128, LR: 0.001, Batch Size: 16, Loss: 0.3329\n",
      "Epoch [49/50], Hidden Size: 128, LR: 0.001, Batch Size: 16, Loss: 0.3407\n",
      "Epoch [50/50], Hidden Size: 128, LR: 0.001, Batch Size: 16, Loss: 0.3348\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\10374\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torchvision\\models\\_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "c:\\Users\\10374\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torchvision\\models\\_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=None`.\n",
      "  warnings.warn(msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/50], Hidden Size: 128, LR: 0.001, Batch Size: 32, Loss: 0.9773\n",
      "Epoch [2/50], Hidden Size: 128, LR: 0.001, Batch Size: 32, Loss: 0.8262\n",
      "Epoch [3/50], Hidden Size: 128, LR: 0.001, Batch Size: 32, Loss: 0.4931\n",
      "Epoch [4/50], Hidden Size: 128, LR: 0.001, Batch Size: 32, Loss: 0.3642\n",
      "Epoch [5/50], Hidden Size: 128, LR: 0.001, Batch Size: 32, Loss: 0.3356\n",
      "Epoch [6/50], Hidden Size: 128, LR: 0.001, Batch Size: 32, Loss: 0.3353\n",
      "Epoch [7/50], Hidden Size: 128, LR: 0.001, Batch Size: 32, Loss: 0.3344\n",
      "Epoch [8/50], Hidden Size: 128, LR: 0.001, Batch Size: 32, Loss: 0.3327\n",
      "Epoch [9/50], Hidden Size: 128, LR: 0.001, Batch Size: 32, Loss: 0.3354\n",
      "Epoch [10/50], Hidden Size: 128, LR: 0.001, Batch Size: 32, Loss: 0.3326\n",
      "Epoch [11/50], Hidden Size: 128, LR: 0.001, Batch Size: 32, Loss: 0.3349\n",
      "Epoch [12/50], Hidden Size: 128, LR: 0.001, Batch Size: 32, Loss: 0.3347\n",
      "Epoch [13/50], Hidden Size: 128, LR: 0.001, Batch Size: 32, Loss: 0.3363\n",
      "Epoch [14/50], Hidden Size: 128, LR: 0.001, Batch Size: 32, Loss: 0.3354\n",
      "Epoch [15/50], Hidden Size: 128, LR: 0.001, Batch Size: 32, Loss: 0.3346\n",
      "Epoch [16/50], Hidden Size: 128, LR: 0.001, Batch Size: 32, Loss: 0.3339\n",
      "Epoch [17/50], Hidden Size: 128, LR: 0.001, Batch Size: 32, Loss: 0.3345\n",
      "Epoch [18/50], Hidden Size: 128, LR: 0.001, Batch Size: 32, Loss: 0.3351\n",
      "Epoch [19/50], Hidden Size: 128, LR: 0.001, Batch Size: 32, Loss: 0.3341\n",
      "Epoch [20/50], Hidden Size: 128, LR: 0.001, Batch Size: 32, Loss: 0.3314\n",
      "Epoch [21/50], Hidden Size: 128, LR: 0.001, Batch Size: 32, Loss: 0.3343\n",
      "Epoch [22/50], Hidden Size: 128, LR: 0.001, Batch Size: 32, Loss: 0.3344\n",
      "Epoch [23/50], Hidden Size: 128, LR: 0.001, Batch Size: 32, Loss: 0.3346\n",
      "Epoch [24/50], Hidden Size: 128, LR: 0.001, Batch Size: 32, Loss: 0.3348\n",
      "Epoch [25/50], Hidden Size: 128, LR: 0.001, Batch Size: 32, Loss: 0.3319\n",
      "Epoch [26/50], Hidden Size: 128, LR: 0.001, Batch Size: 32, Loss: 0.3344\n",
      "Epoch [27/50], Hidden Size: 128, LR: 0.001, Batch Size: 32, Loss: 0.3326\n",
      "Epoch [28/50], Hidden Size: 128, LR: 0.001, Batch Size: 32, Loss: 0.3352\n",
      "Epoch [29/50], Hidden Size: 128, LR: 0.001, Batch Size: 32, Loss: 0.3357\n",
      "Epoch [30/50], Hidden Size: 128, LR: 0.001, Batch Size: 32, Loss: 0.3378\n",
      "Epoch [31/50], Hidden Size: 128, LR: 0.001, Batch Size: 32, Loss: 0.3326\n",
      "Epoch [32/50], Hidden Size: 128, LR: 0.001, Batch Size: 32, Loss: 0.3374\n",
      "Epoch [33/50], Hidden Size: 128, LR: 0.001, Batch Size: 32, Loss: 0.3359\n",
      "Epoch [34/50], Hidden Size: 128, LR: 0.001, Batch Size: 32, Loss: 0.3356\n",
      "Epoch [35/50], Hidden Size: 128, LR: 0.001, Batch Size: 32, Loss: 0.3348\n",
      "Epoch [36/50], Hidden Size: 128, LR: 0.001, Batch Size: 32, Loss: 0.3342\n",
      "Epoch [37/50], Hidden Size: 128, LR: 0.001, Batch Size: 32, Loss: 0.3348\n",
      "Epoch [38/50], Hidden Size: 128, LR: 0.001, Batch Size: 32, Loss: 0.3346\n",
      "Epoch [39/50], Hidden Size: 128, LR: 0.001, Batch Size: 32, Loss: 0.3344\n",
      "Epoch [40/50], Hidden Size: 128, LR: 0.001, Batch Size: 32, Loss: 0.3350\n",
      "Epoch [41/50], Hidden Size: 128, LR: 0.001, Batch Size: 32, Loss: 0.3353\n",
      "Epoch [42/50], Hidden Size: 128, LR: 0.001, Batch Size: 32, Loss: 0.3368\n",
      "Epoch [43/50], Hidden Size: 128, LR: 0.001, Batch Size: 32, Loss: 0.3354\n",
      "Epoch [44/50], Hidden Size: 128, LR: 0.001, Batch Size: 32, Loss: 0.3382\n",
      "Epoch [45/50], Hidden Size: 128, LR: 0.001, Batch Size: 32, Loss: 0.3355\n",
      "Epoch [46/50], Hidden Size: 128, LR: 0.001, Batch Size: 32, Loss: 0.3353\n",
      "Epoch [47/50], Hidden Size: 128, LR: 0.001, Batch Size: 32, Loss: 0.3355\n",
      "Epoch [48/50], Hidden Size: 128, LR: 0.001, Batch Size: 32, Loss: 0.3344\n",
      "Epoch [49/50], Hidden Size: 128, LR: 0.001, Batch Size: 32, Loss: 0.3344\n",
      "Epoch [50/50], Hidden Size: 128, LR: 0.001, Batch Size: 32, Loss: 0.3345\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\10374\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torchvision\\models\\_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "c:\\Users\\10374\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torchvision\\models\\_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=None`.\n",
      "  warnings.warn(msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/50], Hidden Size: 128, LR: 0.001, Batch Size: 64, Loss: 0.9936\n",
      "Epoch [2/50], Hidden Size: 128, LR: 0.001, Batch Size: 64, Loss: 0.9478\n",
      "Epoch [3/50], Hidden Size: 128, LR: 0.001, Batch Size: 64, Loss: 0.8378\n",
      "Epoch [4/50], Hidden Size: 128, LR: 0.001, Batch Size: 64, Loss: 0.6622\n",
      "Epoch [5/50], Hidden Size: 128, LR: 0.001, Batch Size: 64, Loss: 0.4256\n",
      "Epoch [6/50], Hidden Size: 128, LR: 0.001, Batch Size: 64, Loss: 0.3641\n",
      "Epoch [7/50], Hidden Size: 128, LR: 0.001, Batch Size: 64, Loss: 0.3702\n",
      "Epoch [8/50], Hidden Size: 128, LR: 0.001, Batch Size: 64, Loss: 0.3351\n",
      "Epoch [9/50], Hidden Size: 128, LR: 0.001, Batch Size: 64, Loss: 0.3292\n",
      "Epoch [10/50], Hidden Size: 128, LR: 0.001, Batch Size: 64, Loss: 0.3294\n",
      "Epoch [11/50], Hidden Size: 128, LR: 0.001, Batch Size: 64, Loss: 0.3384\n",
      "Epoch [12/50], Hidden Size: 128, LR: 0.001, Batch Size: 64, Loss: 0.3389\n",
      "Epoch [13/50], Hidden Size: 128, LR: 0.001, Batch Size: 64, Loss: 0.3397\n",
      "Epoch [14/50], Hidden Size: 128, LR: 0.001, Batch Size: 64, Loss: 0.3291\n",
      "Epoch [15/50], Hidden Size: 128, LR: 0.001, Batch Size: 64, Loss: 0.3260\n",
      "Epoch [16/50], Hidden Size: 128, LR: 0.001, Batch Size: 64, Loss: 0.3459\n",
      "Epoch [17/50], Hidden Size: 128, LR: 0.001, Batch Size: 64, Loss: 0.3365\n",
      "Epoch [18/50], Hidden Size: 128, LR: 0.001, Batch Size: 64, Loss: 0.3346\n",
      "Epoch [19/50], Hidden Size: 128, LR: 0.001, Batch Size: 64, Loss: 0.3297\n",
      "Epoch [20/50], Hidden Size: 128, LR: 0.001, Batch Size: 64, Loss: 0.3372\n",
      "Epoch [21/50], Hidden Size: 128, LR: 0.001, Batch Size: 64, Loss: 0.3314\n",
      "Epoch [22/50], Hidden Size: 128, LR: 0.001, Batch Size: 64, Loss: 0.3240\n",
      "Epoch [23/50], Hidden Size: 128, LR: 0.001, Batch Size: 64, Loss: 0.3376\n",
      "Epoch [24/50], Hidden Size: 128, LR: 0.001, Batch Size: 64, Loss: 0.3357\n",
      "Epoch [25/50], Hidden Size: 128, LR: 0.001, Batch Size: 64, Loss: 0.3322\n",
      "Epoch [26/50], Hidden Size: 128, LR: 0.001, Batch Size: 64, Loss: 0.3352\n",
      "Epoch [27/50], Hidden Size: 128, LR: 0.001, Batch Size: 64, Loss: 0.3333\n",
      "Epoch [28/50], Hidden Size: 128, LR: 0.001, Batch Size: 64, Loss: 0.3372\n",
      "Epoch [29/50], Hidden Size: 128, LR: 0.001, Batch Size: 64, Loss: 0.3317\n",
      "Epoch [30/50], Hidden Size: 128, LR: 0.001, Batch Size: 64, Loss: 0.3393\n",
      "Epoch [31/50], Hidden Size: 128, LR: 0.001, Batch Size: 64, Loss: 0.3386\n",
      "Epoch [32/50], Hidden Size: 128, LR: 0.001, Batch Size: 64, Loss: 0.3397\n",
      "Epoch [33/50], Hidden Size: 128, LR: 0.001, Batch Size: 64, Loss: 0.3334\n",
      "Epoch [34/50], Hidden Size: 128, LR: 0.001, Batch Size: 64, Loss: 0.3403\n",
      "Epoch [35/50], Hidden Size: 128, LR: 0.001, Batch Size: 64, Loss: 0.3363\n",
      "Epoch [36/50], Hidden Size: 128, LR: 0.001, Batch Size: 64, Loss: 0.3363\n",
      "Epoch [37/50], Hidden Size: 128, LR: 0.001, Batch Size: 64, Loss: 0.3352\n",
      "Epoch [38/50], Hidden Size: 128, LR: 0.001, Batch Size: 64, Loss: 0.3371\n",
      "Epoch [39/50], Hidden Size: 128, LR: 0.001, Batch Size: 64, Loss: 0.3306\n",
      "Epoch [40/50], Hidden Size: 128, LR: 0.001, Batch Size: 64, Loss: 0.3371\n",
      "Epoch [41/50], Hidden Size: 128, LR: 0.001, Batch Size: 64, Loss: 0.3336\n",
      "Epoch [42/50], Hidden Size: 128, LR: 0.001, Batch Size: 64, Loss: 0.3432\n",
      "Epoch [43/50], Hidden Size: 128, LR: 0.001, Batch Size: 64, Loss: 0.3259\n",
      "Epoch [44/50], Hidden Size: 128, LR: 0.001, Batch Size: 64, Loss: 0.3282\n",
      "Epoch [45/50], Hidden Size: 128, LR: 0.001, Batch Size: 64, Loss: 0.3354\n",
      "Epoch [46/50], Hidden Size: 128, LR: 0.001, Batch Size: 64, Loss: 0.3344\n",
      "Epoch [47/50], Hidden Size: 128, LR: 0.001, Batch Size: 64, Loss: 0.3339\n",
      "Epoch [48/50], Hidden Size: 128, LR: 0.001, Batch Size: 64, Loss: 0.3426\n",
      "Epoch [49/50], Hidden Size: 128, LR: 0.001, Batch Size: 64, Loss: 0.3326\n",
      "Epoch [50/50], Hidden Size: 128, LR: 0.001, Batch Size: 64, Loss: 0.3326\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\10374\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torchvision\\models\\_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "c:\\Users\\10374\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torchvision\\models\\_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=None`.\n",
      "  warnings.warn(msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/50], Hidden Size: 128, LR: 0.0001, Batch Size: 16, Loss: 0.9967\n",
      "Epoch [2/50], Hidden Size: 128, LR: 0.0001, Batch Size: 16, Loss: 0.9855\n",
      "Epoch [3/50], Hidden Size: 128, LR: 0.0001, Batch Size: 16, Loss: 0.9581\n",
      "Epoch [4/50], Hidden Size: 128, LR: 0.0001, Batch Size: 16, Loss: 0.9084\n",
      "Epoch [5/50], Hidden Size: 128, LR: 0.0001, Batch Size: 16, Loss: 0.8368\n",
      "Epoch [6/50], Hidden Size: 128, LR: 0.0001, Batch Size: 16, Loss: 0.7393\n",
      "Epoch [7/50], Hidden Size: 128, LR: 0.0001, Batch Size: 16, Loss: 0.6215\n",
      "Epoch [8/50], Hidden Size: 128, LR: 0.0001, Batch Size: 16, Loss: 0.4811\n",
      "Epoch [9/50], Hidden Size: 128, LR: 0.0001, Batch Size: 16, Loss: 0.3732\n",
      "Epoch [10/50], Hidden Size: 128, LR: 0.0001, Batch Size: 16, Loss: 0.3346\n",
      "Epoch [11/50], Hidden Size: 128, LR: 0.0001, Batch Size: 16, Loss: 0.3321\n",
      "Epoch [12/50], Hidden Size: 128, LR: 0.0001, Batch Size: 16, Loss: 0.3325\n",
      "Epoch [13/50], Hidden Size: 128, LR: 0.0001, Batch Size: 16, Loss: 0.3355\n",
      "Epoch [14/50], Hidden Size: 128, LR: 0.0001, Batch Size: 16, Loss: 0.3304\n",
      "Epoch [15/50], Hidden Size: 128, LR: 0.0001, Batch Size: 16, Loss: 0.3334\n",
      "Epoch [16/50], Hidden Size: 128, LR: 0.0001, Batch Size: 16, Loss: 0.3305\n",
      "Epoch [17/50], Hidden Size: 128, LR: 0.0001, Batch Size: 16, Loss: 0.3318\n",
      "Epoch [18/50], Hidden Size: 128, LR: 0.0001, Batch Size: 16, Loss: 0.3292\n",
      "Epoch [19/50], Hidden Size: 128, LR: 0.0001, Batch Size: 16, Loss: 0.3303\n",
      "Epoch [20/50], Hidden Size: 128, LR: 0.0001, Batch Size: 16, Loss: 0.3335\n",
      "Epoch [21/50], Hidden Size: 128, LR: 0.0001, Batch Size: 16, Loss: 0.3307\n",
      "Epoch [22/50], Hidden Size: 128, LR: 0.0001, Batch Size: 16, Loss: 0.3335\n",
      "Epoch [23/50], Hidden Size: 128, LR: 0.0001, Batch Size: 16, Loss: 0.3317\n",
      "Epoch [24/50], Hidden Size: 128, LR: 0.0001, Batch Size: 16, Loss: 0.3317\n",
      "Epoch [25/50], Hidden Size: 128, LR: 0.0001, Batch Size: 16, Loss: 0.3333\n",
      "Epoch [26/50], Hidden Size: 128, LR: 0.0001, Batch Size: 16, Loss: 0.3307\n",
      "Epoch [27/50], Hidden Size: 128, LR: 0.0001, Batch Size: 16, Loss: 0.3314\n",
      "Epoch [28/50], Hidden Size: 128, LR: 0.0001, Batch Size: 16, Loss: 0.3295\n",
      "Epoch [29/50], Hidden Size: 128, LR: 0.0001, Batch Size: 16, Loss: 0.3333\n",
      "Epoch [30/50], Hidden Size: 128, LR: 0.0001, Batch Size: 16, Loss: 0.3302\n",
      "Epoch [31/50], Hidden Size: 128, LR: 0.0001, Batch Size: 16, Loss: 0.3324\n",
      "Epoch [32/50], Hidden Size: 128, LR: 0.0001, Batch Size: 16, Loss: 0.3302\n",
      "Epoch [33/50], Hidden Size: 128, LR: 0.0001, Batch Size: 16, Loss: 0.3344\n",
      "Epoch [34/50], Hidden Size: 128, LR: 0.0001, Batch Size: 16, Loss: 0.3316\n",
      "Epoch [35/50], Hidden Size: 128, LR: 0.0001, Batch Size: 16, Loss: 0.3312\n",
      "Epoch [36/50], Hidden Size: 128, LR: 0.0001, Batch Size: 16, Loss: 0.3306\n",
      "Epoch [37/50], Hidden Size: 128, LR: 0.0001, Batch Size: 16, Loss: 0.3333\n",
      "Epoch [38/50], Hidden Size: 128, LR: 0.0001, Batch Size: 16, Loss: 0.3334\n",
      "Epoch [39/50], Hidden Size: 128, LR: 0.0001, Batch Size: 16, Loss: 0.3331\n",
      "Epoch [40/50], Hidden Size: 128, LR: 0.0001, Batch Size: 16, Loss: 0.3301\n",
      "Epoch [41/50], Hidden Size: 128, LR: 0.0001, Batch Size: 16, Loss: 0.3328\n",
      "Epoch [42/50], Hidden Size: 128, LR: 0.0001, Batch Size: 16, Loss: 0.3336\n",
      "Epoch [43/50], Hidden Size: 128, LR: 0.0001, Batch Size: 16, Loss: 0.3345\n",
      "Epoch [44/50], Hidden Size: 128, LR: 0.0001, Batch Size: 16, Loss: 0.3294\n",
      "Epoch [45/50], Hidden Size: 128, LR: 0.0001, Batch Size: 16, Loss: 0.3318\n",
      "Epoch [46/50], Hidden Size: 128, LR: 0.0001, Batch Size: 16, Loss: 0.3281\n",
      "Epoch [47/50], Hidden Size: 128, LR: 0.0001, Batch Size: 16, Loss: 0.3299\n",
      "Epoch [48/50], Hidden Size: 128, LR: 0.0001, Batch Size: 16, Loss: 0.3304\n",
      "Epoch [49/50], Hidden Size: 128, LR: 0.0001, Batch Size: 16, Loss: 0.3326\n",
      "Epoch [50/50], Hidden Size: 128, LR: 0.0001, Batch Size: 16, Loss: 0.3307\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\10374\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torchvision\\models\\_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "c:\\Users\\10374\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torchvision\\models\\_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=None`.\n",
      "  warnings.warn(msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/50], Hidden Size: 128, LR: 0.0001, Batch Size: 32, Loss: 0.9994\n",
      "Epoch [2/50], Hidden Size: 128, LR: 0.0001, Batch Size: 32, Loss: 0.9966\n",
      "Epoch [3/50], Hidden Size: 128, LR: 0.0001, Batch Size: 32, Loss: 0.9920\n",
      "Epoch [4/50], Hidden Size: 128, LR: 0.0001, Batch Size: 32, Loss: 0.9844\n",
      "Epoch [5/50], Hidden Size: 128, LR: 0.0001, Batch Size: 32, Loss: 0.9722\n",
      "Epoch [6/50], Hidden Size: 128, LR: 0.0001, Batch Size: 32, Loss: 0.9551\n",
      "Epoch [7/50], Hidden Size: 128, LR: 0.0001, Batch Size: 32, Loss: 0.9335\n",
      "Epoch [8/50], Hidden Size: 128, LR: 0.0001, Batch Size: 32, Loss: 0.9076\n",
      "Epoch [9/50], Hidden Size: 128, LR: 0.0001, Batch Size: 32, Loss: 0.8772\n",
      "Epoch [10/50], Hidden Size: 128, LR: 0.0001, Batch Size: 32, Loss: 0.8430\n",
      "Epoch [11/50], Hidden Size: 128, LR: 0.0001, Batch Size: 32, Loss: 0.8021\n",
      "Epoch [12/50], Hidden Size: 128, LR: 0.0001, Batch Size: 32, Loss: 0.7550\n",
      "Epoch [13/50], Hidden Size: 128, LR: 0.0001, Batch Size: 32, Loss: 0.7032\n",
      "Epoch [14/50], Hidden Size: 128, LR: 0.0001, Batch Size: 32, Loss: 0.6457\n",
      "Epoch [15/50], Hidden Size: 128, LR: 0.0001, Batch Size: 32, Loss: 0.5845\n",
      "Epoch [16/50], Hidden Size: 128, LR: 0.0001, Batch Size: 32, Loss: 0.5199\n",
      "Epoch [17/50], Hidden Size: 128, LR: 0.0001, Batch Size: 32, Loss: 0.4560\n",
      "Epoch [18/50], Hidden Size: 128, LR: 0.0001, Batch Size: 32, Loss: 0.4022\n",
      "Epoch [19/50], Hidden Size: 128, LR: 0.0001, Batch Size: 32, Loss: 0.3630\n",
      "Epoch [20/50], Hidden Size: 128, LR: 0.0001, Batch Size: 32, Loss: 0.3400\n",
      "Epoch [21/50], Hidden Size: 128, LR: 0.0001, Batch Size: 32, Loss: 0.3344\n",
      "Epoch [22/50], Hidden Size: 128, LR: 0.0001, Batch Size: 32, Loss: 0.3334\n",
      "Epoch [23/50], Hidden Size: 128, LR: 0.0001, Batch Size: 32, Loss: 0.3347\n",
      "Epoch [24/50], Hidden Size: 128, LR: 0.0001, Batch Size: 32, Loss: 0.3326\n",
      "Epoch [25/50], Hidden Size: 128, LR: 0.0001, Batch Size: 32, Loss: 0.3333\n",
      "Epoch [26/50], Hidden Size: 128, LR: 0.0001, Batch Size: 32, Loss: 0.3345\n",
      "Epoch [27/50], Hidden Size: 128, LR: 0.0001, Batch Size: 32, Loss: 0.3342\n",
      "Epoch [28/50], Hidden Size: 128, LR: 0.0001, Batch Size: 32, Loss: 0.3342\n",
      "Epoch [29/50], Hidden Size: 128, LR: 0.0001, Batch Size: 32, Loss: 0.3323\n",
      "Epoch [30/50], Hidden Size: 128, LR: 0.0001, Batch Size: 32, Loss: 0.3331\n",
      "Epoch [31/50], Hidden Size: 128, LR: 0.0001, Batch Size: 32, Loss: 0.3323\n",
      "Epoch [32/50], Hidden Size: 128, LR: 0.0001, Batch Size: 32, Loss: 0.3347\n",
      "Epoch [33/50], Hidden Size: 128, LR: 0.0001, Batch Size: 32, Loss: 0.3339\n",
      "Epoch [34/50], Hidden Size: 128, LR: 0.0001, Batch Size: 32, Loss: 0.3312\n",
      "Epoch [35/50], Hidden Size: 128, LR: 0.0001, Batch Size: 32, Loss: 0.3343\n",
      "Epoch [36/50], Hidden Size: 128, LR: 0.0001, Batch Size: 32, Loss: 0.3325\n",
      "Epoch [37/50], Hidden Size: 128, LR: 0.0001, Batch Size: 32, Loss: 0.3334\n",
      "Epoch [38/50], Hidden Size: 128, LR: 0.0001, Batch Size: 32, Loss: 0.3299\n",
      "Epoch [39/50], Hidden Size: 128, LR: 0.0001, Batch Size: 32, Loss: 0.3333\n",
      "Epoch [40/50], Hidden Size: 128, LR: 0.0001, Batch Size: 32, Loss: 0.3349\n",
      "Epoch [41/50], Hidden Size: 128, LR: 0.0001, Batch Size: 32, Loss: 0.3347\n",
      "Epoch [42/50], Hidden Size: 128, LR: 0.0001, Batch Size: 32, Loss: 0.3313\n",
      "Epoch [43/50], Hidden Size: 128, LR: 0.0001, Batch Size: 32, Loss: 0.3318\n",
      "Epoch [44/50], Hidden Size: 128, LR: 0.0001, Batch Size: 32, Loss: 0.3321\n",
      "Epoch [45/50], Hidden Size: 128, LR: 0.0001, Batch Size: 32, Loss: 0.3338\n",
      "Epoch [46/50], Hidden Size: 128, LR: 0.0001, Batch Size: 32, Loss: 0.3334\n",
      "Epoch [47/50], Hidden Size: 128, LR: 0.0001, Batch Size: 32, Loss: 0.3336\n",
      "Epoch [48/50], Hidden Size: 128, LR: 0.0001, Batch Size: 32, Loss: 0.3327\n",
      "Epoch [49/50], Hidden Size: 128, LR: 0.0001, Batch Size: 32, Loss: 0.3338\n",
      "Epoch [50/50], Hidden Size: 128, LR: 0.0001, Batch Size: 32, Loss: 0.3319\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\10374\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torchvision\\models\\_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "c:\\Users\\10374\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torchvision\\models\\_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=None`.\n",
      "  warnings.warn(msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/50], Hidden Size: 128, LR: 0.0001, Batch Size: 64, Loss: 0.9995\n",
      "Epoch [2/50], Hidden Size: 128, LR: 0.0001, Batch Size: 64, Loss: 0.9979\n",
      "Epoch [3/50], Hidden Size: 128, LR: 0.0001, Batch Size: 64, Loss: 0.9958\n",
      "Epoch [4/50], Hidden Size: 128, LR: 0.0001, Batch Size: 64, Loss: 0.9931\n",
      "Epoch [5/50], Hidden Size: 128, LR: 0.0001, Batch Size: 64, Loss: 0.9893\n",
      "Epoch [6/50], Hidden Size: 128, LR: 0.0001, Batch Size: 64, Loss: 0.9847\n",
      "Epoch [7/50], Hidden Size: 128, LR: 0.0001, Batch Size: 64, Loss: 0.9785\n",
      "Epoch [8/50], Hidden Size: 128, LR: 0.0001, Batch Size: 64, Loss: 0.9706\n",
      "Epoch [9/50], Hidden Size: 128, LR: 0.0001, Batch Size: 64, Loss: 0.9618\n",
      "Epoch [10/50], Hidden Size: 128, LR: 0.0001, Batch Size: 64, Loss: 0.9516\n",
      "Epoch [11/50], Hidden Size: 128, LR: 0.0001, Batch Size: 64, Loss: 0.9396\n",
      "Epoch [12/50], Hidden Size: 128, LR: 0.0001, Batch Size: 64, Loss: 0.9255\n",
      "Epoch [13/50], Hidden Size: 128, LR: 0.0001, Batch Size: 64, Loss: 0.9108\n",
      "Epoch [14/50], Hidden Size: 128, LR: 0.0001, Batch Size: 64, Loss: 0.8975\n",
      "Epoch [15/50], Hidden Size: 128, LR: 0.0001, Batch Size: 64, Loss: 0.8780\n",
      "Epoch [16/50], Hidden Size: 128, LR: 0.0001, Batch Size: 64, Loss: 0.8587\n",
      "Epoch [17/50], Hidden Size: 128, LR: 0.0001, Batch Size: 64, Loss: 0.8397\n",
      "Epoch [18/50], Hidden Size: 128, LR: 0.0001, Batch Size: 64, Loss: 0.8194\n",
      "Epoch [19/50], Hidden Size: 128, LR: 0.0001, Batch Size: 64, Loss: 0.7942\n",
      "Epoch [20/50], Hidden Size: 128, LR: 0.0001, Batch Size: 64, Loss: 0.7668\n",
      "Epoch [21/50], Hidden Size: 128, LR: 0.0001, Batch Size: 64, Loss: 0.7442\n",
      "Epoch [22/50], Hidden Size: 128, LR: 0.0001, Batch Size: 64, Loss: 0.7102\n",
      "Epoch [23/50], Hidden Size: 128, LR: 0.0001, Batch Size: 64, Loss: 0.6840\n",
      "Epoch [24/50], Hidden Size: 128, LR: 0.0001, Batch Size: 64, Loss: 0.6543\n",
      "Epoch [25/50], Hidden Size: 128, LR: 0.0001, Batch Size: 64, Loss: 0.6168\n",
      "Epoch [26/50], Hidden Size: 128, LR: 0.0001, Batch Size: 64, Loss: 0.5844\n",
      "Epoch [27/50], Hidden Size: 128, LR: 0.0001, Batch Size: 64, Loss: 0.5531\n",
      "Epoch [28/50], Hidden Size: 128, LR: 0.0001, Batch Size: 64, Loss: 0.5152\n",
      "Epoch [29/50], Hidden Size: 128, LR: 0.0001, Batch Size: 64, Loss: 0.4763\n",
      "Epoch [30/50], Hidden Size: 128, LR: 0.0001, Batch Size: 64, Loss: 0.4479\n",
      "Epoch [31/50], Hidden Size: 128, LR: 0.0001, Batch Size: 64, Loss: 0.4198\n",
      "Epoch [32/50], Hidden Size: 128, LR: 0.0001, Batch Size: 64, Loss: 0.3910\n",
      "Epoch [33/50], Hidden Size: 128, LR: 0.0001, Batch Size: 64, Loss: 0.3635\n",
      "Epoch [34/50], Hidden Size: 128, LR: 0.0001, Batch Size: 64, Loss: 0.3515\n",
      "Epoch [35/50], Hidden Size: 128, LR: 0.0001, Batch Size: 64, Loss: 0.3436\n",
      "Epoch [36/50], Hidden Size: 128, LR: 0.0001, Batch Size: 64, Loss: 0.3361\n",
      "Epoch [37/50], Hidden Size: 128, LR: 0.0001, Batch Size: 64, Loss: 0.3323\n",
      "Epoch [38/50], Hidden Size: 128, LR: 0.0001, Batch Size: 64, Loss: 0.3272\n",
      "Epoch [39/50], Hidden Size: 128, LR: 0.0001, Batch Size: 64, Loss: 0.3333\n",
      "Epoch [40/50], Hidden Size: 128, LR: 0.0001, Batch Size: 64, Loss: 0.3302\n",
      "Epoch [41/50], Hidden Size: 128, LR: 0.0001, Batch Size: 64, Loss: 0.3378\n",
      "Epoch [42/50], Hidden Size: 128, LR: 0.0001, Batch Size: 64, Loss: 0.3328\n",
      "Epoch [43/50], Hidden Size: 128, LR: 0.0001, Batch Size: 64, Loss: 0.3374\n",
      "Epoch [44/50], Hidden Size: 128, LR: 0.0001, Batch Size: 64, Loss: 0.3357\n",
      "Epoch [45/50], Hidden Size: 128, LR: 0.0001, Batch Size: 64, Loss: 0.3336\n",
      "Epoch [46/50], Hidden Size: 128, LR: 0.0001, Batch Size: 64, Loss: 0.3350\n",
      "Epoch [47/50], Hidden Size: 128, LR: 0.0001, Batch Size: 64, Loss: 0.3353\n",
      "Epoch [48/50], Hidden Size: 128, LR: 0.0001, Batch Size: 64, Loss: 0.3245\n",
      "Epoch [49/50], Hidden Size: 128, LR: 0.0001, Batch Size: 64, Loss: 0.3322\n",
      "Epoch [50/50], Hidden Size: 128, LR: 0.0001, Batch Size: 64, Loss: 0.3302\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "CombinedModel(\n",
       "  (seq_net): LSTM(2, 64, batch_first=True)\n",
       "  (seq_fc): Linear(in_features=64, out_features=32, bias=True)\n",
       "  (viome_net): LSTM(1, 64, batch_first=True)\n",
       "  (viome_fc): Linear(in_features=64, out_features=32, bias=True)\n",
       "  (img_net): ResNet(\n",
       "    (conv1): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n",
       "    (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (relu): ReLU(inplace=True)\n",
       "    (maxpool): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n",
       "    (layer1): Sequential(\n",
       "      (0): Bottleneck(\n",
       "        (conv1): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "        (downsample): Sequential(\n",
       "          (0): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        )\n",
       "      )\n",
       "      (1): Bottleneck(\n",
       "        (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "      )\n",
       "      (2): Bottleneck(\n",
       "        (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "      )\n",
       "    )\n",
       "    (layer2): Sequential(\n",
       "      (0): Bottleneck(\n",
       "        (conv1): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "        (downsample): Sequential(\n",
       "          (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "          (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        )\n",
       "      )\n",
       "      (1): Bottleneck(\n",
       "        (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "      )\n",
       "      (2): Bottleneck(\n",
       "        (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "      )\n",
       "      (3): Bottleneck(\n",
       "        (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "      )\n",
       "    )\n",
       "    (layer3): Sequential(\n",
       "      (0): Bottleneck(\n",
       "        (conv1): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "        (downsample): Sequential(\n",
       "          (0): Conv2d(512, 1024, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "          (1): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        )\n",
       "      )\n",
       "      (1): Bottleneck(\n",
       "        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "      )\n",
       "      (2): Bottleneck(\n",
       "        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "      )\n",
       "      (3): Bottleneck(\n",
       "        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "      )\n",
       "      (4): Bottleneck(\n",
       "        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "      )\n",
       "      (5): Bottleneck(\n",
       "        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "      )\n",
       "    )\n",
       "    (layer4): Sequential(\n",
       "      (0): Bottleneck(\n",
       "        (conv1): Conv2d(1024, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "        (downsample): Sequential(\n",
       "          (0): Conv2d(1024, 2048, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "          (1): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        )\n",
       "      )\n",
       "      (1): Bottleneck(\n",
       "        (conv1): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "      )\n",
       "      (2): Bottleneck(\n",
       "        (conv1): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "      )\n",
       "    )\n",
       "    (avgpool): AdaptiveAvgPool2d(output_size=(1, 1))\n",
       "    (fc): Linear(in_features=2048, out_features=32, bias=True)\n",
       "  )\n",
       "  (num_net): Sequential(\n",
       "    (0): Linear(in_features=20, out_features=32, bias=True)\n",
       "    (1): ReLU()\n",
       "    (2): Linear(in_features=32, out_features=16, bias=True)\n",
       "  )\n",
       "  (fc): Sequential(\n",
       "    (0): Linear(in_features=144, out_features=64, bias=True)\n",
       "    (1): ReLU()\n",
       "    (2): Linear(in_features=64, out_features=1, bias=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Hyperparameters\n",
    "hidden_sizes = [32, 64, 128]\n",
    "learning_rates = [1e-3, 1e-4]\n",
    "batch_sizes = [16, 32, 64]\n",
    "\n",
    "best_loss = float('inf')\n",
    "best_model = None\n",
    "\n",
    "# Iterate over different combinations of hyperparameters to find the best model\n",
    "for hidden_size in hidden_sizes:\n",
    "    for lr in learning_rates:\n",
    "        for batch_size in batch_sizes:\n",
    "            # Create the dataset loader for training and testing\n",
    "            train_loader = DataLoader(dataset_train, batch_size=batch_size, shuffle=True)\n",
    "            test_loader = DataLoader(dataset_test, batch_size=batch_size, shuffle=False)\n",
    "            \n",
    "            model = CombinedModel(hidden_size=hidden_size)\n",
    "            criterion = RMSRELoss()  \n",
    "            optimizer = optim.Adam(model.parameters(), lr=lr)\n",
    "            \n",
    "            # Learning rate scheduler\n",
    "            scheduler = optim.lr_scheduler.StepLR(optimizer, step_size=50, gamma=0.1)\n",
    "            \n",
    "            # Set up the device\n",
    "            device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "            model = model.to(device)\n",
    "            criterion = criterion.to(device)\n",
    "            \n",
    "            # Training process\n",
    "            num_epochs = 50\n",
    "            for epoch in range(num_epochs):\n",
    "                model.train()\n",
    "                running_loss = 0.0\n",
    "                for batch in train_loader:\n",
    "                    # Move data to device\n",
    "                    sequential_data = batch['sequential_data'].to(device)\n",
    "                    img_b = batch['img_b'].to(device)\n",
    "                    img_l = batch['img_l'].to(device)\n",
    "                    numeric_data = batch['numeric_data'].to(device)\n",
    "                    viome_sequential = batch['viome_sequential'].to(device)\n",
    "                    label = batch['label'].to(device)\n",
    "                    \n",
    "                    # Forward pass\n",
    "                    outputs = model(sequential_data, img_b, img_l, numeric_data, viome_sequential)\n",
    "                    loss = criterion(outputs, label)\n",
    "                    \n",
    "                    # Backward pass and optimization\n",
    "                    optimizer.zero_grad()\n",
    "                    loss.backward()\n",
    "                    optimizer.step()\n",
    "                    \n",
    "                    running_loss += loss.item()\n",
    "                scheduler.step()\n",
    "                \n",
    "                avg_loss = running_loss / len(train_loader)\n",
    "                print(f\"Epoch [{epoch+1}/{num_epochs}], Hidden Size: {hidden_size}, LR: {lr}, Batch Size: {batch_size}, Loss: {avg_loss:.4f}\")\n",
    "            \n",
    "            # Save the best model\n",
    "            if avg_loss < best_loss:\n",
    "                best_loss = avg_loss\n",
    "                best_model = model\n",
    "\n",
    "# Use the best trained model for predictions\n",
    "best_model.eval()  # Set the model to evaluation mode\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predictions saved to 'submission.csv' in Kaggle format.\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "# Use the best trained model for predictions\n",
    "best_model.eval()  # Set the model to evaluation mode\n",
    "test_loss = 0.0\n",
    "all_outputs = []\n",
    "all_labels = []\n",
    "\n",
    "with torch.no_grad():  # Disable gradient calculation\n",
    "    for batch in test_loader:\n",
    "        sequential_data = batch['sequential_data'].to(device)\n",
    "        img_b = batch['img_b'].to(device)\n",
    "        img_l = batch['img_l'].to(device)\n",
    "        numeric_data = batch['numeric_data'].to(device)\n",
    "        viome_sequential = batch['viome_sequential'].to(device)\n",
    "        \n",
    "        # Forward pass\n",
    "        outputs = model(sequential_data, img_b, img_l, numeric_data, viome_sequential)\n",
    "        \n",
    "        all_outputs += ([i[0] for i in outputs.tolist()])\n",
    "        \n",
    "test_ids = np.arange(len(all_outputs)) \n",
    "submission = pd.DataFrame({\"row_id\": test_ids, \"label\": all_outputs})\n",
    "submission.to_csv(\"submission.csv\", index=False)\n",
    "print(\"Predictions saved to 'submission.csv' in Kaggle format.\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ML",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
