{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preprocessed data saved as 'preprocessed_train.csv' and 'preprocessed_test.csv'\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from ast import literal_eval\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
    "from datetime import datetime\n",
    "\n",
    "# Step 1: Load datasets\n",
    "label_train = pd.read_csv('633FinalData/label_train.csv')\n",
    "label_test_breakfast_only = pd.read_csv('633FinalData/label_test_breakfast_only.csv')\n",
    "img_train = pd.read_csv('633FinalData/img_train.csv')  # Contains image data columns\n",
    "img_test = pd.read_csv('633FinalData/img_test.csv')    # Contains image data columns\n",
    "demo_viome_train = pd.read_csv('633FinalData/demo_viome_train.csv')\n",
    "demo_viome_test = pd.read_csv('633FinalData/demo_viome_test.csv')\n",
    "cgm_train = pd.read_csv('633FinalData/cgm_train.csv')\n",
    "cgm_test = pd.read_csv('633FinalData/cgm_test.csv')\n",
    "\n",
    "# Step 2: Remove Subject ID and Day to prevent overfitting\n",
    "label_train = label_train.drop(columns=[\"Subject ID\", \"Day\"])\n",
    "label_test_breakfast_only = label_test_breakfast_only.drop(columns=[\"Subject ID\", \"Day\"])\n",
    "cgm_train = cgm_train.drop(columns=[\"Subject ID\", \"Day\"])\n",
    "cgm_test = cgm_test.drop(columns=[\"Subject ID\", \"Day\"])\n",
    "demo_viome_train = demo_viome_train.drop(columns=[\"Subject ID\"])\n",
    "demo_viome_test = demo_viome_test.drop(columns=[\"Subject ID\"])\n",
    "img_train = img_train.drop(columns=[\"Subject ID\", \"Day\"])\n",
    "img_test = img_test.drop(columns=[\"Subject ID\", \"Day\"])\n",
    "\n",
    "# Convert time to step\n",
    "def to_step(t):\n",
    "    date_obj = datetime.strptime(t, '%Y-%m-%d %H:%M:%S')\n",
    "    return (date_obj.hour*60 + date_obj.minute)//5\n",
    "\n",
    "def cgm_to_steps(cgm):\n",
    "    steps = [0 for _ in range(288)]\n",
    "    for t,value in cgm:\n",
    "        steps[to_step(t)] = value\n",
    "    return steps\n",
    "\n",
    "def time_to_step(t1,t2):\n",
    "    if t1 == '{}' or t2 == '{}':\n",
    "        return [0 for _ in range(288)]\n",
    "    steps = [0 for _ in range(288)]\n",
    "    steps[to_step(t1)] = 1\n",
    "    steps[to_step(t2)] = 1\n",
    "    return steps\n",
    "\n",
    "# Drop rows with NaT values in 'start_time' or 'end_time'\n",
    "# cgm_test = cgm_test.dropna()\n",
    "cgm_train = cgm_train.dropna()\n",
    "# Drop rows containing the string '{}' in any column\n",
    "# cgm_test = cgm_test[~cgm_test.apply(lambda row: row.astype(str).str.contains('{}').any(), axis=1)]\n",
    "cgm_train = cgm_train[~cgm_train.apply(lambda row: row.astype(str).str.contains('{}').any(), axis=1)]\n",
    "\n",
    "cgm_test['cgm_sequential'] = cgm_test['cgm'].apply(cgm_to_steps)\n",
    "cgm_train['cgm_sequential'] = cgm_train['cgm'].apply(cgm_to_steps)\n",
    "cgm_train['when_to_eat'] = cgm_train[['Breakfast Time', 'Lunch Time']].apply(lambda x: time_to_step(x['Breakfast Time'], x['Lunch Time']), axis=1)\n",
    "cgm_test['when_to_eat'] = cgm_test[['Breakfast Time', 'Lunch Time']].apply(lambda x: time_to_step(x['Breakfast Time'], x['Lunch Time']), axis=1)\n",
    "\n",
    "# Step 3: Convert image data from string to numeric arrays\n",
    "def parse_image_data(df, column_name):\n",
    "    \"\"\"Parse image data stored as strings into numeric arrays.\"\"\"\n",
    "    parsed_data = []\n",
    "    for img_str in df[column_name]:\n",
    "        try:\n",
    "            # Convert string representation to an actual array\n",
    "            img_array = np.array(literal_eval(img_str))\n",
    "            # Flatten the 3D array to 1D\n",
    "            img_flat = img_array.flatten()\n",
    "            parsed_data.append(img_flat)\n",
    "        except Exception as e:\n",
    "            print(f\"Error parsing image data: {e}\")\n",
    "            # If parsing fails, add a zero-filled placeholder\n",
    "            parsed_data.append(np.zeros((112 * 112 * 3)))  # Adjust size as needed\n",
    "    return pd.DataFrame(parsed_data)\n",
    "\n",
    "# Process train and test image data\n",
    "train_images = parse_image_data(img_train, 'Image Before Lunch')  # Replace with correct column name if needed\n",
    "test_images = parse_image_data(img_test, 'Image Before Lunch')    # Replace with correct column name if needed\n",
    "\n",
    "# Step 4: Handle numerical and categorical columns in demo_viome datasets\n",
    "# Identify numerical and categorical columns\n",
    "numerical_columns = demo_viome_train.select_dtypes(include=np.number).columns\n",
    "categorical_columns = demo_viome_train.select_dtypes(include='object').columns\n",
    "\n",
    "# Standardize numerical data\n",
    "scaler = StandardScaler()\n",
    "demo_viome_train[numerical_columns] = scaler.fit_transform(demo_viome_train[numerical_columns])\n",
    "demo_viome_test[numerical_columns] = scaler.transform(demo_viome_test[numerical_columns])\n",
    "\n",
    "# Encode categorical data\n",
    "encoder = OneHotEncoder(sparse_output=False, handle_unknown='ignore')\n",
    "encoded_train = encoder.fit_transform(demo_viome_train[categorical_columns])\n",
    "encoded_test = encoder.transform(demo_viome_test[categorical_columns])\n",
    "\n",
    "# Convert encoded data to DataFrame\n",
    "encoded_train_df = pd.DataFrame(encoded_train, columns=encoder.get_feature_names_out(categorical_columns))\n",
    "encoded_test_df = pd.DataFrame(encoded_test, columns=encoder.get_feature_names_out(categorical_columns))\n",
    "\n",
    "# Merge encoded data back with numerical data\n",
    "demo_viome_train = pd.concat([demo_viome_train.reset_index(drop=True), encoded_train_df], axis=1).drop(columns=categorical_columns)\n",
    "demo_viome_test = pd.concat([demo_viome_test.reset_index(drop=True), encoded_test_df], axis=1).drop(columns=categorical_columns)\n",
    "\n",
    "# Step 5: Merge datasets without Subject ID or Day\n",
    "merged_train = pd.concat([cgm_train, label_train, demo_viome_train, train_images], axis=1)\n",
    "merged_test = pd.concat([cgm_test, label_test_breakfast_only, demo_viome_test, test_images], axis=1)\n",
    "\n",
    "# Step 6: Handle missing values\n",
    "merged_train = merged_train.fillna(method='ffill').fillna(method='bfill')\n",
    "merged_test = merged_test.fillna(method='ffill').fillna(method='bfill')\n",
    "\n",
    "# Step 7: Save preprocessed data\n",
    "merged_train.to_csv('preprocessed_train.csv', index=False)\n",
    "merged_test.to_csv('preprocessed_test.csv', index=False)\n",
    "\n",
    "print(\"Preprocessed data saved as 'preprocessed_train.csv' and 'preprocessed_test.csv'\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['Subject ID', 'Day', 'Image Before Breakfast', 'Image Before Lunch'], dtype='object')\n"
     ]
    }
   ],
   "source": [
    "print(img_train.columns)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
