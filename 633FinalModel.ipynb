{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1af09f7a-ffec-40a5-92c3-6d0e6c6cbd9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torchvision.transforms as transforms\n",
    "from PIL import Image\n",
    "from io import BytesIO\n",
    "import ast\n",
    "import re\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "import torch\n",
    "from torch.nn.utils.rnn import pack_padded_sequence, pad_sequence\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "import torch.nn as nn\n",
    "import torchvision.models as models\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error\n",
    "import torch.optim as optim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "057b122d-a0eb-4fc5-9c30-39ddddc50d05",
   "metadata": {},
   "outputs": [],
   "source": [
    "img_train = pd.read_csv('633FinalData/img_train.csv')\n",
    "cgm_train = pd.read_csv('633FinalData/cgm_train.csv')\n",
    "demo_viome_train = pd.read_csv('633FinalData/demo_viome_train.csv')\n",
    "label_train = pd.read_csv('633FinalData/label_train.csv')\n",
    "img_test = pd.read_csv('633FinalData/img_test.csv')\n",
    "cgm_test = pd.read_csv('633FinalData/cgm_test.csv')\n",
    "demo_viome_test= pd.read_csv('633FinalData/demo_viome_test.csv')\n",
    "label_test = pd.read_csv('633FinalData/label_test_breakfast_only.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c2a30d57-7f2f-49d8-8ba9-3dd4d9dfa101",
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_image(image_str):\n",
    "    image_data = ast.literal_eval(image_str)\n",
    "    image_array = np.array(image_data, dtype=np.uint8)\n",
    "    return image_array.astype(np.float32) / 255.0\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b6ece9c6-e9e4-4420-b707-1ce9b77be461",
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_str(image_str):\n",
    "    image_data = ast.literal_eval(image_str)\n",
    "    image_array = np.array(image_data, dtype=np.float64)\n",
    "    return image_array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "36d92808-fe3c-48de-953a-fcd53e30b243",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_time(step):\n",
    "    time_bar = np.zeros((288, 1))\n",
    "    time_bar[step - 1, 0] = 1\n",
    "    return time_bar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d5886f62-2971-4c50-a73a-a8eadc7dd5e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler_global = MinMaxScaler()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "749d78ac-7ce3-4e37-b040-500558822283",
   "metadata": {},
   "outputs": [],
   "source": [
    "def data_preprocess(img_train,cgm_train,label_train,demo_viome_train,train=0):\n",
    "    img_train = img_train.drop('Subject ID', axis=1)\n",
    "    cgm_train = cgm_train.drop('Subject ID', axis=1)\n",
    "    label_train = label_train.drop('Subject ID', axis=1)\n",
    "    demo_viome_train = demo_viome_train.drop('Subject ID', axis=1)\n",
    "    img_train = img_train.drop('Day', axis=1)\n",
    "    cgm_train = cgm_train.drop('Day', axis=1)\n",
    "    label_train = label_train.drop('Day', axis=1)\n",
    "    \n",
    "    repeated_demo_viome_train = demo_viome_train.loc[demo_viome_train.index.repeat(9)].reset_index(drop=True)\n",
    "    \n",
    "    \n",
    "    combined_data = pd.concat([img_train, cgm_train, label_train,repeated_demo_viome_train], axis=1)\n",
    "    combined_data = combined_data.dropna(subset=['Image Before Breakfast'])\n",
    "    \n",
    "    idx = []\n",
    "    for i in range (combined_data.shape[0]):\n",
    "        for j in range (combined_data.shape[1]):\n",
    "            cell = combined_data.iloc[i, j]\n",
    "            if isinstance(cell, str) and len(cell) == 2:  # Checking string length\n",
    "                idx.append(i)\n",
    "    \n",
    "    combined_data.drop(idx, inplace=True)\n",
    "    if train:   \n",
    "        combined_data['Breakfast Time'] = pd.to_datetime(combined_data['Breakfast Time'])\n",
    "        \n",
    "        combined_data['Step'] = np.round((combined_data['Breakfast Time'].dt.hour*60+combined_data['Breakfast Time'].dt.minute)/5).astype(int)\n",
    "        combined_data['Breakfast minute'] = combined_data['Step'].apply(get_time)\n",
    "\n",
    "        combined_data['Lunch Time'] = pd.to_datetime(combined_data['Lunch Time'])\n",
    "        combined_data['Step'] = np.round((combined_data['Lunch Time'].dt.hour*60+combined_data['Lunch Time'].dt.minute)/5).astype(int)\n",
    "        combined_data['Lunch minute'] = combined_data['Step'].apply(get_time)\n",
    "    \n",
    "    \n",
    "    else:\n",
    "        combined_data['Breakfast Time'] = pd.to_datetime(combined_data['Breakfast Time'])     \n",
    "        combined_data['Step'] = np.round((combined_data['Breakfast Time'].dt.hour*60+combined_data['Breakfast Time'].dt.minute)/5).astype(int)\n",
    "        combined_data['Breakfast minute'] = combined_data['Step'].apply(get_time)\n",
    "    \n",
    "        \n",
    "    combined_data['cgm_numbers'] = combined_data['CGM Data'].apply(lambda x: [float(num) for num in re.findall(r\",\\s([\\d\\.]+)\\)\", x)])\n",
    "\n",
    "\n",
    "    combined_data['gcm_start'] = combined_data['CGM Data'].apply(lambda x: ast.literal_eval(x)[0][0])\n",
    "\n",
    "    combined_data['gcm_start_step']  = ((combined_data['gcm_start'].astype('datetime64[ns]').dt.hour*60  + combined_data['gcm_start'].astype('datetime64[ns]').dt.minute)/5).astype(int)\n",
    "    \n",
    "    combined_data['repeated_front'] = combined_data.apply(lambda row: [row['cgm_numbers'][0]] * row['gcm_start_step']+ row['cgm_numbers'][1:],axis=1)\n",
    "    \n",
    "    \n",
    "    combined_data['gcm_number_bar'] = combined_data.apply(lambda row: row['repeated_front'][:-1] + [row['repeated_front'][-1]] * (289-len(row['repeated_front'])),axis=1)\n",
    "    \n",
    "    combined_data['Race'] = pd.Categorical(combined_data['Race'], categories=['Hispanic/Latino', 'White', 'Other'])\n",
    "    \n",
    "    # If needed, convert the categories into numerical codes\n",
    "    combined_data['Race_Categorical'] = combined_data['Race'].cat.codes\n",
    "    \n",
    "    \n",
    "    combined_data = combined_data.drop(['Step','Breakfast Time','Lunch Time','CGM Data','Race','Step','cgm_numbers','gcm_start','gcm_start_step','repeated_front'], axis=1)\n",
    "    \n",
    "    # Print all column names\n",
    "    print(combined_data.columns.tolist())\n",
    "    \n",
    "    \n",
    "    \n",
    "    combined_data['Viome'] = combined_data['Viome'].apply(convert_str)\n",
    "    if train:\n",
    "        img_b = combined_data[['Image Before Breakfast']]\n",
    "\n",
    "        img_l = combined_data[['Image Before Lunch']]\n",
    "        rest = combined_data.drop(columns=['Image Before Breakfast', 'Image Before Lunch'])\n",
    "    else:\n",
    "        img_b = combined_data[['Image Before Breakfast']]\n",
    "\n",
    "        rest = combined_data.drop(columns=['Image Before Breakfast', 'Image Before Lunch'])\n",
    "    \n",
    "    if train:\n",
    "        label_b = combined_data[['Breakfast Calories']]\n",
    "\n",
    "        label_l = combined_data[['Lunch Calories']]\n",
    "        rest = rest.drop(columns=['Breakfast Calories', 'Lunch Calories', 'Breakfast Carbs',\n",
    "                                  'Lunch Carbs', 'Breakfast Fat', 'Lunch Fat',\n",
    "                                  'Breakfast Protein', 'Lunch Protein',])\n",
    "    else:\n",
    "        label_b = combined_data[['Breakfast Calories']]\n",
    "        rest = rest.drop(columns=['Breakfast Calories', 'Breakfast Carbs','Breakfast Fat', \n",
    "                                  'Breakfast Protein'])\n",
    "        \n",
    "\n",
    "    \n",
    "    catagorical = combined_data[['Gender','Diabetes Status','Race_Categorical']]\n",
    "    rest = rest.drop(columns=['Gender','Diabetes Status','Race_Categorical'])\n",
    "\n",
    "    if train:       \n",
    "        time_set  = combined_data[['gcm_number_bar','Viome','Breakfast minute']]\n",
    "        time_set_l = combined_data[['Lunch minute']]\n",
    "        continues = rest.drop(columns=['gcm_number_bar','Viome','Breakfast minute','Lunch minute'])\n",
    "    else:\n",
    "        time_set  = combined_data[['gcm_number_bar','Viome','Breakfast minute']]\n",
    "        continues = rest.drop(columns=['gcm_number_bar','Viome','Breakfast minute'])\n",
    "    scaler1 = MinMaxScaler()  # Or StandardScaler for standardization\n",
    "    scaler2 = MinMaxScaler()\n",
    "    scaler3 = MinMaxScaler()\n",
    "    scaler4 = MinMaxScaler()\n",
    "    scaler5 = MinMaxScaler()\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    ###\n",
    "    # Convert list of sequences to a NumPy array\n",
    "    gcm_number_array = np.array(time_set['gcm_number_bar'].tolist())  \n",
    "    gcm_number_array = gcm_number_array.reshape(-1,288)\n",
    "    # Fit and transform the array\n",
    "    scaled_gcm_number_array = scaler1.fit_transform(gcm_number_array)\n",
    "    scaled_gcm_number_array_expand = np.expand_dims(scaled_gcm_number_array, axis=-1)\n",
    "    gcm_number_tensors = torch.tensor(scaled_gcm_number_array_expand, dtype=torch.float32)\n",
    "\n",
    "\n",
    "    \n",
    "    Viome_array = np.array(time_set['Viome'].tolist())\n",
    "    # Fit and transform the array\n",
    "    scaled_Viome_sequences = scaler2.fit_transform(Viome_array)\n",
    "    scaled_Viome_sequences_expand = np.expand_dims(scaled_Viome_sequences, axis=-1)\n",
    "    Viome_tensors = torch.tensor(scaled_Viome_sequences_expand, dtype=torch.float32)\n",
    "\n",
    "    if train:        \n",
    "        minute_array = np.array(time_set['Breakfast minute'].tolist())\n",
    "        minute_array = minute_array.reshape(-1,288)\n",
    "        # print(minute_array.shape)\n",
    "        # Fit and transform the array\n",
    "        scaled_minute_array = scaler3.fit_transform(minute_array)\n",
    "        scaled_minute_array_expand = np.expand_dims(scaled_minute_array, axis=-1)\n",
    "        minute_tensors = torch.tensor(scaled_minute_array_expand, dtype=torch.float32)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "        minute_array_l = np.array(time_set_l['Lunch minute'].tolist())\n",
    "        minute_array_l = minute_array_l.reshape(-1,288)\n",
    "        # print(minute_array.shape)\n",
    "        # Fit and transform the array\n",
    "        scaled_minute_l_array = scaler3.fit_transform(minute_array_l)\n",
    "        scaled_minute_l_array_expand = np.expand_dims(scaled_minute_l_array, axis=-1)\n",
    "        minute_l_tensors = torch.tensor(scaled_minute_l_array_expand, dtype=torch.float32)\n",
    "    else:\n",
    "        minute_array = np.array(time_set['Breakfast minute'].tolist())      \n",
    "        # Fit and transform the array\n",
    "        minute_array = minute_array.reshape(-1,288)\n",
    "        scaled_minute_array = scaler3.fit_transform(minute_array)\n",
    "        scaled_minute_array_expand = np.expand_dims(scaled_minute_array, axis=-1)\n",
    "        minute_tensors = torch.tensor(scaled_minute_array_expand, dtype=torch.float32)\n",
    "    \n",
    "    # Convert the scaled array back to a tensor\n",
    "\n",
    "\n",
    "    \n",
    "    # Pad the variable-length sequences\n",
    "\n",
    "    # print(len(fixed_sequence_tensors[0]))\n",
    "    # print(len(padded_variable_sequences[0]))\n",
    "\n",
    "    \n",
    "\n",
    "    if train:  \n",
    "        img_b['Image Before Breakfast'] = img_b['Image Before Breakfast'].apply(convert_image)\n",
    "        # img_set['Image Before Lunch'] = img_set['Image Before Lunch'].apply(convert_image)    \n",
    "        # Convert numpy arrays into tensors and stack them\n",
    "        img_b_tensors = torch.stack([torch.tensor(img) for img in img_b['Image Before Breakfast']])\n",
    "        # img_tensors_lunch = torch.stack([torch.tensor(img) for img in img_set['Image Before Lunch']])\n",
    "\n",
    "\n",
    "        img_l['Image Before Lunch'] = img_l['Image Before Lunch'].apply(convert_image)\n",
    "        # img_set['Image Before Lunch'] = img_set['Image Before Lunch'].apply(convert_image)    \n",
    "        # Convert numpy arrays into tensors and stack them\n",
    "        img_l_tensors = torch.stack([torch.tensor(img) for img in img_l['Image Before Lunch']])\n",
    "        # img_tensors_lunch = torch.stack([torch.tensor(img) for img in img_set['Image Before Lunch']])\n",
    "    else:\n",
    "        img_b['Image Before Breakfast'] = img_b['Image Before Breakfast'].apply(convert_image)\n",
    "        # img_set['Image Before Lunch'] = img_set['Image Before Lunch'].apply(convert_image)    \n",
    "        # Convert numpy arrays into tensors and stack them\n",
    "        img_b_tensors = torch.stack([torch.tensor(img) for img in img_b['Image Before Breakfast']])\n",
    "        # img_tensors_lunch = torch.stack([torch.tensor(img) for img in img_set['Image Before Lunch']])\n",
    "    \n",
    "    \n",
    "\n",
    "    continuous_scaled = scaler4.fit_transform(continues)\n",
    "    continuous_tensor = torch.tensor(continuous_scaled, dtype=torch.float32)\n",
    "\n",
    "        \n",
    "    if train:\n",
    "        # Ensure labels are numeric and then convert to tensor\n",
    "        label_l = label_l.apply(pd.to_numeric, errors='coerce')  # Convert to numeric, coercing errors\n",
    "        print(label_l.shape)\n",
    "        label_l_scaled = scaler_global.fit_transform(label_l)\n",
    "        label_l_tensor = torch.tensor(label_l_scaled, dtype=torch.float32)\n",
    "\n",
    "    label_b = label_b.apply(pd.to_numeric, errors='coerce')  # Convert to numeric, coercing errors\n",
    "    label_b_scaled = scaler5.fit_transform(label_b)\n",
    "    label_b_tensor = torch.tensor(label_b_scaled, dtype=torch.float32)\n",
    "\n",
    "    # Ensure categorical data is numeric and then convert to tensor\n",
    "    catagorical = catagorical.apply(pd.to_numeric, errors='coerce')\n",
    "    catagorical_tensor = torch.tensor(catagorical.values, dtype=torch.float32)\n",
    "    \n",
    "    # print(len(continuous_tensor[0]))\n",
    "    # print(len(catagorical_tensor[0]))\n",
    "    # print(img_tensors_breakfast[0].shape)\n",
    "    # print(img_tensors_lunch[0].shape)\n",
    "\n",
    "    if train:\n",
    "        return img_b_tensors,minute_tensors,gcm_number_tensors,Viome_tensors, catagorical_tensor, continuous_tensor,label_b_tensor,label_l_tensor,img_l_tensors,minute_l_tensors\n",
    "    else:\n",
    "        return img_b_tensors,minute_tensors,gcm_number_tensors,Viome_tensors, catagorical_tensor, continuous_tensor,label_b_tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "da9991b3-e95f-43f7-b45c-663f0c0dd3fc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Image Before Breakfast', 'Image Before Lunch', 'Breakfast Calories', 'Lunch Calories', 'Breakfast Carbs', 'Lunch Carbs', 'Breakfast Fat', 'Lunch Fat', 'Breakfast Protein', 'Lunch Protein', 'Age', 'Gender', 'Weight', 'Height', 'Diabetes Status', 'A1C', 'Baseline Fasting Glucose', 'Insulin', 'Triglycerides', 'Cholesterol', 'HDL', 'Non-HDL', 'LDL', 'VLDL', 'CHO/HDL Ratio', 'HOMA-IR', 'BMI', 'Viome', 'Breakfast minute', 'Lunch minute', 'gcm_number_bar', 'Race_Categorical']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\38997\\AppData\\Local\\Temp\\ipykernel_14044\\236630948.py:167: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  img_b['Image Before Breakfast'] = img_b['Image Before Breakfast'].apply(convert_image)\n",
      "C:\\Users\\38997\\AppData\\Local\\Temp\\ipykernel_14044\\236630948.py:174: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  img_l['Image Before Lunch'] = img_l['Image Before Lunch'].apply(convert_image)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(271, 1)\n",
      "['Image Before Breakfast', 'Image Before Lunch', 'Breakfast Calories', 'Breakfast Carbs', 'Breakfast Fat', 'Breakfast Protein', 'Age', 'Gender', 'Weight', 'Height', 'Diabetes Status', 'A1C', 'Baseline Fasting Glucose', 'Insulin', 'Triglycerides', 'Cholesterol', 'HDL', 'Non-HDL', 'LDL', 'VLDL', 'CHO/HDL Ratio', 'HOMA-IR', 'BMI', 'Viome', 'Breakfast minute', 'gcm_number_bar', 'Race_Categorical']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\38997\\AppData\\Local\\Temp\\ipykernel_14044\\236630948.py:180: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  img_b['Image Before Breakfast'] = img_b['Image Before Breakfast'].apply(convert_image)\n"
     ]
    }
   ],
   "source": [
    "img_b_train,minute_b_train,gcm_number_train,Viome_train, catagorical_train, continuous_train,label_b_train,label_l_train,img_l_train,minute_l_train= data_preprocess(img_train,cgm_train,label_train,demo_viome_train,train=1)\n",
    "img_b_test,minute_b_test,gcm_number_test,Viome_test, catagorical_test, continuous_test,label_b_test= data_preprocess(img_test,cgm_test,label_test,demo_viome_test,train=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "4b407f14",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([271, 288, 1])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "minute_b_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "9428324c-4121-42b8-9234-a5e298180308",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomTrainDataset(Dataset):\n",
    "    def __init__(self,img_b_train,minute_b_train,gcm_number_train,\n",
    "                 Viome_train, catagorical_train, continuous_train,\n",
    "                 label_b_train,label_l_train,img_l_train,\n",
    "                 minute_l_train):\n",
    "        \n",
    "        self.catagorical_train = np.vstack([catagorical_train,catagorical_train])\n",
    "        self.gcm_number_train =  np.vstack([gcm_number_train,gcm_number_train])\n",
    "        self.continuous_train = np.vstack([continuous_train,continuous_train])\n",
    "        self.Viome_train = np.vstack([Viome_train,Viome_train])\n",
    "\n",
    "        \n",
    "        self.minute_train = np.vstack([minute_b_train,minute_l_train])\n",
    "        self.img_train = np.vstack([img_b_train,img_l_train])\n",
    "        \n",
    "        \n",
    "        self.label = np.vstack([label_b_train,label_l_train])\n",
    "\n",
    "    \n",
    "    def __len__(self):\n",
    "        # Assuming all tensors have the same first dimension size\n",
    "        return len(self.img_train)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        # Fetch each tensor's slice at the given index\n",
    "        return {\n",
    "            'img': self.img_train[idx],\n",
    "            'catagorical': self.catagorical_train[idx],\n",
    "            'minute_train': self.minute_train[idx],\n",
    "            'gcm_number_train': self.gcm_number_train[idx],\n",
    "            'Viome_train': self.Viome_train[idx],\n",
    "            'continuous': self.continuous_train[idx],\n",
    "            'label': self.label[idx],\n",
    "\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "85044a1e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([271, 288, 1])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gcm_number_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "d52e6ff6-3f13-42dc-a1c2-9bd074c32038",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomTestDataset(Dataset):\n",
    "    def __init__(self, img_b_train,minute_b_train,gcm_number_train,\n",
    "                 Viome_train, catagorical_train, continuous_train,\n",
    "                 label_b_train):\n",
    "        self.img_train = img_b_train\n",
    "        self.catagorical_train = catagorical_train\n",
    "        self.minute_train = minute_b_train\n",
    "        self.gcm_number_train = gcm_number_train\n",
    "        self.Viome_train = Viome_train\n",
    "        self.continuous_train = continuous_train\n",
    "        self.label = label_b_train\n",
    "\n",
    "\n",
    "    \n",
    "    def __len__(self):\n",
    "        # Assuming all tensors have the same first dimension size\n",
    "        return len(self.img_train)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        # Fetch each tensor's slice at the given index\n",
    "        return {\n",
    "            'img': self.img_train[idx],\n",
    "            'catagorical': self.catagorical_train[idx],\n",
    "            'minute_train': self.minute_train[idx],\n",
    "            'gcm_number_train': self.gcm_number_train[idx],\n",
    "            'Viome_train': self.Viome_train[idx],\n",
    "            'continuous': self.continuous_train[idx],\n",
    "            'label': self.label[idx]\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "f022d3cd-8ac7-4eed-8c1e-46086dc14fab",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assume tensors have already been defined as img_tensors, label_tensor, etc.\n",
    "train_dataset = CustomTrainDataset(img_b_train,minute_b_train,gcm_number_train,Viome_train, catagorical_train, continuous_train,label_b_train,label_l_train,img_l_train,minute_l_train)\n",
    "\n",
    "# Define DataLoader with batch size, shuffling, etc.\n",
    "train_loader = DataLoader(train_dataset, batch_size=64, shuffle=True, num_workers=0,drop_last=True)\n",
    "\n",
    "\n",
    "test_dataset = CustomTestDataset(img_b_test,minute_b_test,gcm_number_test,Viome_test, catagorical_test, continuous_test,label_b_test)\n",
    "\n",
    "# Define DataLoader with batch size, shuffling, etc.\n",
    "test_loader = DataLoader(train_dataset, batch_size=64, shuffle=True, num_workers=0,drop_last=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "89bc2e8a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "58"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(test_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "5e63de1a-173d-4cd5-acfc-b2526195acae",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultimodalModel(nn.Module):\n",
    "    def __init__(self, image_model_name=\"resnet18\", fusion_dim=512):\n",
    "        super(MultimodalModel, self).__init__()\n",
    "\n",
    "        # Image Encoder\n",
    "        self.image_model = models.resnet18(pretrained=True)\n",
    "        self.image_model.fc = nn.Linear(self.image_model.fc.in_features, 128)\n",
    "\n",
    "        # Time-Series Encoder\n",
    "        # self.lstm1 = nn.LSTM(input_size=27, hidden_size=128, num_layers=73, batch_first=True)#fixed for 1\n",
    "        # self.lstm2 = nn.LSTM(input_size=288, hidden_size=128, num_layers=73, batch_first=True)\n",
    "        self.lstm1 = nn.LSTM(input_size=1, hidden_size=128, num_layers=3, batch_first=True)#fixed for 1\n",
    "        self.lstm2 = nn.LSTM(input_size=1, hidden_size=128, num_layers=3, batch_first=True)\n",
    "\n",
    "        # Tabular Data Encoder\n",
    "        self.cata_fc = nn.Sequential(\n",
    "            nn.Linear(3, 64),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.2),\n",
    "            nn.Linear(64, 128)\n",
    "        )\n",
    "        self.conti_fc = nn.Sequential(\n",
    "            nn.Linear(15, 64),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.2),\n",
    "            nn.Linear(64, 128)\n",
    "        )\n",
    "\n",
    "        # Fusion Layer\n",
    "        self.fusion_fc = nn.Sequential(\n",
    "            nn.Linear(128 + 128 + 128 + 128 + 128 + 128, fusion_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.3),\n",
    "            nn.Linear(fusion_dim, 128),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "\n",
    "        # Regression Head\n",
    "        self.regressor = nn.Sequential(\n",
    "            nn.Linear(128, 64),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(64, 1),  # Single output for calorie prediction\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "\n",
    "    def forward(self, image, time_series_1, time_series_2, time_series_3, catagotical, continous):\n",
    "        # Process image\n",
    "        image = image.permute(0, 3, 1, 2)\n",
    "        img_feat = self.image_model(image)\n",
    "        \n",
    "        # Process time series\n",
    "        _, (time_feat_1, _) = self.lstm1(time_series_1)\n",
    "        time_feat_1 = time_feat_1[-1]  # Extract the last layer hidden state\n",
    "\n",
    "        _, (time_feat_2, _) = self.lstm2(time_series_2)\n",
    "        time_feat_2 = time_feat_2[-1]\n",
    "\n",
    "        _, (time_feat_3, _) = self.lstm1(time_series_3)\n",
    "        time_feat_3 = time_feat_3[-1]\n",
    "\n",
    "        # Process tabular data\n",
    "        cat_feat = self.cata_fc(catagotical)\n",
    "        conti_feat = self.conti_fc(continous)\n",
    "\n",
    "        # Fuse features\n",
    "        fused = torch.cat([img_feat, time_feat_1, time_feat_2, time_feat_3, cat_feat, conti_feat], dim=1)\n",
    "        fusion_out = self.fusion_fc(fused)\n",
    "\n",
    "        # Predict\n",
    "        output = self.regressor(fusion_out)\n",
    "        return output\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "8fd5e6a1-4d2e-4f61-bf19-dfa832b8a702",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\Anaconda\\envs\\ML\\Lib\\site-packages\\torchvision\\models\\_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "d:\\Anaconda\\envs\\ML\\Lib\\site-packages\\torchvision\\models\\_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ResNet18_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet18_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n"
     ]
    }
   ],
   "source": [
    "model = MultimodalModel(\n",
    "    image_model_name=\"resnet18\",\n",
    "    fusion_dim=512\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "c1bf0652",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch.utils.data.dataloader.DataLoader at 0x2489096b140>"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "981116ab-0353-4f52-80be-67e376d66b13",
   "metadata": {},
   "outputs": [],
   "source": [
    "class RMSRELoss(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(RMSRELoss, self).__init__()\n",
    "\n",
    "    def forward(self, y_pred, y_true):\n",
    "        # Compute relative error\n",
    "        relative_error = (y_pred - y_true) / (y_true + 1e-8)  # Avoid division by zero\n",
    "        # Compute RMSRE\n",
    "        rmsre = torch.sqrt(torch.mean(relative_error ** 2))\n",
    "        return rmsre"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "50ba7d72-57af-4d8c-9ec5-16de5db361ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = RMSRELoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.0001, weight_decay=1e-4)\n",
    "\n",
    "# # Train the model\n",
    "# trained_model = train_model(model, dataloaders, criterion, optimizer, num_epochs=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "9afc397c-0fee-41ca-9f3f-76a7eaee86c7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([64, 1])"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "label.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "53a0b203",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([64, 288, 1])"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "minute.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "eb0c0893-86f4-4a73-a4fe-90a77219879b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/2000], Loss: 21124772.0000\n",
      "Epoch [2/2000], Loss: 19308750.0000\n",
      "Epoch [3/2000], Loss: 11764999.0000\n",
      "Epoch [4/2000], Loss: 613714.6250\n",
      "Epoch [5/2000], Loss: 377236.5625\n",
      "Epoch [6/2000], Loss: 58876.1133\n",
      "Epoch [7/2000], Loss: 11416.7988\n",
      "Epoch [8/2000], Loss: 901.1302\n",
      "Epoch [9/2000], Loss: 1695.0291\n",
      "Epoch [10/2000], Loss: 4364.9473\n",
      "Epoch [11/2000], Loss: 3236.0142\n",
      "Epoch [12/2000], Loss: 638.6122\n",
      "Epoch [13/2000], Loss: 2600.9128\n",
      "Epoch [14/2000], Loss: 619.6202\n",
      "Epoch [15/2000], Loss: 317.9405\n",
      "Epoch [16/2000], Loss: 2097.9131\n",
      "Epoch [17/2000], Loss: 173.7346\n",
      "Epoch [18/2000], Loss: 313.8823\n",
      "Epoch [19/2000], Loss: 1489.1962\n",
      "Epoch [20/2000], Loss: 1045.0649\n",
      "Epoch [21/2000], Loss: 1518.5588\n",
      "Epoch [22/2000], Loss: 402.7976\n",
      "Epoch [23/2000], Loss: 341.5477\n",
      "Epoch [24/2000], Loss: 599.5549\n",
      "Epoch [25/2000], Loss: 344.0292\n",
      "Epoch [26/2000], Loss: 659.5441\n",
      "Epoch [27/2000], Loss: 5226.9775\n",
      "Epoch [28/2000], Loss: 1532.9285\n",
      "Epoch [29/2000], Loss: 541.3856\n",
      "Epoch [30/2000], Loss: 8.6672\n",
      "Epoch [31/2000], Loss: 391.7865\n",
      "Epoch [32/2000], Loss: 2223.9038\n",
      "Epoch [33/2000], Loss: 121.0293\n",
      "Epoch [34/2000], Loss: 509.0367\n",
      "Epoch [35/2000], Loss: 621.1377\n",
      "Epoch [36/2000], Loss: 113.7629\n",
      "Epoch [37/2000], Loss: 562.4620\n",
      "Epoch [38/2000], Loss: 29.2316\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[42], line 14\u001b[0m\n\u001b[0;32m     11\u001b[0m label \u001b[38;5;241m=\u001b[39m data[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlabel\u001b[39m\u001b[38;5;124m'\u001b[39m]\n\u001b[0;32m     12\u001b[0m \u001b[38;5;66;03m# print(gcm_number.shape)\u001b[39;00m\n\u001b[0;32m     13\u001b[0m \u001b[38;5;66;03m# Forward pass\u001b[39;00m\n\u001b[1;32m---> 14\u001b[0m outputs \u001b[38;5;241m=\u001b[39m model(images,minute,gcm_number, Viome, categoricals, continuous)\n\u001b[0;32m     15\u001b[0m loss \u001b[38;5;241m=\u001b[39m criterion(outputs, label)\n\u001b[0;32m     17\u001b[0m \u001b[38;5;66;03m# Backward pass and optimization\u001b[39;00m\n",
      "File \u001b[1;32md:\\Anaconda\\envs\\ML\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1532\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1530\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1531\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1532\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32md:\\Anaconda\\envs\\ML\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1541\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1536\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1537\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1538\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1539\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1540\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1541\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1543\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1544\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "Cell \u001b[1;32mIn[35], line 55\u001b[0m, in \u001b[0;36mMultimodalModel.forward\u001b[1;34m(self, image, time_series_1, time_series_2, time_series_3, catagotical, continous)\u001b[0m\n\u001b[0;32m     52\u001b[0m _, (time_feat_1, _) \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlstm1(time_series_1)\n\u001b[0;32m     53\u001b[0m time_feat_1 \u001b[38;5;241m=\u001b[39m time_feat_1[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m]  \u001b[38;5;66;03m# Extract the last layer hidden state\u001b[39;00m\n\u001b[1;32m---> 55\u001b[0m _, (time_feat_2, _) \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlstm2(time_series_2)\n\u001b[0;32m     56\u001b[0m time_feat_2 \u001b[38;5;241m=\u001b[39m time_feat_2[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m]\n\u001b[0;32m     58\u001b[0m _, (time_feat_3, _) \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlstm1(time_series_3)\n",
      "File \u001b[1;32md:\\Anaconda\\envs\\ML\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1532\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1530\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1531\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1532\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32md:\\Anaconda\\envs\\ML\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1541\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1536\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1537\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1538\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1539\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1540\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1541\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1543\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1544\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[1;32md:\\Anaconda\\envs\\ML\\Lib\\site-packages\\torch\\nn\\modules\\rnn.py:911\u001b[0m, in \u001b[0;36mLSTM.forward\u001b[1;34m(self, input, hx)\u001b[0m\n\u001b[0;32m    908\u001b[0m         hx \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpermute_hidden(hx, sorted_indices)\n\u001b[0;32m    910\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m batch_sizes \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m--> 911\u001b[0m     result \u001b[38;5;241m=\u001b[39m _VF\u001b[38;5;241m.\u001b[39mlstm(\u001b[38;5;28minput\u001b[39m, hx, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_flat_weights, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbias, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnum_layers,\n\u001b[0;32m    912\u001b[0m                       \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdropout, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtraining, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbidirectional, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbatch_first)\n\u001b[0;32m    913\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    914\u001b[0m     result \u001b[38;5;241m=\u001b[39m _VF\u001b[38;5;241m.\u001b[39mlstm(\u001b[38;5;28minput\u001b[39m, batch_sizes, hx, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_flat_weights, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbias,\n\u001b[0;32m    915\u001b[0m                       \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnum_layers, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdropout, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtraining, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbidirectional)\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "num_epochs = 2000\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    for data in train_loader:\n",
    "        images = data['img']\n",
    "        categoricals = data['catagorical']\n",
    "        minute = data['minute_train']\n",
    "        gcm_number = data['gcm_number_train']\n",
    "        Viome = data['Viome_train']\n",
    "        continuous = data['continuous']\n",
    "        label = data['label']\n",
    "        # print(gcm_number.shape)\n",
    "        # Forward pass\n",
    "        outputs = model(images,minute,gcm_number, Viome, categoricals, continuous)\n",
    "        loss = criterion(outputs, label)\n",
    "\n",
    "        # Backward pass and optimization\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "    print(f\"Epoch [{epoch+1}/{num_epochs}], Loss: {loss.item():.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "d463f9c2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/2000], Loss: 10042134.0000\n",
      "Epoch [2/2000], Loss: 2080225.0000\n",
      "Epoch [3/2000], Loss: 182633.2969\n",
      "Epoch [4/2000], Loss: 187469.2344\n",
      "Epoch [5/2000], Loss: 7367.8613\n",
      "Epoch [6/2000], Loss: 4061.2874\n",
      "Epoch [7/2000], Loss: 1752.8878\n",
      "Epoch [8/2000], Loss: 344.0167\n",
      "Epoch [9/2000], Loss: 5825.9443\n",
      "Epoch [10/2000], Loss: 775.9544\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[23], line 20\u001b[0m\n\u001b[0;32m     18\u001b[0m     \u001b[38;5;66;03m# 反向传播和优化\u001b[39;00m\n\u001b[0;32m     19\u001b[0m     optimizer\u001b[38;5;241m.\u001b[39mzero_grad()\n\u001b[1;32m---> 20\u001b[0m     loss\u001b[38;5;241m.\u001b[39mbackward()\n\u001b[0;32m     21\u001b[0m     optimizer\u001b[38;5;241m.\u001b[39mstep()\n\u001b[0;32m     23\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mEpoch [\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mepoch\u001b[38;5;241m+\u001b[39m\u001b[38;5;241m1\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mnum_epochs\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m], Loss: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mloss\u001b[38;5;241m.\u001b[39mitem()\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.4f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[1;32md:\\Anaconda\\envs\\ML\\Lib\\site-packages\\torch\\_tensor.py:525\u001b[0m, in \u001b[0;36mTensor.backward\u001b[1;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[0;32m    515\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m    516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[0;32m    517\u001b[0m         Tensor\u001b[38;5;241m.\u001b[39mbackward,\n\u001b[0;32m    518\u001b[0m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    523\u001b[0m         inputs\u001b[38;5;241m=\u001b[39minputs,\n\u001b[0;32m    524\u001b[0m     )\n\u001b[1;32m--> 525\u001b[0m torch\u001b[38;5;241m.\u001b[39mautograd\u001b[38;5;241m.\u001b[39mbackward(\n\u001b[0;32m    526\u001b[0m     \u001b[38;5;28mself\u001b[39m, gradient, retain_graph, create_graph, inputs\u001b[38;5;241m=\u001b[39minputs\n\u001b[0;32m    527\u001b[0m )\n",
      "File \u001b[1;32md:\\Anaconda\\envs\\ML\\Lib\\site-packages\\torch\\autograd\\__init__.py:267\u001b[0m, in \u001b[0;36mbackward\u001b[1;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[0;32m    262\u001b[0m     retain_graph \u001b[38;5;241m=\u001b[39m create_graph\n\u001b[0;32m    264\u001b[0m \u001b[38;5;66;03m# The reason we repeat the same comment below is that\u001b[39;00m\n\u001b[0;32m    265\u001b[0m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[0;32m    266\u001b[0m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[1;32m--> 267\u001b[0m _engine_run_backward(\n\u001b[0;32m    268\u001b[0m     tensors,\n\u001b[0;32m    269\u001b[0m     grad_tensors_,\n\u001b[0;32m    270\u001b[0m     retain_graph,\n\u001b[0;32m    271\u001b[0m     create_graph,\n\u001b[0;32m    272\u001b[0m     inputs,\n\u001b[0;32m    273\u001b[0m     allow_unreachable\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[0;32m    274\u001b[0m     accumulate_grad\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[0;32m    275\u001b[0m )\n",
      "File \u001b[1;32md:\\Anaconda\\envs\\ML\\Lib\\site-packages\\torch\\autograd\\graph.py:744\u001b[0m, in \u001b[0;36m_engine_run_backward\u001b[1;34m(t_outputs, *args, **kwargs)\u001b[0m\n\u001b[0;32m    742\u001b[0m     unregister_hooks \u001b[38;5;241m=\u001b[39m _register_logging_hooks_on_whole_graph(t_outputs)\n\u001b[0;32m    743\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 744\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m Variable\u001b[38;5;241m.\u001b[39m_execution_engine\u001b[38;5;241m.\u001b[39mrun_backward(  \u001b[38;5;66;03m# Calls into the C++ engine to run the backward pass\u001b[39;00m\n\u001b[0;32m    745\u001b[0m         t_outputs, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs\n\u001b[0;32m    746\u001b[0m     )  \u001b[38;5;66;03m# Calls into the C++ engine to run the backward pass\u001b[39;00m\n\u001b[0;32m    747\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[0;32m    748\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m attach_logging_hooks:\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "num_epochs = 2000\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    for data in train_loader:\n",
    "        # 获取数据\n",
    "        images = data['img']\n",
    "        categoricals = data['catagorical']\n",
    "        minute = data['minute_train']\n",
    "        gcm_number = data['gcm_number_train']\n",
    "        Viome = data['Viome_train']\n",
    "        continuous = data['continuous']\n",
    "        label = data['label']\n",
    "        \n",
    "        # 前向传播\n",
    "        outputs = model(images, minute, gcm_number, Viome, categoricals, continuous)\n",
    "        loss = criterion(outputs, label)\n",
    "\n",
    "        # 反向传播和优化\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "    print(f\"Epoch [{epoch+1}/{num_epochs}], Loss: {loss.item():.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "id": "fecc59f5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([64, 1])"
      ]
     },
     "execution_count": 106,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "label.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "id": "52832d6e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('img', torch.Size([64, 64, 64, 3])),\n",
       " ('catagorical', torch.Size([64, 3])),\n",
       " ('minute_train', torch.Size([64, 288, 1])),\n",
       " ('gcm_number_train', torch.Size([64, 288, 1])),\n",
       " ('Viome_train', torch.Size([64, 27, 1])),\n",
       " ('continuous', torch.Size([64, 15])),\n",
       " ('label', torch.Size([64, 1]))]"
      ]
     },
     "execution_count": 101,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[(k,v.shape) for k,v in data.items()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "c3fada30-3a87-425b-9c80-9a57ce85bad7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(model, test_loader):\n",
    "    model.eval()  # Set model to evaluation mode\n",
    "    \n",
    "\n",
    "    with torch.no_grad():  # Disable gradient computation for testing\n",
    "        for data in test_loader:\n",
    "            images = data['img']\n",
    "            categoricals = data['catagorical']\n",
    "            minute = data['minute_train']\n",
    "            gcm_number = data['gcm_number_train']\n",
    "            Viome = data['Viome_train']\n",
    "            continuous = data['continuous']\n",
    "            label = data['label']\n",
    "\n",
    "            # Forward pass\n",
    "    outputs = model(images,minute,gcm_number, Viome, categoricals, continuous)\n",
    "    Prediction = scaler_global.inverse_transform(outputs.detach().numpy())\n",
    "\n",
    "\n",
    "    return Prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "7c7cdcd1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(model, test_loader):\n",
    "    model.eval()  # Set model to evaluation mode\n",
    "    \n",
    "    predictions = []  # Initialize an empty list to store predictions\n",
    "    \n",
    "    with torch.no_grad():  # Disable gradient computation for testing\n",
    "        for data in test_loader:\n",
    "            images = data['img']\n",
    "            categoricals = data['catagorical']\n",
    "            minute = data['minute_train']\n",
    "            gcm_number = data['gcm_number_train']\n",
    "            Viome = data['Viome_train']\n",
    "            continuous = data['continuous']\n",
    "            label = data['label']\n",
    "\n",
    "            # Forward pass\n",
    "            outputs = model(images, minute, gcm_number, Viome, categoricals, continuous)\n",
    "\n",
    "            # Reverse normalization (assuming outputs are normalized)\n",
    "            outputs = scaler_global.inverse_transform(outputs.detach().cpu().numpy())\n",
    "\n",
    "            # Append the batch of predictions to the predictions list\n",
    "            predictions.append(outputs)\n",
    "    \n",
    "    # Concatenate all predictions along the first dimension (batch dimension)\n",
    "    predictions = np.concatenate(predictions, axis=0)\n",
    "    \n",
    "    return predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "3b871643-dfac-4682-97ba-a5b62e235f38",
   "metadata": {},
   "outputs": [],
   "source": [
    "outputs  = predict(model, test_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "e71202ad-2c67-4f14-a458-e678b586b1f1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "512"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "outputs.size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "c7e34872-0485-4513-a745-1e9c42daf079",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of samples: 542\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "b76972c7-494e-4842-82d6-a1ccb1ee1cb3",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame(outputs, columns=['Column1'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "f92e77d8-a3ce-44f9-9501-f2b579cd7041",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Length of values (73) does not match length of index (64)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[99], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m df[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mrow_id\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m73\u001b[39m)\n\u001b[0;32m      2\u001b[0m \u001b[38;5;66;03m# Save to CSV\u001b[39;00m\n\u001b[0;32m      3\u001b[0m df\u001b[38;5;241m.\u001b[39mto_csv(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmy_data.csv\u001b[39m\u001b[38;5;124m'\u001b[39m, index\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n",
      "File \u001b[1;32md:\\Anaconda\\envs\\ML\\Lib\\site-packages\\pandas\\core\\frame.py:4311\u001b[0m, in \u001b[0;36mDataFrame.__setitem__\u001b[1;34m(self, key, value)\u001b[0m\n\u001b[0;32m   4308\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_setitem_array([key], value)\n\u001b[0;32m   4309\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m   4310\u001b[0m     \u001b[38;5;66;03m# set column\u001b[39;00m\n\u001b[1;32m-> 4311\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_set_item(key, value)\n",
      "File \u001b[1;32md:\\Anaconda\\envs\\ML\\Lib\\site-packages\\pandas\\core\\frame.py:4524\u001b[0m, in \u001b[0;36mDataFrame._set_item\u001b[1;34m(self, key, value)\u001b[0m\n\u001b[0;32m   4514\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_set_item\u001b[39m(\u001b[38;5;28mself\u001b[39m, key, value) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m   4515\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m   4516\u001b[0m \u001b[38;5;124;03m    Add series to DataFrame in specified column.\u001b[39;00m\n\u001b[0;32m   4517\u001b[0m \n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   4522\u001b[0m \u001b[38;5;124;03m    ensure homogeneity.\u001b[39;00m\n\u001b[0;32m   4523\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m-> 4524\u001b[0m     value, refs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sanitize_column(value)\n\u001b[0;32m   4526\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[0;32m   4527\u001b[0m         key \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcolumns\n\u001b[0;32m   4528\u001b[0m         \u001b[38;5;129;01mand\u001b[39;00m value\u001b[38;5;241m.\u001b[39mndim \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[0;32m   4529\u001b[0m         \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(value\u001b[38;5;241m.\u001b[39mdtype, ExtensionDtype)\n\u001b[0;32m   4530\u001b[0m     ):\n\u001b[0;32m   4531\u001b[0m         \u001b[38;5;66;03m# broadcast across multiple columns if necessary\u001b[39;00m\n\u001b[0;32m   4532\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcolumns\u001b[38;5;241m.\u001b[39mis_unique \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcolumns, MultiIndex):\n",
      "File \u001b[1;32md:\\Anaconda\\envs\\ML\\Lib\\site-packages\\pandas\\core\\frame.py:5266\u001b[0m, in \u001b[0;36mDataFrame._sanitize_column\u001b[1;34m(self, value)\u001b[0m\n\u001b[0;32m   5263\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m _reindex_for_setitem(value, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mindex)\n\u001b[0;32m   5265\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m is_list_like(value):\n\u001b[1;32m-> 5266\u001b[0m     com\u001b[38;5;241m.\u001b[39mrequire_length_match(value, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mindex)\n\u001b[0;32m   5267\u001b[0m arr \u001b[38;5;241m=\u001b[39m sanitize_array(value, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mindex, copy\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, allow_2d\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[0;32m   5268\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[0;32m   5269\u001b[0m     \u001b[38;5;28misinstance\u001b[39m(value, Index)\n\u001b[0;32m   5270\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m value\u001b[38;5;241m.\u001b[39mdtype \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mobject\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   5273\u001b[0m     \u001b[38;5;66;03m# TODO: Remove kludge in sanitize_array for string mode when enforcing\u001b[39;00m\n\u001b[0;32m   5274\u001b[0m     \u001b[38;5;66;03m# this deprecation\u001b[39;00m\n",
      "File \u001b[1;32md:\\Anaconda\\envs\\ML\\Lib\\site-packages\\pandas\\core\\common.py:573\u001b[0m, in \u001b[0;36mrequire_length_match\u001b[1;34m(data, index)\u001b[0m\n\u001b[0;32m    569\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    570\u001b[0m \u001b[38;5;124;03mCheck the length of data matches the length of the index.\u001b[39;00m\n\u001b[0;32m    571\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    572\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(data) \u001b[38;5;241m!=\u001b[39m \u001b[38;5;28mlen\u001b[39m(index):\n\u001b[1;32m--> 573\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m    574\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mLength of values \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    575\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m(\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlen\u001b[39m(data)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m) \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    576\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdoes not match length of index \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    577\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m(\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlen\u001b[39m(index)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m)\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    578\u001b[0m     )\n",
      "\u001b[1;31mValueError\u001b[0m: Length of values (73) does not match length of index (64)"
     ]
    }
   ],
   "source": [
    "df['row_id'] = range(73)\n",
    "# Save to CSV\n",
    "df.to_csv('my_data.csv', index=True)  # index=False means do not write row names (index)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f94f0a4-5a73-4646-8ffb-22c8b2267bad",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ML",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
