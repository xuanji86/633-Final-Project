{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data Loaders Created Successfully!\n",
      "Batch Features Shape: torch.Size([32, 12353])\n",
      "Batch Labels Shape: torch.Size([32])\n",
      "Test Batch Features Shape: torch.Size([32, 12353])\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import numpy as np\n",
    "\n",
    "# Load preprocessed data\n",
    "train_file = 'preprocessed_train.csv'\n",
    "test_file = 'preprocessed_test.csv'\n",
    "\n",
    "train_data = pd.read_csv(train_file)\n",
    "test_data = pd.read_csv(test_file)\n",
    "\n",
    "# Ensure all columns are numeric\n",
    "train_data_numeric = train_data.apply(pd.to_numeric, errors='coerce')\n",
    "test_data_numeric = test_data.apply(pd.to_numeric, errors='coerce')\n",
    "\n",
    "# Handle missing values\n",
    "train_data_numeric = train_data_numeric.fillna(0)\n",
    "test_data_numeric = test_data_numeric.fillna(0)\n",
    "\n",
    "# Align features between train and test data\n",
    "target_column = 'Lunch Calories'  # Replace with the correct column name\n",
    "\n",
    "# Identify training features (excluding the target column)\n",
    "train_columns = train_data_numeric.drop(columns=[target_column]).columns\n",
    "test_columns = test_data_numeric.columns\n",
    "\n",
    "# Add missing columns to test data with default value 0\n",
    "for col in train_columns:\n",
    "    if col not in test_columns:\n",
    "        test_data_numeric[col] = 0\n",
    "\n",
    "# Remove extra columns from test data\n",
    "test_data_numeric = test_data_numeric[train_columns]\n",
    "\n",
    "# Extract labels and features\n",
    "train_features = train_data_numeric[train_columns].values.astype(np.float32)\n",
    "train_labels = train_data_numeric[target_column].values.astype(np.float32)\n",
    "test_features = test_data_numeric.values.astype(np.float32)\n",
    "\n",
    "# Custom Dataset class for PyTorch\n",
    "class MultiModalDataset(Dataset):\n",
    "    def __init__(self, features, labels=None):\n",
    "        self.features = torch.tensor(features, dtype=torch.float32)\n",
    "        self.labels = None if labels is None else torch.tensor(labels, dtype=torch.float32)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.features)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        if self.labels is not None:\n",
    "            return self.features[idx], self.labels[idx]\n",
    "        return self.features[idx]\n",
    "\n",
    "# Create DataLoader for train and test datasets\n",
    "train_dataset = MultiModalDataset(train_features, train_labels)\n",
    "test_dataset = MultiModalDataset(test_features)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=32, shuffle=False)\n",
    "\n",
    "print(\"Data Loaders Created Successfully!\")\n",
    "\n",
    "# Validate DataLoader outputs\n",
    "for batch_features, batch_labels in train_loader:\n",
    "    print(\"Batch Features Shape:\", batch_features.shape)\n",
    "    print(\"Batch Labels Shape:\", batch_labels.shape)\n",
    "    break\n",
    "\n",
    "# Validate test data\n",
    "for batch_features in test_loader:\n",
    "    print(\"Test Batch Features Shape:\", batch_features.shape)\n",
    "    break\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/200, Loss: 0.6046\n",
      "Epoch 2/200, Loss: 0.3813\n",
      "Epoch 3/200, Loss: 0.3493\n",
      "Epoch 4/200, Loss: 0.3476\n",
      "Epoch 5/200, Loss: 0.3308\n",
      "Epoch 6/200, Loss: 0.3585\n",
      "Epoch 7/200, Loss: 0.3420\n",
      "Epoch 8/200, Loss: 0.3472\n",
      "Epoch 9/200, Loss: 0.3138\n",
      "Epoch 10/200, Loss: 0.3678\n",
      "Epoch 11/200, Loss: 0.3499\n",
      "Epoch 12/200, Loss: 0.3303\n",
      "Epoch 13/200, Loss: 0.3366\n",
      "Epoch 14/200, Loss: 0.3401\n",
      "Epoch 15/200, Loss: 0.3287\n",
      "Epoch 16/200, Loss: 0.3084\n",
      "Epoch 17/200, Loss: 0.2961\n",
      "Epoch 18/200, Loss: 0.2962\n",
      "Epoch 19/200, Loss: 0.3030\n",
      "Epoch 20/200, Loss: 0.3179\n",
      "Epoch 21/200, Loss: 0.3180\n",
      "Epoch 22/200, Loss: 0.3284\n",
      "Epoch 23/200, Loss: 0.3289\n",
      "Epoch 24/200, Loss: 0.2919\n",
      "Epoch 25/200, Loss: 0.2859\n",
      "Epoch 26/200, Loss: 0.2856\n",
      "Epoch 27/200, Loss: 0.2846\n",
      "Epoch 28/200, Loss: 0.3024\n",
      "Epoch 29/200, Loss: 0.2798\n",
      "Epoch 30/200, Loss: 0.3442\n",
      "Epoch 31/200, Loss: 0.2862\n",
      "Epoch 32/200, Loss: 0.2735\n",
      "Epoch 33/200, Loss: 0.3652\n",
      "Epoch 34/200, Loss: 0.3429\n",
      "Epoch 35/200, Loss: 0.3236\n",
      "Epoch 36/200, Loss: 0.2909\n",
      "Epoch 37/200, Loss: 0.2692\n",
      "Epoch 38/200, Loss: 0.2865\n",
      "Epoch 39/200, Loss: 0.2947\n",
      "Epoch 40/200, Loss: 0.2738\n",
      "Epoch 41/200, Loss: 0.2898\n",
      "Epoch 42/200, Loss: 0.2873\n",
      "Epoch 43/200, Loss: 0.2638\n",
      "Epoch 44/200, Loss: 0.2875\n",
      "Epoch 45/200, Loss: 0.2494\n",
      "Epoch 46/200, Loss: 0.2802\n",
      "Epoch 47/200, Loss: 0.2569\n",
      "Epoch 48/200, Loss: 0.2334\n",
      "Epoch 49/200, Loss: 0.3291\n",
      "Epoch 50/200, Loss: 0.3054\n",
      "Epoch 51/200, Loss: 0.3780\n",
      "Epoch 52/200, Loss: 0.3287\n",
      "Epoch 53/200, Loss: 0.3170\n",
      "Epoch 54/200, Loss: 0.2954\n",
      "Epoch 55/200, Loss: 0.2820\n",
      "Epoch 56/200, Loss: 0.2743\n",
      "Epoch 57/200, Loss: 0.2731\n",
      "Epoch 58/200, Loss: 0.2578\n",
      "Epoch 59/200, Loss: 0.2693\n",
      "Epoch 60/200, Loss: 0.2635\n",
      "Epoch 61/200, Loss: 0.2694\n",
      "Epoch 62/200, Loss: 0.2465\n",
      "Epoch 63/200, Loss: 0.3063\n",
      "Epoch 64/200, Loss: 0.2276\n",
      "Epoch 65/200, Loss: 0.2164\n",
      "Epoch 66/200, Loss: 0.2767\n",
      "Epoch 67/200, Loss: 0.2571\n",
      "Epoch 68/200, Loss: 0.2437\n",
      "Epoch 69/200, Loss: 0.2306\n",
      "Epoch 70/200, Loss: 0.2411\n",
      "Epoch 71/200, Loss: 0.2370\n",
      "Epoch 72/200, Loss: 0.2172\n",
      "Epoch 73/200, Loss: 0.2089\n",
      "Epoch 74/200, Loss: 0.2594\n",
      "Epoch 75/200, Loss: 0.2416\n",
      "Epoch 76/200, Loss: 0.2110\n",
      "Epoch 77/200, Loss: 0.2054\n",
      "Epoch 78/200, Loss: 0.2047\n",
      "Epoch 79/200, Loss: 0.1907\n",
      "Epoch 80/200, Loss: 0.1993\n",
      "Epoch 81/200, Loss: 0.1812\n",
      "Epoch 82/200, Loss: 0.2139\n",
      "Epoch 83/200, Loss: 0.1977\n",
      "Epoch 84/200, Loss: 0.2227\n",
      "Epoch 85/200, Loss: 0.2042\n",
      "Epoch 86/200, Loss: 0.2122\n",
      "Epoch 87/200, Loss: 0.1737\n",
      "Epoch 88/200, Loss: 0.2037\n",
      "Epoch 89/200, Loss: 0.2154\n",
      "Epoch 90/200, Loss: 0.2009\n",
      "Epoch 91/200, Loss: 0.2071\n",
      "Epoch 92/200, Loss: 0.1693\n",
      "Epoch 93/200, Loss: 0.1769\n",
      "Epoch 94/200, Loss: 0.1920\n",
      "Epoch 95/200, Loss: 0.1686\n",
      "Epoch 96/200, Loss: 0.1680\n",
      "Epoch 97/200, Loss: 0.1938\n",
      "Epoch 98/200, Loss: 0.1801\n",
      "Epoch 99/200, Loss: 0.1896\n",
      "Epoch 100/200, Loss: 0.1764\n",
      "Epoch 101/200, Loss: 0.2069\n",
      "Epoch 102/200, Loss: 0.1616\n",
      "Epoch 103/200, Loss: 0.1515\n",
      "Epoch 104/200, Loss: 0.2571\n",
      "Epoch 105/200, Loss: 0.1954\n",
      "Epoch 106/200, Loss: 0.1708\n",
      "Epoch 107/200, Loss: 0.1560\n",
      "Epoch 108/200, Loss: 0.1611\n",
      "Epoch 109/200, Loss: 0.1511\n",
      "Epoch 110/200, Loss: 0.1532\n",
      "Epoch 111/200, Loss: 0.1751\n",
      "Epoch 112/200, Loss: 0.1618\n",
      "Epoch 113/200, Loss: 0.1943\n",
      "Epoch 114/200, Loss: 0.1681\n",
      "Epoch 115/200, Loss: 0.1731\n",
      "Epoch 116/200, Loss: 0.1592\n",
      "Epoch 117/200, Loss: 0.1736\n",
      "Epoch 118/200, Loss: 0.3047\n",
      "Epoch 119/200, Loss: 0.2210\n",
      "Epoch 120/200, Loss: 0.1633\n",
      "Epoch 121/200, Loss: 0.1445\n",
      "Epoch 122/200, Loss: 0.1590\n",
      "Epoch 123/200, Loss: 0.1527\n",
      "Epoch 124/200, Loss: 0.1914\n",
      "Epoch 125/200, Loss: 0.2194\n",
      "Epoch 126/200, Loss: 0.1441\n",
      "Epoch 127/200, Loss: 0.1601\n",
      "Epoch 128/200, Loss: 0.1611\n",
      "Epoch 129/200, Loss: 0.2030\n",
      "Epoch 130/200, Loss: 0.2040\n",
      "Epoch 131/200, Loss: 0.1944\n",
      "Epoch 132/200, Loss: 0.1518\n",
      "Epoch 133/200, Loss: 0.1539\n",
      "Epoch 134/200, Loss: 0.1337\n",
      "Epoch 135/200, Loss: 0.1482\n",
      "Epoch 136/200, Loss: 0.1301\n",
      "Epoch 137/200, Loss: 0.1312\n",
      "Epoch 138/200, Loss: 0.1457\n",
      "Epoch 139/200, Loss: 0.1748\n",
      "Epoch 140/200, Loss: 0.1754\n",
      "Epoch 141/200, Loss: 0.1607\n",
      "Epoch 142/200, Loss: 0.1777\n",
      "Epoch 143/200, Loss: 0.1386\n",
      "Epoch 144/200, Loss: 0.1335\n",
      "Epoch 145/200, Loss: 0.1396\n",
      "Epoch 146/200, Loss: 0.1236\n",
      "Epoch 147/200, Loss: 0.1452\n",
      "Epoch 148/200, Loss: 0.2642\n",
      "Epoch 149/200, Loss: 0.2164\n",
      "Epoch 150/200, Loss: 0.2025\n",
      "Epoch 151/200, Loss: 0.1642\n",
      "Epoch 152/200, Loss: 0.1696\n",
      "Epoch 153/200, Loss: 0.1517\n",
      "Epoch 154/200, Loss: 0.1333\n",
      "Epoch 155/200, Loss: 0.1482\n",
      "Epoch 156/200, Loss: 0.1196\n",
      "Epoch 157/200, Loss: 0.1536\n",
      "Epoch 158/200, Loss: 0.1245\n",
      "Epoch 159/200, Loss: 0.1193\n",
      "Epoch 160/200, Loss: 0.1237\n",
      "Epoch 161/200, Loss: 0.1125\n",
      "Epoch 162/200, Loss: 0.1622\n",
      "Epoch 163/200, Loss: 0.1241\n",
      "Epoch 164/200, Loss: 0.1234\n",
      "Epoch 165/200, Loss: 0.1200\n",
      "Epoch 166/200, Loss: 0.1209\n",
      "Epoch 167/200, Loss: 0.1552\n",
      "Epoch 168/200, Loss: 0.1444\n",
      "Epoch 169/200, Loss: 0.1150\n",
      "Epoch 170/200, Loss: 0.1410\n",
      "Epoch 171/200, Loss: 0.1731\n",
      "Epoch 172/200, Loss: 0.1637\n",
      "Epoch 173/200, Loss: 0.1846\n",
      "Epoch 174/200, Loss: 0.2182\n",
      "Epoch 175/200, Loss: 0.1274\n",
      "Epoch 176/200, Loss: 0.1259\n",
      "Epoch 177/200, Loss: 0.1725\n",
      "Epoch 178/200, Loss: 0.1594\n",
      "Epoch 179/200, Loss: 0.1334\n",
      "Epoch 180/200, Loss: 0.1305\n",
      "Epoch 181/200, Loss: 0.1104\n",
      "Epoch 182/200, Loss: 0.1543\n",
      "Epoch 183/200, Loss: 0.1302\n",
      "Epoch 184/200, Loss: 0.1422\n",
      "Epoch 185/200, Loss: 0.1233\n",
      "Epoch 186/200, Loss: 0.1196\n",
      "Epoch 187/200, Loss: 0.1089\n",
      "Epoch 188/200, Loss: 0.1171\n",
      "Epoch 189/200, Loss: 0.1995\n",
      "Epoch 190/200, Loss: 0.1296\n",
      "Epoch 191/200, Loss: 0.1193\n",
      "Epoch 192/200, Loss: 0.1272\n",
      "Epoch 193/200, Loss: 0.1202\n",
      "Epoch 194/200, Loss: 0.1400\n",
      "Epoch 195/200, Loss: 0.1610\n",
      "Epoch 196/200, Loss: 0.1517\n",
      "Epoch 197/200, Loss: 0.1280\n",
      "Epoch 198/200, Loss: 0.1235\n",
      "Epoch 199/200, Loss: 0.1241\n",
      "Epoch 200/200, Loss: 0.1040\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 800x600 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Define the MultiModal Model\n",
    "class MultiModalModel(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim, embedding_dim):\n",
    "        super(MultiModalModel, self).__init__()\n",
    "        self.fc1 = nn.Linear(input_dim, hidden_dim)\n",
    "        self.fc2 = nn.Linear(hidden_dim, embedding_dim)\n",
    "        self.fc3 = nn.Linear(embedding_dim, 1)  # Output a single value\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = torch.relu(self.fc1(x))\n",
    "        x = torch.relu(self.fc2(x))\n",
    "        x = self.fc3(x)\n",
    "        return x\n",
    "\n",
    "# Set input_dim to match the aligned features\n",
    "input_dim = train_features.shape[1]  # Number of features in the input\n",
    "hidden_dim = 128  # Size of the hidden layer\n",
    "embedding_dim = 64  # Size of the embedding layer\n",
    "learning_rate = 0.001  # Learning rate\n",
    "num_epochs = 200  # Number of training epochs\n",
    "\n",
    "# Instantiate the model\n",
    "model = MultiModalModel(input_dim, hidden_dim, embedding_dim)\n",
    "\n",
    "# Define RMSRE as the loss function\n",
    "def rmsre(y_pred, y_true):\n",
    "    epsilon = 1e-8  # Small value to avoid division by zero\n",
    "    loss = torch.sqrt(torch.mean(((y_true - y_pred) / (y_true + epsilon)) ** 2))\n",
    "    return loss\n",
    "\n",
    "# Optimizer\n",
    "optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
    "\n",
    "# Training the model\n",
    "train_losses = []\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    epoch_loss = 0\n",
    "    for features, labels in train_loader:\n",
    "        optimizer.zero_grad()\n",
    "        predictions = model(features).squeeze()\n",
    "        # Ensure labels are positive to avoid issues with RMSRE\n",
    "        labels = torch.clamp(labels, min=1e-8)\n",
    "        loss = rmsre(predictions, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        epoch_loss += loss.item()\n",
    "    train_losses.append(epoch_loss / len(train_loader))\n",
    "    print(f\"Epoch {epoch+1}/{num_epochs}, Loss: {train_losses[-1]:.4f}\")\n",
    "\n",
    "# Plot the training curve\n",
    "plt.figure(figsize=(8, 6))\n",
    "plt.plot(range(1, num_epochs + 1), train_losses, marker='o')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Training RMSRE')\n",
    "plt.title('Training Curve')\n",
    "plt.grid()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training with hidden_dim=64, lr=0.001\n",
      "Epoch 1/500, Train Loss: 0.5337, Val Loss: 0.4873\n",
      "Epoch 2/500, Train Loss: 0.4168, Val Loss: 0.4618\n",
      "Epoch 3/500, Train Loss: 0.3625, Val Loss: 0.4783\n",
      "Epoch 4/500, Train Loss: 0.3561, Val Loss: 0.4649\n",
      "Epoch 5/500, Train Loss: 0.3641, Val Loss: 0.4619\n",
      "Epoch 6/500, Train Loss: 0.3407, Val Loss: 0.4536\n",
      "Epoch 7/500, Train Loss: 0.3259, Val Loss: 0.4748\n",
      "Epoch 8/500, Train Loss: 0.3792, Val Loss: 0.5036\n",
      "Epoch 9/500, Train Loss: 0.3691, Val Loss: 0.4426\n",
      "Epoch 10/500, Train Loss: 0.3426, Val Loss: 0.4602\n",
      "Epoch 11/500, Train Loss: 0.3583, Val Loss: 0.4521\n",
      "Epoch 12/500, Train Loss: 0.3370, Val Loss: 0.4502\n",
      "Epoch 13/500, Train Loss: 0.3247, Val Loss: 0.4533\n",
      "Epoch 14/500, Train Loss: 0.3082, Val Loss: 0.4579\n",
      "Epoch 15/500, Train Loss: 0.3104, Val Loss: 0.4620\n",
      "Epoch 16/500, Train Loss: 0.3370, Val Loss: 0.4685\n",
      "Epoch 17/500, Train Loss: 0.2769, Val Loss: 0.4824\n",
      "Epoch 18/500, Train Loss: 0.3013, Val Loss: 0.5268\n",
      "Epoch 19/500, Train Loss: 0.3570, Val Loss: 0.4849\n",
      "Epoch 20/500, Train Loss: 0.3070, Val Loss: 0.4734\n",
      "Epoch 21/500, Train Loss: 0.3212, Val Loss: 0.4886\n",
      "Epoch 22/500, Train Loss: 0.3199, Val Loss: 0.5110\n",
      "Epoch 23/500, Train Loss: 0.3277, Val Loss: 0.4836\n",
      "Epoch 24/500, Train Loss: 0.3118, Val Loss: 0.4808\n",
      "Epoch 25/500, Train Loss: 0.2991, Val Loss: 0.5164\n",
      "Epoch 26/500, Train Loss: 0.3364, Val Loss: 0.5054\n",
      "Epoch 27/500, Train Loss: 0.3324, Val Loss: 0.4987\n",
      "Epoch 28/500, Train Loss: 0.3271, Val Loss: 0.4677\n",
      "Epoch 29/500, Train Loss: 0.2892, Val Loss: 0.4829\n",
      "Epoch 30/500, Train Loss: 0.2838, Val Loss: 0.5004\n",
      "Epoch 31/500, Train Loss: 0.3155, Val Loss: 0.4710\n",
      "Epoch 32/500, Train Loss: 0.2772, Val Loss: 0.4783\n",
      "Epoch 33/500, Train Loss: 0.3014, Val Loss: 0.5427\n",
      "Epoch 34/500, Train Loss: 0.3600, Val Loss: 0.5305\n",
      "Epoch 35/500, Train Loss: 0.3030, Val Loss: 0.5169\n",
      "Epoch 36/500, Train Loss: 0.3428, Val Loss: 0.4869\n",
      "Epoch 37/500, Train Loss: 0.3484, Val Loss: 0.4782\n",
      "Epoch 38/500, Train Loss: 0.3323, Val Loss: 0.4608\n",
      "Epoch 39/500, Train Loss: 0.3439, Val Loss: 0.4775\n",
      "Epoch 40/500, Train Loss: 0.2982, Val Loss: 0.4858\n",
      "Epoch 41/500, Train Loss: 0.3092, Val Loss: 0.4788\n",
      "Epoch 42/500, Train Loss: 0.2901, Val Loss: 0.4792\n",
      "Epoch 43/500, Train Loss: 0.2653, Val Loss: 0.4846\n",
      "Epoch 44/500, Train Loss: 0.3024, Val Loss: 0.4828\n",
      "Epoch 45/500, Train Loss: 0.2820, Val Loss: 0.4938\n",
      "Epoch 46/500, Train Loss: 0.2871, Val Loss: 0.4926\n",
      "Epoch 47/500, Train Loss: 0.2509, Val Loss: 0.5153\n",
      "Epoch 48/500, Train Loss: 0.2937, Val Loss: 0.5029\n",
      "Epoch 49/500, Train Loss: 0.2562, Val Loss: 0.5056\n",
      "Epoch 50/500, Train Loss: 0.2828, Val Loss: 0.5009\n",
      "Epoch 51/500, Train Loss: 0.2553, Val Loss: 0.4990\n",
      "Epoch 52/500, Train Loss: 0.2600, Val Loss: 0.4959\n",
      "Epoch 53/500, Train Loss: 0.2588, Val Loss: 0.5060\n",
      "Epoch 54/500, Train Loss: 0.2498, Val Loss: 0.5656\n",
      "Epoch 55/500, Train Loss: 0.3865, Val Loss: 0.4724\n",
      "Epoch 56/500, Train Loss: 0.2842, Val Loss: 0.4915\n",
      "Epoch 57/500, Train Loss: 0.2471, Val Loss: 0.5040\n",
      "Epoch 58/500, Train Loss: 0.2987, Val Loss: 0.4910\n",
      "Epoch 59/500, Train Loss: 0.2611, Val Loss: 0.4977\n",
      "Epoch 60/500, Train Loss: 0.2629, Val Loss: 0.4939\n",
      "Epoch 61/500, Train Loss: 0.2719, Val Loss: 0.4919\n",
      "Epoch 62/500, Train Loss: 0.2360, Val Loss: 0.4987\n",
      "Epoch 63/500, Train Loss: 0.2162, Val Loss: 0.4960\n",
      "Epoch 64/500, Train Loss: 0.2458, Val Loss: 0.4928\n",
      "Epoch 65/500, Train Loss: 0.2216, Val Loss: 0.5166\n",
      "Epoch 66/500, Train Loss: 0.2736, Val Loss: 0.5060\n",
      "Epoch 67/500, Train Loss: 0.2301, Val Loss: 0.4921\n",
      "Epoch 68/500, Train Loss: 0.2579, Val Loss: 0.5600\n",
      "Epoch 69/500, Train Loss: 0.2603, Val Loss: 0.4972\n",
      "Epoch 70/500, Train Loss: 0.2354, Val Loss: 0.5106\n",
      "Epoch 71/500, Train Loss: 0.2199, Val Loss: 0.5119\n",
      "Epoch 72/500, Train Loss: 0.2250, Val Loss: 0.4895\n",
      "Epoch 73/500, Train Loss: 0.2317, Val Loss: 0.5060\n",
      "Epoch 74/500, Train Loss: 0.2135, Val Loss: 0.4961\n",
      "Epoch 75/500, Train Loss: 0.2180, Val Loss: 0.5024\n",
      "Epoch 76/500, Train Loss: 0.2896, Val Loss: 0.5305\n",
      "Epoch 77/500, Train Loss: 0.3263, Val Loss: 0.4953\n",
      "Epoch 78/500, Train Loss: 0.2279, Val Loss: 0.4930\n",
      "Epoch 79/500, Train Loss: 0.2279, Val Loss: 0.5051\n",
      "Epoch 80/500, Train Loss: 0.2118, Val Loss: 0.5102\n",
      "Epoch 81/500, Train Loss: 0.2337, Val Loss: 0.4804\n",
      "Epoch 82/500, Train Loss: 0.2688, Val Loss: 0.4987\n",
      "Epoch 83/500, Train Loss: 0.2810, Val Loss: 0.4779\n",
      "Epoch 84/500, Train Loss: 0.2401, Val Loss: 0.4995\n",
      "Epoch 85/500, Train Loss: 0.2609, Val Loss: 0.4949\n",
      "Epoch 86/500, Train Loss: 0.2410, Val Loss: 0.5106\n",
      "Epoch 87/500, Train Loss: 0.2696, Val Loss: 0.4679\n",
      "Epoch 88/500, Train Loss: 0.2515, Val Loss: 0.4871\n",
      "Epoch 89/500, Train Loss: 0.2272, Val Loss: 0.5192\n",
      "Epoch 90/500, Train Loss: 0.2275, Val Loss: 0.4890\n",
      "Epoch 91/500, Train Loss: 0.1937, Val Loss: 0.5122\n",
      "Epoch 92/500, Train Loss: 0.2279, Val Loss: 0.4912\n",
      "Epoch 93/500, Train Loss: 0.2118, Val Loss: 0.5046\n",
      "Epoch 94/500, Train Loss: 0.1998, Val Loss: 0.5041\n",
      "Epoch 95/500, Train Loss: 0.1959, Val Loss: 0.4961\n",
      "Epoch 96/500, Train Loss: 0.1866, Val Loss: 0.4912\n",
      "Epoch 97/500, Train Loss: 0.2002, Val Loss: 0.5033\n",
      "Epoch 98/500, Train Loss: 0.1854, Val Loss: 0.5135\n",
      "Epoch 99/500, Train Loss: 0.2193, Val Loss: 0.5050\n",
      "Epoch 100/500, Train Loss: 0.1762, Val Loss: 0.5562\n",
      "Epoch 101/500, Train Loss: 0.2225, Val Loss: 0.5027\n",
      "Epoch 102/500, Train Loss: 0.1977, Val Loss: 0.5295\n",
      "Epoch 103/500, Train Loss: 0.2145, Val Loss: 0.5396\n",
      "Epoch 104/500, Train Loss: 0.3263, Val Loss: 0.5179\n",
      "Epoch 105/500, Train Loss: 0.2460, Val Loss: 0.5039\n",
      "Epoch 106/500, Train Loss: 0.2056, Val Loss: 0.4950\n",
      "Epoch 107/500, Train Loss: 0.1817, Val Loss: 0.5033\n",
      "Epoch 108/500, Train Loss: 0.1916, Val Loss: 0.4963\n",
      "Epoch 109/500, Train Loss: 0.1947, Val Loss: 0.4924\n",
      "Epoch 110/500, Train Loss: 0.1512, Val Loss: 0.4953\n",
      "Epoch 111/500, Train Loss: 0.1703, Val Loss: 0.4898\n",
      "Epoch 112/500, Train Loss: 0.1636, Val Loss: 0.4912\n",
      "Epoch 113/500, Train Loss: 0.1845, Val Loss: 0.5358\n",
      "Epoch 114/500, Train Loss: 0.2819, Val Loss: 0.4842\n",
      "Epoch 115/500, Train Loss: 0.2144, Val Loss: 0.4777\n",
      "Epoch 116/500, Train Loss: 0.2111, Val Loss: 0.5069\n",
      "Epoch 117/500, Train Loss: 0.1707, Val Loss: 0.5677\n",
      "Epoch 118/500, Train Loss: 0.2259, Val Loss: 0.4940\n",
      "Epoch 119/500, Train Loss: 0.1533, Val Loss: 0.5177\n",
      "Epoch 120/500, Train Loss: 0.1706, Val Loss: 0.5018\n",
      "Epoch 121/500, Train Loss: 0.1812, Val Loss: 0.4893\n",
      "Epoch 122/500, Train Loss: 0.1543, Val Loss: 0.5037\n",
      "Epoch 123/500, Train Loss: 0.1453, Val Loss: 0.5352\n",
      "Epoch 124/500, Train Loss: 0.1999, Val Loss: 0.4808\n",
      "Epoch 125/500, Train Loss: 0.1729, Val Loss: 0.4785\n",
      "Epoch 126/500, Train Loss: 0.1758, Val Loss: 0.5036\n",
      "Epoch 127/500, Train Loss: 0.1556, Val Loss: 0.4949\n",
      "Epoch 128/500, Train Loss: 0.2407, Val Loss: 0.4889\n",
      "Epoch 129/500, Train Loss: 0.1597, Val Loss: 0.5373\n",
      "Epoch 130/500, Train Loss: 0.1539, Val Loss: 0.4758\n",
      "Epoch 131/500, Train Loss: 0.1528, Val Loss: 0.5220\n",
      "Epoch 132/500, Train Loss: 0.2642, Val Loss: 0.4945\n",
      "Epoch 133/500, Train Loss: 0.1994, Val Loss: 0.5055\n",
      "Epoch 134/500, Train Loss: 0.1480, Val Loss: 0.4964\n",
      "Epoch 135/500, Train Loss: 0.2462, Val Loss: 0.4632\n",
      "Epoch 136/500, Train Loss: 0.1487, Val Loss: 0.4834\n",
      "Epoch 137/500, Train Loss: 0.2046, Val Loss: 0.4824\n",
      "Epoch 138/500, Train Loss: 0.1491, Val Loss: 0.4825\n",
      "Epoch 139/500, Train Loss: 0.1604, Val Loss: 0.5433\n",
      "Epoch 140/500, Train Loss: 0.1884, Val Loss: 0.4894\n",
      "Epoch 141/500, Train Loss: 0.1800, Val Loss: 0.4797\n",
      "Epoch 142/500, Train Loss: 0.1714, Val Loss: 0.5168\n",
      "Epoch 143/500, Train Loss: 0.2221, Val Loss: 0.5200\n",
      "Epoch 144/500, Train Loss: 0.1815, Val Loss: 0.4924\n",
      "Epoch 145/500, Train Loss: 0.1630, Val Loss: 0.4999\n",
      "Epoch 146/500, Train Loss: 0.1492, Val Loss: 0.4791\n",
      "Epoch 147/500, Train Loss: 0.1433, Val Loss: 0.4774\n",
      "Epoch 148/500, Train Loss: 0.1591, Val Loss: 0.5859\n",
      "Epoch 149/500, Train Loss: 0.2053, Val Loss: 0.4723\n",
      "Epoch 150/500, Train Loss: 0.2458, Val Loss: 0.4808\n",
      "Epoch 151/500, Train Loss: 0.1984, Val Loss: 0.6793\n",
      "Epoch 152/500, Train Loss: 0.3853, Val Loss: 0.4334\n",
      "Epoch 153/500, Train Loss: 0.3469, Val Loss: 0.4517\n",
      "Epoch 154/500, Train Loss: 0.3146, Val Loss: 0.4402\n",
      "Epoch 155/500, Train Loss: 0.2621, Val Loss: 0.4518\n",
      "Epoch 156/500, Train Loss: 0.1898, Val Loss: 0.4917\n",
      "Epoch 157/500, Train Loss: 0.1808, Val Loss: 0.4931\n",
      "Epoch 158/500, Train Loss: 0.2043, Val Loss: 0.4834\n",
      "Epoch 159/500, Train Loss: 0.1917, Val Loss: 0.4891\n",
      "Epoch 160/500, Train Loss: 0.1678, Val Loss: 0.5266\n",
      "Epoch 161/500, Train Loss: 0.1815, Val Loss: 0.4755\n",
      "Epoch 162/500, Train Loss: 0.1435, Val Loss: 0.4746\n",
      "Epoch 163/500, Train Loss: 0.2195, Val Loss: 0.4823\n",
      "Epoch 164/500, Train Loss: 0.1994, Val Loss: 0.4805\n",
      "Epoch 165/500, Train Loss: 0.2159, Val Loss: 0.4871\n",
      "Epoch 166/500, Train Loss: 0.1671, Val Loss: 0.4924\n",
      "Epoch 167/500, Train Loss: 0.2347, Val Loss: 0.4765\n",
      "Epoch 168/500, Train Loss: 0.2395, Val Loss: 0.4743\n",
      "Epoch 169/500, Train Loss: 0.1944, Val Loss: 0.4989\n",
      "Epoch 170/500, Train Loss: 0.1674, Val Loss: 0.4868\n",
      "Epoch 171/500, Train Loss: 0.2624, Val Loss: 0.4763\n",
      "Epoch 172/500, Train Loss: 0.1603, Val Loss: 0.4895\n",
      "Epoch 173/500, Train Loss: 0.1770, Val Loss: 0.4784\n",
      "Epoch 174/500, Train Loss: 0.1520, Val Loss: 0.4797\n",
      "Epoch 175/500, Train Loss: 0.1741, Val Loss: 0.4730\n",
      "Epoch 176/500, Train Loss: 0.1519, Val Loss: 0.4727\n",
      "Epoch 177/500, Train Loss: 0.1596, Val Loss: 0.4746\n",
      "Epoch 178/500, Train Loss: 0.1530, Val Loss: 0.4829\n",
      "Epoch 179/500, Train Loss: 0.1319, Val Loss: 0.4947\n",
      "Epoch 180/500, Train Loss: 0.1460, Val Loss: 0.4753\n",
      "Epoch 181/500, Train Loss: 0.1304, Val Loss: 0.4803\n",
      "Epoch 182/500, Train Loss: 0.2166, Val Loss: 0.4879\n",
      "Epoch 183/500, Train Loss: 0.2050, Val Loss: 0.4618\n",
      "Epoch 184/500, Train Loss: 0.1527, Val Loss: 0.5421\n",
      "Epoch 185/500, Train Loss: 0.1413, Val Loss: 0.4870\n",
      "Epoch 186/500, Train Loss: 0.1222, Val Loss: 0.4842\n",
      "Epoch 187/500, Train Loss: 0.1405, Val Loss: 0.4946\n",
      "Epoch 188/500, Train Loss: 0.1744, Val Loss: 0.4979\n",
      "Epoch 189/500, Train Loss: 0.1577, Val Loss: 0.4698\n",
      "Epoch 190/500, Train Loss: 0.1903, Val Loss: 0.4731\n",
      "Epoch 191/500, Train Loss: 0.2003, Val Loss: 0.4634\n",
      "Epoch 192/500, Train Loss: 0.1664, Val Loss: 0.4915\n",
      "Epoch 193/500, Train Loss: 0.1461, Val Loss: 0.4648\n",
      "Epoch 194/500, Train Loss: 0.1418, Val Loss: 0.5095\n",
      "Epoch 195/500, Train Loss: 0.2104, Val Loss: 0.4867\n",
      "Epoch 196/500, Train Loss: 0.1471, Val Loss: 0.5141\n",
      "Epoch 197/500, Train Loss: 0.1323, Val Loss: 0.4928\n",
      "Epoch 198/500, Train Loss: 0.1157, Val Loss: 0.4845\n",
      "Epoch 199/500, Train Loss: 0.1207, Val Loss: 0.4831\n",
      "Epoch 200/500, Train Loss: 0.1766, Val Loss: 0.4796\n",
      "Epoch 201/500, Train Loss: 0.1556, Val Loss: 0.5455\n",
      "Epoch 202/500, Train Loss: 0.1728, Val Loss: 0.5109\n",
      "Epoch 203/500, Train Loss: 0.1658, Val Loss: 0.5118\n",
      "Epoch 204/500, Train Loss: 0.1413, Val Loss: 0.4866\n",
      "Epoch 205/500, Train Loss: 0.1288, Val Loss: 0.4806\n",
      "Epoch 206/500, Train Loss: 0.1327, Val Loss: 0.4688\n",
      "Epoch 207/500, Train Loss: 0.1623, Val Loss: 0.4825\n",
      "Epoch 208/500, Train Loss: 0.1599, Val Loss: 0.4832\n",
      "Epoch 209/500, Train Loss: 0.1226, Val Loss: 0.4746\n",
      "Epoch 210/500, Train Loss: 0.1269, Val Loss: 0.5033\n",
      "Epoch 211/500, Train Loss: 0.1259, Val Loss: 0.5262\n",
      "Epoch 212/500, Train Loss: 0.1595, Val Loss: 0.4629\n",
      "Epoch 213/500, Train Loss: 0.1401, Val Loss: 0.4943\n",
      "Epoch 214/500, Train Loss: 0.1344, Val Loss: 0.4705\n",
      "Epoch 215/500, Train Loss: 0.1911, Val Loss: 0.4767\n",
      "Epoch 216/500, Train Loss: 0.1259, Val Loss: 0.4749\n",
      "Epoch 217/500, Train Loss: 0.1857, Val Loss: 0.4813\n",
      "Epoch 218/500, Train Loss: 0.1833, Val Loss: 0.4781\n",
      "Epoch 219/500, Train Loss: 0.1338, Val Loss: 0.4798\n",
      "Epoch 220/500, Train Loss: 0.1635, Val Loss: 0.4724\n",
      "Epoch 221/500, Train Loss: 0.1291, Val Loss: 0.4801\n",
      "Epoch 222/500, Train Loss: 0.1137, Val Loss: 0.4832\n",
      "Epoch 223/500, Train Loss: 0.1293, Val Loss: 0.4656\n",
      "Epoch 224/500, Train Loss: 0.1514, Val Loss: 0.4596\n",
      "Epoch 225/500, Train Loss: 0.1681, Val Loss: 0.5440\n",
      "Epoch 226/500, Train Loss: 0.1685, Val Loss: 0.4588\n",
      "Epoch 227/500, Train Loss: 0.1635, Val Loss: 0.4910\n",
      "Epoch 228/500, Train Loss: 0.2253, Val Loss: 0.4876\n",
      "Epoch 229/500, Train Loss: 0.1642, Val Loss: 0.4867\n",
      "Epoch 230/500, Train Loss: 0.1146, Val Loss: 0.4864\n",
      "Epoch 231/500, Train Loss: 0.1107, Val Loss: 0.4693\n",
      "Epoch 232/500, Train Loss: 0.1404, Val Loss: 0.4603\n",
      "Epoch 233/500, Train Loss: 0.1953, Val Loss: 0.4767\n",
      "Epoch 234/500, Train Loss: 0.1407, Val Loss: 0.4979\n",
      "Epoch 235/500, Train Loss: 0.1201, Val Loss: 0.4657\n",
      "Epoch 236/500, Train Loss: 0.1306, Val Loss: 0.5334\n",
      "Epoch 237/500, Train Loss: 0.1235, Val Loss: 0.4714\n",
      "Epoch 238/500, Train Loss: 0.1196, Val Loss: 0.4640\n",
      "Epoch 239/500, Train Loss: 0.1453, Val Loss: 0.4694\n",
      "Epoch 240/500, Train Loss: 0.1148, Val Loss: 0.4849\n",
      "Epoch 241/500, Train Loss: 0.1173, Val Loss: 0.4795\n",
      "Epoch 242/500, Train Loss: 0.1159, Val Loss: 0.4979\n",
      "Epoch 243/500, Train Loss: 0.1268, Val Loss: 0.4650\n",
      "Epoch 244/500, Train Loss: 0.1112, Val Loss: 0.5256\n",
      "Epoch 245/500, Train Loss: 0.1533, Val Loss: 0.4862\n",
      "Epoch 246/500, Train Loss: 0.1219, Val Loss: 0.5024\n",
      "Epoch 247/500, Train Loss: 0.1375, Val Loss: 0.5029\n",
      "Epoch 248/500, Train Loss: 0.1271, Val Loss: 0.4654\n",
      "Epoch 249/500, Train Loss: 0.1054, Val Loss: 0.4829\n",
      "Epoch 250/500, Train Loss: 0.1237, Val Loss: 0.4791\n",
      "Epoch 251/500, Train Loss: 0.1309, Val Loss: 0.4744\n",
      "Epoch 252/500, Train Loss: 0.1053, Val Loss: 0.4639\n",
      "Epoch 253/500, Train Loss: 0.1219, Val Loss: 0.5035\n",
      "Epoch 254/500, Train Loss: 0.1397, Val Loss: 0.4526\n",
      "Epoch 255/500, Train Loss: 0.1650, Val Loss: 0.4547\n",
      "Epoch 256/500, Train Loss: 0.1290, Val Loss: 0.5327\n",
      "Epoch 257/500, Train Loss: 0.1147, Val Loss: 0.4864\n",
      "Epoch 258/500, Train Loss: 0.1104, Val Loss: 0.4613\n",
      "Epoch 259/500, Train Loss: 0.1531, Val Loss: 0.4864\n",
      "Epoch 260/500, Train Loss: 0.1299, Val Loss: 0.4819\n",
      "Epoch 261/500, Train Loss: 0.1470, Val Loss: 0.4788\n",
      "Epoch 262/500, Train Loss: 0.1470, Val Loss: 0.5532\n",
      "Epoch 263/500, Train Loss: 0.1630, Val Loss: 0.4867\n",
      "Epoch 264/500, Train Loss: 0.1359, Val Loss: 0.5039\n",
      "Epoch 265/500, Train Loss: 0.1065, Val Loss: 0.4767\n",
      "Epoch 266/500, Train Loss: 0.1580, Val Loss: 0.4713\n",
      "Epoch 267/500, Train Loss: 0.1485, Val Loss: 0.5156\n",
      "Epoch 268/500, Train Loss: 0.1360, Val Loss: 0.4763\n",
      "Epoch 269/500, Train Loss: 0.1359, Val Loss: 0.4721\n",
      "Epoch 270/500, Train Loss: 0.1334, Val Loss: 0.5311\n",
      "Epoch 271/500, Train Loss: 0.1504, Val Loss: 0.4935\n",
      "Epoch 272/500, Train Loss: 0.1486, Val Loss: 0.5559\n",
      "Epoch 273/500, Train Loss: 0.1594, Val Loss: 0.5075\n",
      "Epoch 274/500, Train Loss: 0.1315, Val Loss: 0.4917\n",
      "Epoch 275/500, Train Loss: 0.1958, Val Loss: 0.4638\n",
      "Epoch 276/500, Train Loss: 0.1222, Val Loss: 0.4869\n",
      "Epoch 277/500, Train Loss: 0.1334, Val Loss: 0.4978\n",
      "Epoch 278/500, Train Loss: 0.1364, Val Loss: 0.4879\n",
      "Epoch 279/500, Train Loss: 0.1197, Val Loss: 0.4919\n",
      "Epoch 280/500, Train Loss: 0.1022, Val Loss: 0.5028\n",
      "Epoch 281/500, Train Loss: 0.1102, Val Loss: 0.5168\n",
      "Epoch 282/500, Train Loss: 0.1259, Val Loss: 0.4894\n",
      "Epoch 283/500, Train Loss: 0.1190, Val Loss: 0.5672\n",
      "Epoch 284/500, Train Loss: 0.1757, Val Loss: 0.4950\n",
      "Epoch 285/500, Train Loss: 0.1299, Val Loss: 0.4740\n",
      "Epoch 286/500, Train Loss: 0.1248, Val Loss: 0.4842\n",
      "Epoch 287/500, Train Loss: 0.1068, Val Loss: 0.5058\n",
      "Epoch 288/500, Train Loss: 0.1271, Val Loss: 0.4682\n",
      "Epoch 289/500, Train Loss: 0.1040, Val Loss: 0.4795\n",
      "Epoch 290/500, Train Loss: 0.0898, Val Loss: 0.4717\n",
      "Epoch 291/500, Train Loss: 0.1010, Val Loss: 0.4813\n",
      "Epoch 292/500, Train Loss: 0.0973, Val Loss: 0.4740\n",
      "Epoch 293/500, Train Loss: 0.1191, Val Loss: 0.4953\n",
      "Epoch 294/500, Train Loss: 0.1144, Val Loss: 0.5011\n",
      "Epoch 295/500, Train Loss: 0.1216, Val Loss: 0.4789\n",
      "Epoch 296/500, Train Loss: 0.1240, Val Loss: 0.4982\n",
      "Epoch 297/500, Train Loss: 0.1298, Val Loss: 0.4762\n",
      "Epoch 298/500, Train Loss: 0.1168, Val Loss: 0.4826\n",
      "Epoch 299/500, Train Loss: 0.1302, Val Loss: 0.4726\n",
      "Epoch 300/500, Train Loss: 0.1631, Val Loss: 0.4891\n",
      "Epoch 301/500, Train Loss: 0.1766, Val Loss: 0.4812\n",
      "Epoch 302/500, Train Loss: 0.1598, Val Loss: 0.5008\n",
      "Epoch 303/500, Train Loss: 0.1107, Val Loss: 0.4862\n",
      "Epoch 304/500, Train Loss: 0.1163, Val Loss: 0.4934\n",
      "Epoch 305/500, Train Loss: 0.1020, Val Loss: 0.4805\n",
      "Epoch 306/500, Train Loss: 0.1230, Val Loss: 0.4596\n",
      "Epoch 307/500, Train Loss: 0.0983, Val Loss: 0.4747\n",
      "Epoch 308/500, Train Loss: 0.1123, Val Loss: 0.4733\n",
      "Epoch 309/500, Train Loss: 0.1009, Val Loss: 0.4804\n",
      "Epoch 310/500, Train Loss: 0.1076, Val Loss: 0.5089\n",
      "Epoch 311/500, Train Loss: 0.1207, Val Loss: 0.4593\n",
      "Epoch 312/500, Train Loss: 0.1276, Val Loss: 0.4851\n",
      "Epoch 313/500, Train Loss: 0.0955, Val Loss: 0.4798\n",
      "Epoch 314/500, Train Loss: 0.0927, Val Loss: 0.4619\n",
      "Epoch 315/500, Train Loss: 0.1225, Val Loss: 0.4889\n",
      "Epoch 316/500, Train Loss: 0.1182, Val Loss: 0.4704\n",
      "Epoch 317/500, Train Loss: 0.0994, Val Loss: 0.4952\n",
      "Epoch 318/500, Train Loss: 0.1119, Val Loss: 0.4862\n",
      "Epoch 319/500, Train Loss: 0.0953, Val Loss: 0.4833\n",
      "Epoch 320/500, Train Loss: 0.0865, Val Loss: 0.5071\n",
      "Epoch 321/500, Train Loss: 0.1130, Val Loss: 0.4584\n",
      "Epoch 322/500, Train Loss: 0.1033, Val Loss: 0.4654\n",
      "Epoch 323/500, Train Loss: 0.1452, Val Loss: 0.4792\n",
      "Epoch 324/500, Train Loss: 0.1264, Val Loss: 0.4617\n",
      "Epoch 325/500, Train Loss: 0.1511, Val Loss: 0.4797\n",
      "Epoch 326/500, Train Loss: 0.1207, Val Loss: 0.4982\n",
      "Epoch 327/500, Train Loss: 0.0909, Val Loss: 0.5273\n",
      "Epoch 328/500, Train Loss: 0.1526, Val Loss: 0.4972\n",
      "Epoch 329/500, Train Loss: 0.1029, Val Loss: 0.4753\n",
      "Epoch 330/500, Train Loss: 0.1006, Val Loss: 0.4834\n",
      "Epoch 331/500, Train Loss: 0.0828, Val Loss: 0.4750\n",
      "Epoch 332/500, Train Loss: 0.0981, Val Loss: 0.4900\n",
      "Epoch 333/500, Train Loss: 0.0940, Val Loss: 0.4815\n",
      "Epoch 334/500, Train Loss: 0.0931, Val Loss: 0.4693\n",
      "Epoch 335/500, Train Loss: 0.1146, Val Loss: 0.4965\n",
      "Epoch 336/500, Train Loss: 0.1125, Val Loss: 0.4880\n",
      "Epoch 337/500, Train Loss: 0.1252, Val Loss: 0.4764\n",
      "Epoch 338/500, Train Loss: 0.1268, Val Loss: 0.5206\n",
      "Epoch 339/500, Train Loss: 0.1302, Val Loss: 0.4753\n",
      "Epoch 340/500, Train Loss: 0.0912, Val Loss: 0.4807\n",
      "Epoch 341/500, Train Loss: 0.0863, Val Loss: 0.4743\n",
      "Epoch 342/500, Train Loss: 0.1335, Val Loss: 0.5467\n",
      "Epoch 343/500, Train Loss: 0.1860, Val Loss: 0.5297\n",
      "Epoch 344/500, Train Loss: 0.1251, Val Loss: 0.4621\n",
      "Epoch 345/500, Train Loss: 0.1122, Val Loss: 0.5124\n",
      "Epoch 346/500, Train Loss: 0.1322, Val Loss: 0.4657\n",
      "Epoch 347/500, Train Loss: 0.2079, Val Loss: 0.5517\n",
      "Epoch 348/500, Train Loss: 0.1662, Val Loss: 0.5010\n",
      "Epoch 349/500, Train Loss: 0.1082, Val Loss: 0.4905\n",
      "Epoch 350/500, Train Loss: 0.0883, Val Loss: 0.4780\n",
      "Epoch 351/500, Train Loss: 0.0929, Val Loss: 0.4598\n",
      "Epoch 352/500, Train Loss: 0.1585, Val Loss: 0.4835\n",
      "Epoch 353/500, Train Loss: 0.1107, Val Loss: 0.4983\n",
      "Epoch 354/500, Train Loss: 0.0980, Val Loss: 0.4724\n",
      "Epoch 355/500, Train Loss: 0.1008, Val Loss: 0.4878\n",
      "Epoch 356/500, Train Loss: 0.1057, Val Loss: 0.4739\n",
      "Epoch 357/500, Train Loss: 0.1016, Val Loss: 0.4702\n",
      "Epoch 358/500, Train Loss: 0.1222, Val Loss: 0.4847\n",
      "Epoch 359/500, Train Loss: 0.1243, Val Loss: 0.4824\n",
      "Epoch 360/500, Train Loss: 0.1232, Val Loss: 0.4774\n",
      "Epoch 361/500, Train Loss: 0.1290, Val Loss: 0.5058\n",
      "Epoch 362/500, Train Loss: 0.0981, Val Loss: 0.4904\n",
      "Epoch 363/500, Train Loss: 0.1079, Val Loss: 0.5369\n",
      "Epoch 364/500, Train Loss: 0.1482, Val Loss: 0.4707\n",
      "Epoch 365/500, Train Loss: 0.1040, Val Loss: 0.4793\n",
      "Epoch 366/500, Train Loss: 0.1084, Val Loss: 0.4913\n",
      "Epoch 367/500, Train Loss: 0.0885, Val Loss: 0.4898\n",
      "Epoch 368/500, Train Loss: 0.1260, Val Loss: 0.4709\n",
      "Epoch 369/500, Train Loss: 0.1307, Val Loss: 0.4732\n",
      "Epoch 370/500, Train Loss: 0.1027, Val Loss: 0.5472\n",
      "Epoch 371/500, Train Loss: 0.1053, Val Loss: 0.4903\n",
      "Epoch 372/500, Train Loss: 0.0961, Val Loss: 0.4932\n",
      "Epoch 373/500, Train Loss: 0.0995, Val Loss: 0.4875\n",
      "Epoch 374/500, Train Loss: 0.1020, Val Loss: 0.4923\n",
      "Epoch 375/500, Train Loss: 0.0913, Val Loss: 0.4921\n",
      "Epoch 376/500, Train Loss: 0.0913, Val Loss: 0.4719\n",
      "Epoch 377/500, Train Loss: 0.0895, Val Loss: 0.4996\n",
      "Epoch 378/500, Train Loss: 0.0979, Val Loss: 0.4684\n",
      "Epoch 379/500, Train Loss: 0.1289, Val Loss: 0.4639\n",
      "Epoch 380/500, Train Loss: 0.1535, Val Loss: 0.4933\n",
      "Epoch 381/500, Train Loss: 0.1353, Val Loss: 0.5607\n",
      "Epoch 382/500, Train Loss: 0.1632, Val Loss: 0.4903\n",
      "Epoch 383/500, Train Loss: 0.1464, Val Loss: 0.5040\n",
      "Epoch 384/500, Train Loss: 0.1262, Val Loss: 0.4837\n",
      "Epoch 385/500, Train Loss: 0.1347, Val Loss: 0.5003\n",
      "Epoch 386/500, Train Loss: 0.0912, Val Loss: 0.4735\n",
      "Epoch 387/500, Train Loss: 0.1233, Val Loss: 0.4637\n",
      "Epoch 388/500, Train Loss: 0.1028, Val Loss: 0.4799\n",
      "Epoch 389/500, Train Loss: 0.1159, Val Loss: 0.4778\n",
      "Epoch 390/500, Train Loss: 0.1119, Val Loss: 0.4780\n",
      "Epoch 391/500, Train Loss: 0.0891, Val Loss: 0.5053\n",
      "Epoch 392/500, Train Loss: 0.0935, Val Loss: 0.4971\n",
      "Epoch 393/500, Train Loss: 0.1357, Val Loss: 0.4630\n",
      "Epoch 394/500, Train Loss: 0.1776, Val Loss: 0.5124\n",
      "Epoch 395/500, Train Loss: 0.1359, Val Loss: 0.4807\n",
      "Epoch 396/500, Train Loss: 0.1107, Val Loss: 0.5354\n",
      "Epoch 397/500, Train Loss: 0.1079, Val Loss: 0.4821\n",
      "Epoch 398/500, Train Loss: 0.0950, Val Loss: 0.4697\n",
      "Epoch 399/500, Train Loss: 0.0977, Val Loss: 0.5003\n",
      "Epoch 400/500, Train Loss: 0.1518, Val Loss: 0.4878\n",
      "Epoch 401/500, Train Loss: 0.1479, Val Loss: 0.4869\n",
      "Epoch 402/500, Train Loss: 0.1179, Val Loss: 0.4825\n",
      "Epoch 403/500, Train Loss: 0.1051, Val Loss: 0.5101\n",
      "Epoch 404/500, Train Loss: 0.1193, Val Loss: 0.4652\n",
      "Epoch 405/500, Train Loss: 0.1411, Val Loss: 0.4810\n",
      "Epoch 406/500, Train Loss: 0.1536, Val Loss: 0.4812\n",
      "Epoch 407/500, Train Loss: 0.1179, Val Loss: 0.5128\n",
      "Epoch 408/500, Train Loss: 0.0947, Val Loss: 0.4822\n",
      "Epoch 409/500, Train Loss: 0.1283, Val Loss: 0.4799\n",
      "Epoch 410/500, Train Loss: 0.1101, Val Loss: 0.4969\n",
      "Epoch 411/500, Train Loss: 0.1297, Val Loss: 0.5047\n",
      "Epoch 412/500, Train Loss: 0.1051, Val Loss: 0.4964\n",
      "Epoch 413/500, Train Loss: 0.0898, Val Loss: 0.4916\n",
      "Epoch 414/500, Train Loss: 0.0906, Val Loss: 0.4822\n",
      "Epoch 415/500, Train Loss: 0.0849, Val Loss: 0.5238\n",
      "Epoch 416/500, Train Loss: 0.1084, Val Loss: 0.4760\n",
      "Epoch 417/500, Train Loss: 0.1895, Val Loss: 0.4598\n",
      "Epoch 418/500, Train Loss: 0.1091, Val Loss: 0.4754\n",
      "Epoch 419/500, Train Loss: 0.1196, Val Loss: 0.4738\n",
      "Epoch 420/500, Train Loss: 0.1127, Val Loss: 0.4968\n",
      "Epoch 421/500, Train Loss: 0.1179, Val Loss: 0.4884\n",
      "Epoch 422/500, Train Loss: 0.0964, Val Loss: 0.4772\n",
      "Epoch 423/500, Train Loss: 0.0966, Val Loss: 0.4946\n",
      "Epoch 424/500, Train Loss: 0.0931, Val Loss: 0.4693\n",
      "Epoch 425/500, Train Loss: 0.1029, Val Loss: 0.4792\n",
      "Epoch 426/500, Train Loss: 0.1019, Val Loss: 0.4825\n",
      "Epoch 427/500, Train Loss: 0.0915, Val Loss: 0.5249\n",
      "Epoch 428/500, Train Loss: 0.1113, Val Loss: 0.4810\n",
      "Epoch 429/500, Train Loss: 0.0877, Val Loss: 0.4927\n",
      "Epoch 430/500, Train Loss: 0.0915, Val Loss: 0.4907\n",
      "Epoch 431/500, Train Loss: 0.1046, Val Loss: 0.5019\n",
      "Epoch 432/500, Train Loss: 0.0913, Val Loss: 0.4812\n",
      "Epoch 433/500, Train Loss: 0.0966, Val Loss: 0.4637\n",
      "Epoch 434/500, Train Loss: 0.1025, Val Loss: 0.4966\n",
      "Epoch 435/500, Train Loss: 0.0958, Val Loss: 0.4687\n",
      "Epoch 436/500, Train Loss: 0.1037, Val Loss: 0.5130\n",
      "Epoch 437/500, Train Loss: 0.1024, Val Loss: 0.4703\n",
      "Epoch 438/500, Train Loss: 0.1117, Val Loss: 0.5328\n",
      "Epoch 439/500, Train Loss: 0.1016, Val Loss: 0.4575\n",
      "Epoch 440/500, Train Loss: 0.1269, Val Loss: 0.4674\n",
      "Epoch 441/500, Train Loss: 0.1029, Val Loss: 0.4791\n",
      "Epoch 442/500, Train Loss: 0.0970, Val Loss: 0.4985\n",
      "Epoch 443/500, Train Loss: 0.1206, Val Loss: 0.4801\n",
      "Epoch 444/500, Train Loss: 0.0996, Val Loss: 0.5305\n",
      "Epoch 445/500, Train Loss: 0.1393, Val Loss: 0.5373\n",
      "Epoch 446/500, Train Loss: 0.1201, Val Loss: 0.4960\n",
      "Epoch 447/500, Train Loss: 0.0828, Val Loss: 0.4898\n",
      "Epoch 448/500, Train Loss: 0.0965, Val Loss: 0.4843\n",
      "Epoch 449/500, Train Loss: 0.1208, Val Loss: 0.4924\n",
      "Epoch 450/500, Train Loss: 0.1456, Val Loss: 0.4676\n",
      "Epoch 451/500, Train Loss: 0.1373, Val Loss: 0.4978\n",
      "Epoch 452/500, Train Loss: 0.0994, Val Loss: 0.4706\n",
      "Epoch 453/500, Train Loss: 0.0930, Val Loss: 0.4955\n",
      "Epoch 454/500, Train Loss: 0.1456, Val Loss: 0.4689\n",
      "Epoch 455/500, Train Loss: 0.0920, Val Loss: 0.4752\n",
      "Epoch 456/500, Train Loss: 0.1044, Val Loss: 0.4947\n",
      "Epoch 457/500, Train Loss: 0.0993, Val Loss: 0.4763\n",
      "Epoch 458/500, Train Loss: 0.1142, Val Loss: 0.5042\n",
      "Epoch 459/500, Train Loss: 0.1218, Val Loss: 0.5019\n",
      "Epoch 460/500, Train Loss: 0.0914, Val Loss: 0.4720\n",
      "Epoch 461/500, Train Loss: 0.1624, Val Loss: 0.4567\n",
      "Epoch 462/500, Train Loss: 0.1041, Val Loss: 0.5032\n",
      "Epoch 463/500, Train Loss: 0.1085, Val Loss: 0.4906\n",
      "Epoch 464/500, Train Loss: 0.1082, Val Loss: 0.4641\n",
      "Epoch 465/500, Train Loss: 0.1166, Val Loss: 0.5319\n",
      "Epoch 466/500, Train Loss: 0.1441, Val Loss: 0.4787\n",
      "Epoch 467/500, Train Loss: 0.1092, Val Loss: 0.4851\n",
      "Epoch 468/500, Train Loss: 0.0847, Val Loss: 0.4756\n",
      "Epoch 469/500, Train Loss: 0.1004, Val Loss: 0.4622\n",
      "Epoch 470/500, Train Loss: 0.1013, Val Loss: 0.4695\n",
      "Epoch 471/500, Train Loss: 0.0853, Val Loss: 0.4753\n",
      "Epoch 472/500, Train Loss: 0.0856, Val Loss: 0.4623\n",
      "Epoch 473/500, Train Loss: 0.0805, Val Loss: 0.4689\n",
      "Epoch 474/500, Train Loss: 0.1072, Val Loss: 0.4676\n",
      "Epoch 475/500, Train Loss: 0.0856, Val Loss: 0.5025\n",
      "Epoch 476/500, Train Loss: 0.1176, Val Loss: 0.4857\n",
      "Epoch 477/500, Train Loss: 0.1122, Val Loss: 0.4515\n",
      "Epoch 478/500, Train Loss: 0.0974, Val Loss: 0.5076\n",
      "Epoch 479/500, Train Loss: 0.1014, Val Loss: 0.4715\n",
      "Epoch 480/500, Train Loss: 0.1196, Val Loss: 0.5279\n",
      "Epoch 481/500, Train Loss: 0.1124, Val Loss: 0.4974\n",
      "Epoch 482/500, Train Loss: 0.1158, Val Loss: 0.4786\n",
      "Epoch 483/500, Train Loss: 0.0970, Val Loss: 0.4822\n",
      "Epoch 484/500, Train Loss: 0.0931, Val Loss: 0.4529\n",
      "Epoch 485/500, Train Loss: 0.1481, Val Loss: 0.4405\n",
      "Epoch 486/500, Train Loss: 0.1329, Val Loss: 0.5241\n",
      "Epoch 487/500, Train Loss: 0.1321, Val Loss: 0.4513\n",
      "Epoch 488/500, Train Loss: 0.1195, Val Loss: 0.4659\n",
      "Epoch 489/500, Train Loss: 0.1113, Val Loss: 0.4892\n",
      "Epoch 490/500, Train Loss: 0.0901, Val Loss: 0.4856\n",
      "Epoch 491/500, Train Loss: 0.0948, Val Loss: 0.4716\n",
      "Epoch 492/500, Train Loss: 0.0748, Val Loss: 0.4821\n",
      "Epoch 493/500, Train Loss: 0.0901, Val Loss: 0.4730\n",
      "Epoch 494/500, Train Loss: 0.0874, Val Loss: 0.4717\n",
      "Epoch 495/500, Train Loss: 0.1006, Val Loss: 0.4632\n",
      "Epoch 496/500, Train Loss: 0.1077, Val Loss: 0.4705\n",
      "Epoch 497/500, Train Loss: 0.0814, Val Loss: 0.4665\n",
      "Epoch 498/500, Train Loss: 0.0860, Val Loss: 0.4975\n",
      "Epoch 499/500, Train Loss: 0.1069, Val Loss: 0.4880\n",
      "Epoch 500/500, Train Loss: 0.0884, Val Loss: 0.5136\n",
      "Training with hidden_dim=64, lr=0.0005\n",
      "Epoch 1/500, Train Loss: 0.4646, Val Loss: 0.4420\n",
      "Epoch 2/500, Train Loss: 0.3701, Val Loss: 0.4419\n",
      "Epoch 3/500, Train Loss: 0.3393, Val Loss: 0.4583\n",
      "Epoch 4/500, Train Loss: 0.3884, Val Loss: 0.4432\n",
      "Epoch 5/500, Train Loss: 0.3560, Val Loss: 0.4500\n",
      "Epoch 6/500, Train Loss: 0.3451, Val Loss: 0.4520\n",
      "Epoch 7/500, Train Loss: 0.3242, Val Loss: 0.4521\n",
      "Epoch 8/500, Train Loss: 0.3208, Val Loss: 0.4920\n",
      "Epoch 9/500, Train Loss: 0.3461, Val Loss: 0.4626\n",
      "Epoch 10/500, Train Loss: 0.3544, Val Loss: 0.4557\n",
      "Epoch 11/500, Train Loss: 0.3428, Val Loss: 0.4529\n",
      "Epoch 12/500, Train Loss: 0.3117, Val Loss: 0.4649\n",
      "Epoch 13/500, Train Loss: 0.3467, Val Loss: 0.5217\n",
      "Epoch 14/500, Train Loss: 0.3546, Val Loss: 0.4575\n",
      "Epoch 15/500, Train Loss: 0.3488, Val Loss: 0.4621\n",
      "Epoch 16/500, Train Loss: 0.3290, Val Loss: 0.4644\n",
      "Epoch 17/500, Train Loss: 0.3306, Val Loss: 0.4831\n",
      "Epoch 18/500, Train Loss: 0.3297, Val Loss: 0.4712\n",
      "Epoch 19/500, Train Loss: 0.3073, Val Loss: 0.4760\n",
      "Epoch 20/500, Train Loss: 0.3368, Val Loss: 0.4814\n",
      "Epoch 21/500, Train Loss: 0.3215, Val Loss: 0.4771\n",
      "Epoch 22/500, Train Loss: 0.3153, Val Loss: 0.4730\n",
      "Epoch 23/500, Train Loss: 0.3023, Val Loss: 0.5139\n",
      "Epoch 24/500, Train Loss: 0.3137, Val Loss: 0.4639\n",
      "Epoch 25/500, Train Loss: 0.3203, Val Loss: 0.4677\n",
      "Epoch 26/500, Train Loss: 0.3204, Val Loss: 0.4801\n",
      "Epoch 27/500, Train Loss: 0.3089, Val Loss: 0.4759\n",
      "Epoch 28/500, Train Loss: 0.3213, Val Loss: 0.4910\n",
      "Epoch 29/500, Train Loss: 0.2922, Val Loss: 0.5001\n",
      "Epoch 30/500, Train Loss: 0.2985, Val Loss: 0.4880\n",
      "Epoch 31/500, Train Loss: 0.2895, Val Loss: 0.5039\n",
      "Epoch 32/500, Train Loss: 0.3083, Val Loss: 0.4965\n",
      "Epoch 33/500, Train Loss: 0.3307, Val Loss: 0.4748\n",
      "Epoch 34/500, Train Loss: 0.3245, Val Loss: 0.4533\n",
      "Epoch 35/500, Train Loss: 0.2912, Val Loss: 0.4647\n",
      "Epoch 36/500, Train Loss: 0.2862, Val Loss: 0.4805\n",
      "Epoch 37/500, Train Loss: 0.3246, Val Loss: 0.4792\n",
      "Epoch 38/500, Train Loss: 0.3117, Val Loss: 0.4640\n",
      "Epoch 39/500, Train Loss: 0.2788, Val Loss: 0.4795\n",
      "Epoch 40/500, Train Loss: 0.2804, Val Loss: 0.4855\n",
      "Epoch 41/500, Train Loss: 0.2628, Val Loss: 0.4919\n",
      "Epoch 42/500, Train Loss: 0.2686, Val Loss: 0.5438\n",
      "Epoch 43/500, Train Loss: 0.3550, Val Loss: 0.5104\n",
      "Epoch 44/500, Train Loss: 0.3065, Val Loss: 0.4729\n",
      "Epoch 45/500, Train Loss: 0.2962, Val Loss: 0.4753\n",
      "Epoch 46/500, Train Loss: 0.2991, Val Loss: 0.4950\n",
      "Epoch 47/500, Train Loss: 0.2709, Val Loss: 0.4954\n",
      "Epoch 48/500, Train Loss: 0.3063, Val Loss: 0.4840\n",
      "Epoch 49/500, Train Loss: 0.2956, Val Loss: 0.4857\n",
      "Epoch 50/500, Train Loss: 0.2805, Val Loss: 0.5162\n",
      "Epoch 51/500, Train Loss: 0.3000, Val Loss: 0.4951\n",
      "Epoch 52/500, Train Loss: 0.3001, Val Loss: 0.4914\n",
      "Epoch 53/500, Train Loss: 0.2862, Val Loss: 0.4942\n",
      "Epoch 54/500, Train Loss: 0.2763, Val Loss: 0.4971\n",
      "Epoch 55/500, Train Loss: 0.2824, Val Loss: 0.4964\n",
      "Epoch 56/500, Train Loss: 0.2767, Val Loss: 0.4980\n",
      "Epoch 57/500, Train Loss: 0.2511, Val Loss: 0.4988\n",
      "Epoch 58/500, Train Loss: 0.2791, Val Loss: 0.5005\n",
      "Epoch 59/500, Train Loss: 0.2827, Val Loss: 0.4911\n",
      "Epoch 60/500, Train Loss: 0.2770, Val Loss: 0.4982\n",
      "Epoch 61/500, Train Loss: 0.2698, Val Loss: 0.5401\n",
      "Epoch 62/500, Train Loss: 0.3250, Val Loss: 0.4820\n",
      "Epoch 63/500, Train Loss: 0.2664, Val Loss: 0.5112\n",
      "Epoch 64/500, Train Loss: 0.2461, Val Loss: 0.5034\n",
      "Epoch 65/500, Train Loss: 0.2644, Val Loss: 0.4889\n",
      "Epoch 66/500, Train Loss: 0.2482, Val Loss: 0.5049\n",
      "Epoch 67/500, Train Loss: 0.2436, Val Loss: 0.4963\n",
      "Epoch 68/500, Train Loss: 0.2264, Val Loss: 0.4994\n",
      "Epoch 69/500, Train Loss: 0.2223, Val Loss: 0.5093\n",
      "Epoch 70/500, Train Loss: 0.2942, Val Loss: 0.4943\n",
      "Epoch 71/500, Train Loss: 0.2385, Val Loss: 0.5027\n",
      "Epoch 72/500, Train Loss: 0.2664, Val Loss: 0.5021\n",
      "Epoch 73/500, Train Loss: 0.2418, Val Loss: 0.5037\n",
      "Epoch 74/500, Train Loss: 0.2306, Val Loss: 0.5493\n",
      "Epoch 75/500, Train Loss: 0.3047, Val Loss: 0.4877\n",
      "Epoch 76/500, Train Loss: 0.2752, Val Loss: 0.5144\n",
      "Epoch 77/500, Train Loss: 0.2441, Val Loss: 0.4990\n",
      "Epoch 78/500, Train Loss: 0.2129, Val Loss: 0.4923\n",
      "Epoch 79/500, Train Loss: 0.2022, Val Loss: 0.5190\n",
      "Epoch 80/500, Train Loss: 0.2431, Val Loss: 0.5097\n",
      "Epoch 81/500, Train Loss: 0.2320, Val Loss: 0.5021\n",
      "Epoch 82/500, Train Loss: 0.2373, Val Loss: 0.5139\n",
      "Epoch 83/500, Train Loss: 0.2383, Val Loss: 0.5441\n",
      "Epoch 84/500, Train Loss: 0.2430, Val Loss: 0.4917\n",
      "Epoch 85/500, Train Loss: 0.2242, Val Loss: 0.5008\n",
      "Epoch 86/500, Train Loss: 0.2385, Val Loss: 0.5049\n",
      "Epoch 87/500, Train Loss: 0.2699, Val Loss: 0.4855\n",
      "Epoch 88/500, Train Loss: 0.2165, Val Loss: 0.5103\n",
      "Epoch 89/500, Train Loss: 0.2405, Val Loss: 0.4928\n",
      "Epoch 90/500, Train Loss: 0.2077, Val Loss: 0.5176\n",
      "Epoch 91/500, Train Loss: 0.2378, Val Loss: 0.5265\n",
      "Epoch 92/500, Train Loss: 0.2324, Val Loss: 0.4982\n",
      "Epoch 93/500, Train Loss: 0.2393, Val Loss: 0.5406\n",
      "Epoch 94/500, Train Loss: 0.2008, Val Loss: 0.5112\n",
      "Epoch 95/500, Train Loss: 0.2159, Val Loss: 0.5182\n",
      "Epoch 96/500, Train Loss: 0.2212, Val Loss: 0.5024\n",
      "Epoch 97/500, Train Loss: 0.2237, Val Loss: 0.5014\n",
      "Epoch 98/500, Train Loss: 0.2080, Val Loss: 0.5031\n",
      "Epoch 99/500, Train Loss: 0.1929, Val Loss: 0.5050\n",
      "Epoch 100/500, Train Loss: 0.2059, Val Loss: 0.5046\n",
      "Epoch 101/500, Train Loss: 0.1909, Val Loss: 0.4914\n",
      "Epoch 102/500, Train Loss: 0.2262, Val Loss: 0.5061\n",
      "Epoch 103/500, Train Loss: 0.1914, Val Loss: 0.5573\n",
      "Epoch 104/500, Train Loss: 0.2416, Val Loss: 0.4987\n",
      "Epoch 105/500, Train Loss: 0.1829, Val Loss: 0.5114\n",
      "Epoch 106/500, Train Loss: 0.1752, Val Loss: 0.4993\n",
      "Epoch 107/500, Train Loss: 0.2009, Val Loss: 0.5115\n",
      "Epoch 108/500, Train Loss: 0.1771, Val Loss: 0.5245\n",
      "Epoch 109/500, Train Loss: 0.2057, Val Loss: 0.4990\n",
      "Epoch 110/500, Train Loss: 0.1793, Val Loss: 0.4932\n",
      "Epoch 111/500, Train Loss: 0.2325, Val Loss: 0.4882\n",
      "Epoch 112/500, Train Loss: 0.1904, Val Loss: 0.5082\n",
      "Epoch 113/500, Train Loss: 0.1714, Val Loss: 0.4853\n",
      "Epoch 114/500, Train Loss: 0.2103, Val Loss: 0.4901\n",
      "Epoch 115/500, Train Loss: 0.1768, Val Loss: 0.5300\n",
      "Epoch 116/500, Train Loss: 0.1950, Val Loss: 0.4847\n",
      "Epoch 117/500, Train Loss: 0.1606, Val Loss: 0.4931\n",
      "Epoch 118/500, Train Loss: 0.1651, Val Loss: 0.4890\n",
      "Epoch 119/500, Train Loss: 0.2021, Val Loss: 0.4954\n",
      "Epoch 120/500, Train Loss: 0.1797, Val Loss: 0.4902\n",
      "Epoch 121/500, Train Loss: 0.2360, Val Loss: 0.5052\n",
      "Epoch 122/500, Train Loss: 0.2249, Val Loss: 0.5058\n",
      "Epoch 123/500, Train Loss: 0.2038, Val Loss: 0.5055\n",
      "Epoch 124/500, Train Loss: 0.1880, Val Loss: 0.5124\n",
      "Epoch 125/500, Train Loss: 0.1637, Val Loss: 0.5292\n",
      "Epoch 126/500, Train Loss: 0.1789, Val Loss: 0.4944\n",
      "Epoch 127/500, Train Loss: 0.1912, Val Loss: 0.4966\n",
      "Epoch 128/500, Train Loss: 0.2123, Val Loss: 0.5250\n",
      "Epoch 129/500, Train Loss: 0.1615, Val Loss: 0.4949\n",
      "Epoch 130/500, Train Loss: 0.1745, Val Loss: 0.5042\n",
      "Epoch 131/500, Train Loss: 0.2012, Val Loss: 0.5248\n",
      "Epoch 132/500, Train Loss: 0.1737, Val Loss: 0.4959\n",
      "Epoch 133/500, Train Loss: 0.1446, Val Loss: 0.4866\n",
      "Epoch 134/500, Train Loss: 0.1894, Val Loss: 0.4891\n",
      "Epoch 135/500, Train Loss: 0.1760, Val Loss: 0.5122\n",
      "Epoch 136/500, Train Loss: 0.1787, Val Loss: 0.4885\n",
      "Epoch 137/500, Train Loss: 0.1652, Val Loss: 0.5145\n",
      "Epoch 138/500, Train Loss: 0.1717, Val Loss: 0.4880\n",
      "Epoch 139/500, Train Loss: 0.1820, Val Loss: 0.5074\n",
      "Epoch 140/500, Train Loss: 0.1729, Val Loss: 0.5319\n",
      "Epoch 141/500, Train Loss: 0.1614, Val Loss: 0.5071\n",
      "Epoch 142/500, Train Loss: 0.2099, Val Loss: 0.5158\n",
      "Epoch 143/500, Train Loss: 0.2558, Val Loss: 0.4830\n",
      "Epoch 144/500, Train Loss: 0.1762, Val Loss: 0.4905\n",
      "Epoch 145/500, Train Loss: 0.1749, Val Loss: 0.5064\n",
      "Epoch 146/500, Train Loss: 0.1801, Val Loss: 0.6232\n",
      "Epoch 147/500, Train Loss: 0.2493, Val Loss: 0.5341\n",
      "Epoch 148/500, Train Loss: 0.2083, Val Loss: 0.5521\n",
      "Epoch 149/500, Train Loss: 0.1991, Val Loss: 0.4837\n",
      "Epoch 150/500, Train Loss: 0.1603, Val Loss: 0.5077\n",
      "Epoch 151/500, Train Loss: 0.1538, Val Loss: 0.5065\n",
      "Epoch 152/500, Train Loss: 0.2857, Val Loss: 0.4589\n",
      "Epoch 153/500, Train Loss: 0.2201, Val Loss: 0.5179\n",
      "Epoch 154/500, Train Loss: 0.2750, Val Loss: 0.5223\n",
      "Epoch 155/500, Train Loss: 0.2219, Val Loss: 0.5005\n",
      "Epoch 156/500, Train Loss: 0.2168, Val Loss: 0.5584\n",
      "Epoch 157/500, Train Loss: 0.2422, Val Loss: 0.5071\n",
      "Epoch 158/500, Train Loss: 0.1827, Val Loss: 0.4954\n",
      "Epoch 159/500, Train Loss: 0.2032, Val Loss: 0.4802\n",
      "Epoch 160/500, Train Loss: 0.1838, Val Loss: 0.5006\n",
      "Epoch 161/500, Train Loss: 0.1364, Val Loss: 0.5042\n",
      "Epoch 162/500, Train Loss: 0.1615, Val Loss: 0.5018\n",
      "Epoch 163/500, Train Loss: 0.1862, Val Loss: 0.4986\n",
      "Epoch 164/500, Train Loss: 0.1618, Val Loss: 0.5149\n",
      "Epoch 165/500, Train Loss: 0.1521, Val Loss: 0.5034\n",
      "Epoch 166/500, Train Loss: 0.1562, Val Loss: 0.5032\n",
      "Epoch 167/500, Train Loss: 0.1524, Val Loss: 0.5529\n",
      "Epoch 168/500, Train Loss: 0.1827, Val Loss: 0.5274\n",
      "Epoch 169/500, Train Loss: 0.1527, Val Loss: 0.5168\n",
      "Epoch 170/500, Train Loss: 0.1464, Val Loss: 0.5172\n",
      "Epoch 171/500, Train Loss: 0.1458, Val Loss: 0.4967\n",
      "Epoch 172/500, Train Loss: 0.1546, Val Loss: 0.4929\n",
      "Epoch 173/500, Train Loss: 0.1491, Val Loss: 0.5649\n",
      "Epoch 174/500, Train Loss: 0.1861, Val Loss: 0.5354\n",
      "Epoch 175/500, Train Loss: 0.1535, Val Loss: 0.5131\n",
      "Epoch 176/500, Train Loss: 0.1371, Val Loss: 0.5036\n",
      "Epoch 177/500, Train Loss: 0.1303, Val Loss: 0.5040\n",
      "Epoch 178/500, Train Loss: 0.1319, Val Loss: 0.5272\n",
      "Epoch 179/500, Train Loss: 0.1408, Val Loss: 0.4970\n",
      "Epoch 180/500, Train Loss: 0.1447, Val Loss: 0.5140\n",
      "Epoch 181/500, Train Loss: 0.1357, Val Loss: 0.5305\n",
      "Epoch 182/500, Train Loss: 0.1394, Val Loss: 0.5119\n",
      "Epoch 183/500, Train Loss: 0.1355, Val Loss: 0.5647\n",
      "Epoch 184/500, Train Loss: 0.1650, Val Loss: 0.4979\n",
      "Epoch 185/500, Train Loss: 0.1520, Val Loss: 0.5067\n",
      "Epoch 186/500, Train Loss: 0.1910, Val Loss: 0.4996\n",
      "Epoch 187/500, Train Loss: 0.1814, Val Loss: 0.4760\n",
      "Epoch 188/500, Train Loss: 0.1355, Val Loss: 0.4829\n",
      "Epoch 189/500, Train Loss: 0.1299, Val Loss: 0.4783\n",
      "Epoch 190/500, Train Loss: 0.1489, Val Loss: 0.4928\n",
      "Epoch 191/500, Train Loss: 0.1341, Val Loss: 0.4874\n",
      "Epoch 192/500, Train Loss: 0.1524, Val Loss: 0.4984\n",
      "Epoch 193/500, Train Loss: 0.1445, Val Loss: 0.5706\n",
      "Epoch 194/500, Train Loss: 0.1768, Val Loss: 0.5172\n",
      "Epoch 195/500, Train Loss: 0.1500, Val Loss: 0.4945\n",
      "Epoch 196/500, Train Loss: 0.1544, Val Loss: 0.4892\n",
      "Epoch 197/500, Train Loss: 0.1319, Val Loss: 0.5070\n",
      "Epoch 198/500, Train Loss: 0.1490, Val Loss: 0.4931\n",
      "Epoch 199/500, Train Loss: 0.1630, Val Loss: 0.4888\n",
      "Epoch 200/500, Train Loss: 0.1518, Val Loss: 0.4937\n",
      "Epoch 201/500, Train Loss: 0.1343, Val Loss: 0.4984\n",
      "Epoch 202/500, Train Loss: 0.1297, Val Loss: 0.5163\n",
      "Epoch 203/500, Train Loss: 0.1493, Val Loss: 0.5194\n",
      "Epoch 204/500, Train Loss: 0.1347, Val Loss: 0.4785\n",
      "Epoch 205/500, Train Loss: 0.1260, Val Loss: 0.4827\n",
      "Epoch 206/500, Train Loss: 0.1176, Val Loss: 0.4982\n",
      "Epoch 207/500, Train Loss: 0.1265, Val Loss: 0.4887\n",
      "Epoch 208/500, Train Loss: 0.1167, Val Loss: 0.5070\n",
      "Epoch 209/500, Train Loss: 0.1193, Val Loss: 0.5198\n",
      "Epoch 210/500, Train Loss: 0.1452, Val Loss: 0.5278\n",
      "Epoch 211/500, Train Loss: 0.1406, Val Loss: 0.4781\n",
      "Epoch 212/500, Train Loss: 0.1625, Val Loss: 0.4855\n",
      "Epoch 213/500, Train Loss: 0.1338, Val Loss: 0.5033\n",
      "Epoch 214/500, Train Loss: 0.1162, Val Loss: 0.4862\n",
      "Epoch 215/500, Train Loss: 0.1321, Val Loss: 0.5084\n",
      "Epoch 216/500, Train Loss: 0.1350, Val Loss: 0.4942\n",
      "Epoch 217/500, Train Loss: 0.1287, Val Loss: 0.5142\n",
      "Epoch 218/500, Train Loss: 0.1168, Val Loss: 0.4913\n",
      "Epoch 219/500, Train Loss: 0.1208, Val Loss: 0.5057\n",
      "Epoch 220/500, Train Loss: 0.1237, Val Loss: 0.5142\n",
      "Epoch 221/500, Train Loss: 0.1261, Val Loss: 0.4886\n",
      "Epoch 222/500, Train Loss: 0.1386, Val Loss: 0.5404\n",
      "Epoch 223/500, Train Loss: 0.1555, Val Loss: 0.5009\n",
      "Epoch 224/500, Train Loss: 0.1205, Val Loss: 0.5164\n",
      "Epoch 225/500, Train Loss: 0.1225, Val Loss: 0.4818\n",
      "Epoch 226/500, Train Loss: 0.1255, Val Loss: 0.5059\n",
      "Epoch 227/500, Train Loss: 0.1131, Val Loss: 0.4801\n",
      "Epoch 228/500, Train Loss: 0.1270, Val Loss: 0.4876\n",
      "Epoch 229/500, Train Loss: 0.1105, Val Loss: 0.4975\n",
      "Epoch 230/500, Train Loss: 0.1107, Val Loss: 0.4847\n",
      "Epoch 231/500, Train Loss: 0.1567, Val Loss: 0.5042\n",
      "Epoch 232/500, Train Loss: 0.1097, Val Loss: 0.4894\n",
      "Epoch 233/500, Train Loss: 0.1477, Val Loss: 0.5326\n",
      "Epoch 234/500, Train Loss: 0.1442, Val Loss: 0.4764\n",
      "Epoch 235/500, Train Loss: 0.1213, Val Loss: 0.4903\n",
      "Epoch 236/500, Train Loss: 0.1044, Val Loss: 0.4801\n",
      "Epoch 237/500, Train Loss: 0.1411, Val Loss: 0.5044\n",
      "Epoch 238/500, Train Loss: 0.1440, Val Loss: 0.5458\n",
      "Epoch 239/500, Train Loss: 0.1508, Val Loss: 0.5004\n",
      "Epoch 240/500, Train Loss: 0.1482, Val Loss: 0.5049\n",
      "Epoch 241/500, Train Loss: 0.2104, Val Loss: 0.4990\n",
      "Epoch 242/500, Train Loss: 0.1711, Val Loss: 0.5082\n",
      "Epoch 243/500, Train Loss: 0.1358, Val Loss: 0.5097\n",
      "Epoch 244/500, Train Loss: 0.1349, Val Loss: 0.5438\n",
      "Epoch 245/500, Train Loss: 0.1886, Val Loss: 0.5183\n",
      "Epoch 246/500, Train Loss: 0.1510, Val Loss: 0.4723\n",
      "Epoch 247/500, Train Loss: 0.1659, Val Loss: 0.4809\n",
      "Epoch 248/500, Train Loss: 0.1297, Val Loss: 0.4969\n",
      "Epoch 249/500, Train Loss: 0.1623, Val Loss: 0.5071\n",
      "Epoch 250/500, Train Loss: 0.1274, Val Loss: 0.5228\n",
      "Epoch 251/500, Train Loss: 0.1497, Val Loss: 0.4975\n",
      "Epoch 252/500, Train Loss: 0.1110, Val Loss: 0.5146\n",
      "Epoch 253/500, Train Loss: 0.1029, Val Loss: 0.4866\n",
      "Epoch 254/500, Train Loss: 0.1394, Val Loss: 0.5223\n",
      "Epoch 255/500, Train Loss: 0.1366, Val Loss: 0.5115\n",
      "Epoch 256/500, Train Loss: 0.1161, Val Loss: 0.4828\n",
      "Epoch 257/500, Train Loss: 0.1398, Val Loss: 0.4988\n",
      "Epoch 258/500, Train Loss: 0.1219, Val Loss: 0.4921\n",
      "Epoch 259/500, Train Loss: 0.1148, Val Loss: 0.5132\n",
      "Epoch 260/500, Train Loss: 0.1033, Val Loss: 0.4776\n",
      "Epoch 261/500, Train Loss: 0.1229, Val Loss: 0.5076\n",
      "Epoch 262/500, Train Loss: 0.1238, Val Loss: 0.4858\n",
      "Epoch 263/500, Train Loss: 0.1698, Val Loss: 0.4989\n",
      "Epoch 264/500, Train Loss: 0.1497, Val Loss: 0.4958\n",
      "Epoch 265/500, Train Loss: 0.1351, Val Loss: 0.4960\n",
      "Epoch 266/500, Train Loss: 0.1058, Val Loss: 0.4911\n",
      "Epoch 267/500, Train Loss: 0.1297, Val Loss: 0.4845\n",
      "Epoch 268/500, Train Loss: 0.1214, Val Loss: 0.4910\n",
      "Epoch 269/500, Train Loss: 0.0985, Val Loss: 0.4888\n",
      "Epoch 270/500, Train Loss: 0.1110, Val Loss: 0.4852\n",
      "Epoch 271/500, Train Loss: 0.1524, Val Loss: 0.4818\n",
      "Epoch 272/500, Train Loss: 0.1925, Val Loss: 0.4989\n",
      "Epoch 273/500, Train Loss: 0.1437, Val Loss: 0.4829\n",
      "Epoch 274/500, Train Loss: 0.2042, Val Loss: 0.4780\n",
      "Epoch 275/500, Train Loss: 0.1176, Val Loss: 0.4894\n",
      "Epoch 276/500, Train Loss: 0.1075, Val Loss: 0.4783\n",
      "Epoch 277/500, Train Loss: 0.1161, Val Loss: 0.5308\n",
      "Epoch 278/500, Train Loss: 0.1396, Val Loss: 0.5123\n",
      "Epoch 279/500, Train Loss: 0.1247, Val Loss: 0.5027\n",
      "Epoch 280/500, Train Loss: 0.0993, Val Loss: 0.4739\n",
      "Epoch 281/500, Train Loss: 0.1445, Val Loss: 0.4802\n",
      "Epoch 282/500, Train Loss: 0.1201, Val Loss: 0.4933\n",
      "Epoch 283/500, Train Loss: 0.1008, Val Loss: 0.5075\n",
      "Epoch 284/500, Train Loss: 0.1123, Val Loss: 0.4965\n",
      "Epoch 285/500, Train Loss: 0.0995, Val Loss: 0.4887\n",
      "Epoch 286/500, Train Loss: 0.1013, Val Loss: 0.5213\n",
      "Epoch 287/500, Train Loss: 0.1048, Val Loss: 0.4966\n",
      "Epoch 288/500, Train Loss: 0.1002, Val Loss: 0.5098\n",
      "Epoch 289/500, Train Loss: 0.1245, Val Loss: 0.4792\n",
      "Epoch 290/500, Train Loss: 0.1317, Val Loss: 0.4735\n",
      "Epoch 291/500, Train Loss: 0.1231, Val Loss: 0.5095\n",
      "Epoch 292/500, Train Loss: 0.1015, Val Loss: 0.4996\n",
      "Epoch 293/500, Train Loss: 0.1100, Val Loss: 0.5029\n",
      "Epoch 294/500, Train Loss: 0.1354, Val Loss: 0.4729\n",
      "Epoch 295/500, Train Loss: 0.1166, Val Loss: 0.5017\n",
      "Epoch 296/500, Train Loss: 0.1074, Val Loss: 0.4872\n",
      "Epoch 297/500, Train Loss: 0.1133, Val Loss: 0.4948\n",
      "Epoch 298/500, Train Loss: 0.0931, Val Loss: 0.5190\n",
      "Epoch 299/500, Train Loss: 0.1074, Val Loss: 0.5040\n",
      "Epoch 300/500, Train Loss: 0.1030, Val Loss: 0.5110\n",
      "Epoch 301/500, Train Loss: 0.1115, Val Loss: 0.5443\n",
      "Epoch 302/500, Train Loss: 0.1331, Val Loss: 0.5123\n",
      "Epoch 303/500, Train Loss: 0.1070, Val Loss: 0.4890\n",
      "Epoch 304/500, Train Loss: 0.0962, Val Loss: 0.5034\n",
      "Epoch 305/500, Train Loss: 0.0968, Val Loss: 0.5013\n",
      "Epoch 306/500, Train Loss: 0.1122, Val Loss: 0.5012\n",
      "Epoch 307/500, Train Loss: 0.1007, Val Loss: 0.4960\n",
      "Epoch 308/500, Train Loss: 0.1078, Val Loss: 0.5020\n",
      "Epoch 309/500, Train Loss: 0.0983, Val Loss: 0.5108\n",
      "Epoch 310/500, Train Loss: 0.1321, Val Loss: 0.4882\n",
      "Epoch 311/500, Train Loss: 0.1040, Val Loss: 0.4866\n",
      "Epoch 312/500, Train Loss: 0.1200, Val Loss: 0.4974\n",
      "Epoch 313/500, Train Loss: 0.1074, Val Loss: 0.4915\n",
      "Epoch 314/500, Train Loss: 0.1196, Val Loss: 0.4797\n",
      "Epoch 315/500, Train Loss: 0.1187, Val Loss: 0.4921\n",
      "Epoch 316/500, Train Loss: 0.0987, Val Loss: 0.4837\n",
      "Epoch 317/500, Train Loss: 0.1156, Val Loss: 0.5092\n",
      "Epoch 318/500, Train Loss: 0.1161, Val Loss: 0.4895\n",
      "Epoch 319/500, Train Loss: 0.0957, Val Loss: 0.5015\n",
      "Epoch 320/500, Train Loss: 0.0969, Val Loss: 0.4992\n",
      "Epoch 321/500, Train Loss: 0.1200, Val Loss: 0.4890\n",
      "Epoch 322/500, Train Loss: 0.1590, Val Loss: 0.4836\n",
      "Epoch 323/500, Train Loss: 0.1451, Val Loss: 0.4983\n",
      "Epoch 324/500, Train Loss: 0.1196, Val Loss: 0.4938\n",
      "Epoch 325/500, Train Loss: 0.1237, Val Loss: 0.4923\n",
      "Epoch 326/500, Train Loss: 0.1554, Val Loss: 0.5074\n",
      "Epoch 327/500, Train Loss: 0.1880, Val Loss: 0.4980\n",
      "Epoch 328/500, Train Loss: 0.1511, Val Loss: 0.4749\n",
      "Epoch 329/500, Train Loss: 0.1133, Val Loss: 0.5215\n",
      "Epoch 330/500, Train Loss: 0.1004, Val Loss: 0.5327\n",
      "Epoch 331/500, Train Loss: 0.1098, Val Loss: 0.5145\n",
      "Epoch 332/500, Train Loss: 0.0936, Val Loss: 0.5210\n",
      "Epoch 333/500, Train Loss: 0.1150, Val Loss: 0.5061\n",
      "Epoch 334/500, Train Loss: 0.1057, Val Loss: 0.5210\n",
      "Epoch 335/500, Train Loss: 0.1089, Val Loss: 0.4824\n",
      "Epoch 336/500, Train Loss: 0.1559, Val Loss: 0.4859\n",
      "Epoch 337/500, Train Loss: 0.1053, Val Loss: 0.4911\n",
      "Epoch 338/500, Train Loss: 0.1225, Val Loss: 0.4903\n",
      "Epoch 339/500, Train Loss: 0.1022, Val Loss: 0.4985\n",
      "Epoch 340/500, Train Loss: 0.1007, Val Loss: 0.4998\n",
      "Epoch 341/500, Train Loss: 0.1077, Val Loss: 0.5065\n",
      "Epoch 342/500, Train Loss: 0.1093, Val Loss: 0.4848\n",
      "Epoch 343/500, Train Loss: 0.1163, Val Loss: 0.5297\n",
      "Epoch 344/500, Train Loss: 0.1522, Val Loss: 0.5262\n",
      "Epoch 345/500, Train Loss: 0.1939, Val Loss: 0.4851\n",
      "Epoch 346/500, Train Loss: 0.1408, Val Loss: 0.4847\n",
      "Epoch 347/500, Train Loss: 0.1191, Val Loss: 0.4998\n",
      "Epoch 348/500, Train Loss: 0.1107, Val Loss: 0.4793\n",
      "Epoch 349/500, Train Loss: 0.1041, Val Loss: 0.5057\n",
      "Epoch 350/500, Train Loss: 0.1160, Val Loss: 0.4805\n",
      "Epoch 351/500, Train Loss: 0.1203, Val Loss: 0.4789\n",
      "Epoch 352/500, Train Loss: 0.1198, Val Loss: 0.5199\n",
      "Epoch 353/500, Train Loss: 0.1472, Val Loss: 0.4947\n",
      "Epoch 354/500, Train Loss: 0.1402, Val Loss: 0.5169\n",
      "Epoch 355/500, Train Loss: 0.1307, Val Loss: 0.4906\n",
      "Epoch 356/500, Train Loss: 0.0867, Val Loss: 0.4968\n",
      "Epoch 357/500, Train Loss: 0.0921, Val Loss: 0.4860\n",
      "Epoch 358/500, Train Loss: 0.0922, Val Loss: 0.4919\n",
      "Epoch 359/500, Train Loss: 0.0932, Val Loss: 0.5005\n",
      "Epoch 360/500, Train Loss: 0.1005, Val Loss: 0.5180\n",
      "Epoch 361/500, Train Loss: 0.1092, Val Loss: 0.5069\n",
      "Epoch 362/500, Train Loss: 0.1275, Val Loss: 0.4938\n",
      "Epoch 363/500, Train Loss: 0.1180, Val Loss: 0.4942\n",
      "Epoch 364/500, Train Loss: 0.1272, Val Loss: 0.5005\n",
      "Epoch 365/500, Train Loss: 0.1263, Val Loss: 0.4973\n",
      "Epoch 366/500, Train Loss: 0.0973, Val Loss: 0.4794\n",
      "Epoch 367/500, Train Loss: 0.1077, Val Loss: 0.4810\n",
      "Epoch 368/500, Train Loss: 0.1100, Val Loss: 0.4868\n",
      "Epoch 369/500, Train Loss: 0.0975, Val Loss: 0.5001\n",
      "Epoch 370/500, Train Loss: 0.0861, Val Loss: 0.4837\n",
      "Epoch 371/500, Train Loss: 0.0946, Val Loss: 0.4837\n",
      "Epoch 372/500, Train Loss: 0.0960, Val Loss: 0.5038\n",
      "Epoch 373/500, Train Loss: 0.0893, Val Loss: 0.4926\n",
      "Epoch 374/500, Train Loss: 0.0992, Val Loss: 0.4977\n",
      "Epoch 375/500, Train Loss: 0.0943, Val Loss: 0.5219\n",
      "Epoch 376/500, Train Loss: 0.1157, Val Loss: 0.5069\n",
      "Epoch 377/500, Train Loss: 0.1297, Val Loss: 0.5007\n",
      "Epoch 378/500, Train Loss: 0.0936, Val Loss: 0.5123\n",
      "Epoch 379/500, Train Loss: 0.0967, Val Loss: 0.4844\n",
      "Epoch 380/500, Train Loss: 0.0913, Val Loss: 0.5275\n",
      "Epoch 381/500, Train Loss: 0.1158, Val Loss: 0.5408\n",
      "Epoch 382/500, Train Loss: 0.1308, Val Loss: 0.4749\n",
      "Epoch 383/500, Train Loss: 0.1131, Val Loss: 0.5051\n",
      "Epoch 384/500, Train Loss: 0.1054, Val Loss: 0.5036\n",
      "Epoch 385/500, Train Loss: 0.1144, Val Loss: 0.4831\n",
      "Epoch 386/500, Train Loss: 0.1057, Val Loss: 0.5367\n",
      "Epoch 387/500, Train Loss: 0.1430, Val Loss: 0.5237\n",
      "Epoch 388/500, Train Loss: 0.1459, Val Loss: 0.5144\n",
      "Epoch 389/500, Train Loss: 0.1150, Val Loss: 0.4858\n",
      "Epoch 390/500, Train Loss: 0.1236, Val Loss: 0.4985\n",
      "Epoch 391/500, Train Loss: 0.1078, Val Loss: 0.5062\n",
      "Epoch 392/500, Train Loss: 0.0913, Val Loss: 0.5002\n",
      "Epoch 393/500, Train Loss: 0.0875, Val Loss: 0.5053\n",
      "Epoch 394/500, Train Loss: 0.0855, Val Loss: 0.5099\n",
      "Epoch 395/500, Train Loss: 0.0937, Val Loss: 0.5038\n",
      "Epoch 396/500, Train Loss: 0.1085, Val Loss: 0.4983\n",
      "Epoch 397/500, Train Loss: 0.0961, Val Loss: 0.5124\n",
      "Epoch 398/500, Train Loss: 0.0847, Val Loss: 0.5273\n",
      "Epoch 399/500, Train Loss: 0.1152, Val Loss: 0.5180\n",
      "Epoch 400/500, Train Loss: 0.1007, Val Loss: 0.5218\n",
      "Epoch 401/500, Train Loss: 0.0875, Val Loss: 0.4872\n",
      "Epoch 402/500, Train Loss: 0.1471, Val Loss: 0.5155\n",
      "Epoch 403/500, Train Loss: 0.0985, Val Loss: 0.5100\n",
      "Epoch 404/500, Train Loss: 0.1133, Val Loss: 0.5003\n",
      "Epoch 405/500, Train Loss: 0.1027, Val Loss: 0.5510\n",
      "Epoch 406/500, Train Loss: 0.1082, Val Loss: 0.4861\n",
      "Epoch 407/500, Train Loss: 0.1029, Val Loss: 0.5150\n",
      "Epoch 408/500, Train Loss: 0.1075, Val Loss: 0.4985\n",
      "Epoch 409/500, Train Loss: 0.1113, Val Loss: 0.4727\n",
      "Epoch 410/500, Train Loss: 0.1075, Val Loss: 0.5394\n",
      "Epoch 411/500, Train Loss: 0.1084, Val Loss: 0.5151\n",
      "Epoch 412/500, Train Loss: 0.1075, Val Loss: 0.4753\n",
      "Epoch 413/500, Train Loss: 0.1263, Val Loss: 0.4861\n",
      "Epoch 414/500, Train Loss: 0.1362, Val Loss: 0.4809\n",
      "Epoch 415/500, Train Loss: 0.1070, Val Loss: 0.4789\n",
      "Epoch 416/500, Train Loss: 0.1002, Val Loss: 0.4987\n",
      "Epoch 417/500, Train Loss: 0.0946, Val Loss: 0.4855\n",
      "Epoch 418/500, Train Loss: 0.0942, Val Loss: 0.5221\n",
      "Epoch 419/500, Train Loss: 0.0880, Val Loss: 0.5055\n",
      "Epoch 420/500, Train Loss: 0.0915, Val Loss: 0.4924\n",
      "Epoch 421/500, Train Loss: 0.1208, Val Loss: 0.4912\n",
      "Epoch 422/500, Train Loss: 0.1255, Val Loss: 0.5252\n",
      "Epoch 423/500, Train Loss: 0.0906, Val Loss: 0.5162\n",
      "Epoch 424/500, Train Loss: 0.0940, Val Loss: 0.4872\n",
      "Epoch 425/500, Train Loss: 0.1115, Val Loss: 0.4877\n",
      "Epoch 426/500, Train Loss: 0.1032, Val Loss: 0.5029\n",
      "Epoch 427/500, Train Loss: 0.0829, Val Loss: 0.5032\n",
      "Epoch 428/500, Train Loss: 0.0894, Val Loss: 0.5363\n",
      "Epoch 429/500, Train Loss: 0.1157, Val Loss: 0.5479\n",
      "Epoch 430/500, Train Loss: 0.1177, Val Loss: 0.5014\n",
      "Epoch 431/500, Train Loss: 0.1191, Val Loss: 0.5300\n",
      "Epoch 432/500, Train Loss: 0.1018, Val Loss: 0.5098\n",
      "Epoch 433/500, Train Loss: 0.0941, Val Loss: 0.5243\n",
      "Epoch 434/500, Train Loss: 0.0895, Val Loss: 0.4970\n",
      "Epoch 435/500, Train Loss: 0.0992, Val Loss: 0.5072\n",
      "Epoch 436/500, Train Loss: 0.1093, Val Loss: 0.5066\n",
      "Epoch 437/500, Train Loss: 0.0962, Val Loss: 0.5134\n",
      "Epoch 438/500, Train Loss: 0.1066, Val Loss: 0.4922\n",
      "Epoch 439/500, Train Loss: 0.0961, Val Loss: 0.5058\n",
      "Epoch 440/500, Train Loss: 0.1125, Val Loss: 0.4823\n",
      "Epoch 441/500, Train Loss: 0.0895, Val Loss: 0.5044\n",
      "Epoch 442/500, Train Loss: 0.1034, Val Loss: 0.5290\n",
      "Epoch 443/500, Train Loss: 0.1342, Val Loss: 0.4914\n",
      "Epoch 444/500, Train Loss: 0.1009, Val Loss: 0.4884\n",
      "Epoch 445/500, Train Loss: 0.1656, Val Loss: 0.4831\n",
      "Epoch 446/500, Train Loss: 0.1568, Val Loss: 0.4899\n",
      "Epoch 447/500, Train Loss: 0.1463, Val Loss: 0.5253\n",
      "Epoch 448/500, Train Loss: 0.1232, Val Loss: 0.4769\n",
      "Epoch 449/500, Train Loss: 0.1088, Val Loss: 0.4831\n",
      "Epoch 450/500, Train Loss: 0.1261, Val Loss: 0.4961\n",
      "Epoch 451/500, Train Loss: 0.1134, Val Loss: 0.5500\n",
      "Epoch 452/500, Train Loss: 0.1136, Val Loss: 0.5378\n",
      "Epoch 453/500, Train Loss: 0.1016, Val Loss: 0.4932\n",
      "Epoch 454/500, Train Loss: 0.1095, Val Loss: 0.4839\n",
      "Epoch 455/500, Train Loss: 0.1255, Val Loss: 0.5005\n",
      "Epoch 456/500, Train Loss: 0.1008, Val Loss: 0.5348\n",
      "Epoch 457/500, Train Loss: 0.0980, Val Loss: 0.4865\n",
      "Epoch 458/500, Train Loss: 0.1518, Val Loss: 0.4881\n",
      "Epoch 459/500, Train Loss: 0.1368, Val Loss: 0.5005\n",
      "Epoch 460/500, Train Loss: 0.1164, Val Loss: 0.4955\n",
      "Epoch 461/500, Train Loss: 0.1154, Val Loss: 0.4950\n",
      "Epoch 462/500, Train Loss: 0.0977, Val Loss: 0.5468\n",
      "Epoch 463/500, Train Loss: 0.1108, Val Loss: 0.4902\n",
      "Epoch 464/500, Train Loss: 0.0940, Val Loss: 0.4951\n",
      "Epoch 465/500, Train Loss: 0.1045, Val Loss: 0.5402\n",
      "Epoch 466/500, Train Loss: 0.1152, Val Loss: 0.5167\n",
      "Epoch 467/500, Train Loss: 0.1023, Val Loss: 0.4874\n",
      "Epoch 468/500, Train Loss: 0.1283, Val Loss: 0.5301\n",
      "Epoch 469/500, Train Loss: 0.1105, Val Loss: 0.5020\n",
      "Epoch 470/500, Train Loss: 0.0891, Val Loss: 0.5127\n",
      "Epoch 471/500, Train Loss: 0.0811, Val Loss: 0.5286\n",
      "Epoch 472/500, Train Loss: 0.0984, Val Loss: 0.4799\n",
      "Epoch 473/500, Train Loss: 0.0947, Val Loss: 0.4998\n",
      "Epoch 474/500, Train Loss: 0.0973, Val Loss: 0.4941\n",
      "Epoch 475/500, Train Loss: 0.0914, Val Loss: 0.4923\n",
      "Epoch 476/500, Train Loss: 0.1175, Val Loss: 0.4873\n",
      "Epoch 477/500, Train Loss: 0.0971, Val Loss: 0.5017\n",
      "Epoch 478/500, Train Loss: 0.0889, Val Loss: 0.5051\n",
      "Epoch 479/500, Train Loss: 0.1049, Val Loss: 0.5027\n",
      "Epoch 480/500, Train Loss: 0.0791, Val Loss: 0.5178\n",
      "Epoch 481/500, Train Loss: 0.0853, Val Loss: 0.4995\n",
      "Epoch 482/500, Train Loss: 0.0860, Val Loss: 0.5111\n",
      "Epoch 483/500, Train Loss: 0.0857, Val Loss: 0.4727\n",
      "Epoch 484/500, Train Loss: 0.1147, Val Loss: 0.4670\n",
      "Epoch 485/500, Train Loss: 0.1123, Val Loss: 0.5257\n",
      "Epoch 486/500, Train Loss: 0.0994, Val Loss: 0.5036\n",
      "Epoch 487/500, Train Loss: 0.0900, Val Loss: 0.4967\n",
      "Epoch 488/500, Train Loss: 0.0904, Val Loss: 0.4982\n",
      "Epoch 489/500, Train Loss: 0.1086, Val Loss: 0.5337\n",
      "Epoch 490/500, Train Loss: 0.1058, Val Loss: 0.5194\n",
      "Epoch 491/500, Train Loss: 0.1083, Val Loss: 0.5031\n",
      "Epoch 492/500, Train Loss: 0.0780, Val Loss: 0.5230\n",
      "Epoch 493/500, Train Loss: 0.0944, Val Loss: 0.4886\n",
      "Epoch 494/500, Train Loss: 0.1130, Val Loss: 0.4749\n",
      "Epoch 495/500, Train Loss: 0.0943, Val Loss: 0.5064\n",
      "Epoch 496/500, Train Loss: 0.0806, Val Loss: 0.4903\n",
      "Epoch 497/500, Train Loss: 0.1001, Val Loss: 0.4853\n",
      "Epoch 498/500, Train Loss: 0.1343, Val Loss: 0.5184\n",
      "Epoch 499/500, Train Loss: 0.0933, Val Loss: 0.4884\n",
      "Epoch 500/500, Train Loss: 0.0946, Val Loss: 0.4921\n",
      "Training with hidden_dim=128, lr=0.001\n",
      "Epoch 1/500, Train Loss: 0.5620, Val Loss: 0.6118\n",
      "Epoch 2/500, Train Loss: 0.5928, Val Loss: 0.4567\n",
      "Epoch 3/500, Train Loss: 0.3857, Val Loss: 0.4979\n",
      "Epoch 4/500, Train Loss: 0.4322, Val Loss: 0.4438\n",
      "Epoch 5/500, Train Loss: 0.3836, Val Loss: 0.4510\n",
      "Epoch 6/500, Train Loss: 0.3710, Val Loss: 0.4546\n",
      "Epoch 7/500, Train Loss: 0.3328, Val Loss: 0.5285\n",
      "Epoch 8/500, Train Loss: 0.3790, Val Loss: 0.4512\n",
      "Epoch 9/500, Train Loss: 0.3319, Val Loss: 0.4591\n",
      "Epoch 10/500, Train Loss: 0.3347, Val Loss: 0.4503\n",
      "Epoch 11/500, Train Loss: 0.3412, Val Loss: 0.4966\n",
      "Epoch 12/500, Train Loss: 0.3620, Val Loss: 0.4581\n",
      "Epoch 13/500, Train Loss: 0.3606, Val Loss: 0.4503\n",
      "Epoch 14/500, Train Loss: 0.3273, Val Loss: 0.4541\n",
      "Epoch 15/500, Train Loss: 0.3346, Val Loss: 0.5031\n",
      "Epoch 16/500, Train Loss: 0.3647, Val Loss: 0.4543\n",
      "Epoch 17/500, Train Loss: 0.3434, Val Loss: 0.4637\n",
      "Epoch 18/500, Train Loss: 0.3020, Val Loss: 0.4808\n",
      "Epoch 19/500, Train Loss: 0.3271, Val Loss: 0.4594\n",
      "Epoch 20/500, Train Loss: 0.3309, Val Loss: 0.4593\n",
      "Epoch 21/500, Train Loss: 0.3117, Val Loss: 0.4569\n",
      "Epoch 22/500, Train Loss: 0.3301, Val Loss: 0.4580\n",
      "Epoch 23/500, Train Loss: 0.3383, Val Loss: 0.4602\n",
      "Epoch 24/500, Train Loss: 0.3636, Val Loss: 0.4735\n",
      "Epoch 25/500, Train Loss: 0.3265, Val Loss: 0.4876\n",
      "Epoch 26/500, Train Loss: 0.3236, Val Loss: 0.4841\n",
      "Epoch 27/500, Train Loss: 0.2950, Val Loss: 0.4677\n",
      "Epoch 28/500, Train Loss: 0.3210, Val Loss: 0.4891\n",
      "Epoch 29/500, Train Loss: 0.3595, Val Loss: 0.4795\n",
      "Epoch 30/500, Train Loss: 0.3096, Val Loss: 0.4667\n",
      "Epoch 31/500, Train Loss: 0.3395, Val Loss: 0.5207\n",
      "Epoch 32/500, Train Loss: 0.3689, Val Loss: 0.4912\n",
      "Epoch 33/500, Train Loss: 0.3309, Val Loss: 0.4693\n",
      "Epoch 34/500, Train Loss: 0.3149, Val Loss: 0.5276\n",
      "Epoch 35/500, Train Loss: 0.3810, Val Loss: 0.4946\n",
      "Epoch 36/500, Train Loss: 0.3411, Val Loss: 0.4615\n",
      "Epoch 37/500, Train Loss: 0.2887, Val Loss: 0.4836\n",
      "Epoch 38/500, Train Loss: 0.2894, Val Loss: 0.4797\n",
      "Epoch 39/500, Train Loss: 0.3107, Val Loss: 0.4730\n",
      "Epoch 40/500, Train Loss: 0.3170, Val Loss: 0.4974\n",
      "Epoch 41/500, Train Loss: 0.3129, Val Loss: 0.4958\n",
      "Epoch 42/500, Train Loss: 0.3438, Val Loss: 0.4778\n",
      "Epoch 43/500, Train Loss: 0.2895, Val Loss: 0.4771\n",
      "Epoch 44/500, Train Loss: 0.2658, Val Loss: 0.4941\n",
      "Epoch 45/500, Train Loss: 0.2887, Val Loss: 0.4988\n",
      "Epoch 46/500, Train Loss: 0.2763, Val Loss: 0.5177\n",
      "Epoch 47/500, Train Loss: 0.2862, Val Loss: 0.5004\n",
      "Epoch 48/500, Train Loss: 0.2627, Val Loss: 0.5182\n",
      "Epoch 49/500, Train Loss: 0.2917, Val Loss: 0.5067\n",
      "Epoch 50/500, Train Loss: 0.2624, Val Loss: 0.4960\n",
      "Epoch 51/500, Train Loss: 0.2463, Val Loss: 0.5247\n",
      "Epoch 52/500, Train Loss: 0.2827, Val Loss: 0.5050\n",
      "Epoch 53/500, Train Loss: 0.2552, Val Loss: 0.5064\n",
      "Epoch 54/500, Train Loss: 0.2579, Val Loss: 0.5036\n",
      "Epoch 55/500, Train Loss: 0.2685, Val Loss: 0.5184\n",
      "Epoch 56/500, Train Loss: 0.2793, Val Loss: 0.5095\n",
      "Epoch 57/500, Train Loss: 0.2327, Val Loss: 0.5096\n",
      "Epoch 58/500, Train Loss: 0.2828, Val Loss: 0.5152\n",
      "Epoch 59/500, Train Loss: 0.2787, Val Loss: 0.5402\n",
      "Epoch 60/500, Train Loss: 0.3229, Val Loss: 0.4779\n",
      "Epoch 61/500, Train Loss: 0.2666, Val Loss: 0.5098\n",
      "Epoch 62/500, Train Loss: 0.2776, Val Loss: 0.4963\n",
      "Epoch 63/500, Train Loss: 0.2827, Val Loss: 0.5437\n",
      "Epoch 64/500, Train Loss: 0.3300, Val Loss: 0.4646\n",
      "Epoch 65/500, Train Loss: 0.3129, Val Loss: 0.5215\n",
      "Epoch 66/500, Train Loss: 0.2731, Val Loss: 0.4936\n",
      "Epoch 67/500, Train Loss: 0.2655, Val Loss: 0.4975\n",
      "Epoch 68/500, Train Loss: 0.2648, Val Loss: 0.5103\n",
      "Epoch 69/500, Train Loss: 0.2375, Val Loss: 0.4982\n",
      "Epoch 70/500, Train Loss: 0.2590, Val Loss: 0.6299\n",
      "Epoch 71/500, Train Loss: 0.3436, Val Loss: 0.4484\n",
      "Epoch 72/500, Train Loss: 0.2707, Val Loss: 0.4824\n",
      "Epoch 73/500, Train Loss: 0.2841, Val Loss: 0.5341\n",
      "Epoch 74/500, Train Loss: 0.3162, Val Loss: 0.4912\n",
      "Epoch 75/500, Train Loss: 0.2945, Val Loss: 0.4848\n",
      "Epoch 76/500, Train Loss: 0.2520, Val Loss: 0.4919\n",
      "Epoch 77/500, Train Loss: 0.2430, Val Loss: 0.4981\n",
      "Epoch 78/500, Train Loss: 0.2408, Val Loss: 0.5252\n",
      "Epoch 79/500, Train Loss: 0.2714, Val Loss: 0.5460\n",
      "Epoch 80/500, Train Loss: 0.2976, Val Loss: 0.5106\n",
      "Epoch 81/500, Train Loss: 0.2552, Val Loss: 0.5031\n",
      "Epoch 82/500, Train Loss: 0.2741, Val Loss: 0.4944\n",
      "Epoch 83/500, Train Loss: 0.2558, Val Loss: 0.5170\n",
      "Epoch 84/500, Train Loss: 0.2705, Val Loss: 0.4967\n",
      "Epoch 85/500, Train Loss: 0.2123, Val Loss: 0.5497\n",
      "Epoch 86/500, Train Loss: 0.3324, Val Loss: 0.4781\n",
      "Epoch 87/500, Train Loss: 0.3145, Val Loss: 0.4496\n",
      "Epoch 88/500, Train Loss: 0.2642, Val Loss: 0.4860\n",
      "Epoch 89/500, Train Loss: 0.2450, Val Loss: 0.4900\n",
      "Epoch 90/500, Train Loss: 0.2308, Val Loss: 0.4984\n",
      "Epoch 91/500, Train Loss: 0.2907, Val Loss: 0.4852\n",
      "Epoch 92/500, Train Loss: 0.2321, Val Loss: 0.4961\n",
      "Epoch 93/500, Train Loss: 0.2239, Val Loss: 0.5056\n",
      "Epoch 94/500, Train Loss: 0.2168, Val Loss: 0.5158\n",
      "Epoch 95/500, Train Loss: 0.2591, Val Loss: 0.5035\n",
      "Epoch 96/500, Train Loss: 0.2497, Val Loss: 0.4957\n",
      "Epoch 97/500, Train Loss: 0.2467, Val Loss: 0.5025\n",
      "Epoch 98/500, Train Loss: 0.2362, Val Loss: 0.5055\n",
      "Epoch 99/500, Train Loss: 0.2065, Val Loss: 0.5040\n",
      "Epoch 100/500, Train Loss: 0.2083, Val Loss: 0.5449\n",
      "Epoch 101/500, Train Loss: 0.2783, Val Loss: 0.5256\n",
      "Epoch 102/500, Train Loss: 0.2329, Val Loss: 0.5014\n",
      "Epoch 103/500, Train Loss: 0.2262, Val Loss: 0.5225\n",
      "Epoch 104/500, Train Loss: 0.1940, Val Loss: 0.4860\n",
      "Epoch 105/500, Train Loss: 0.2324, Val Loss: 0.4976\n",
      "Epoch 106/500, Train Loss: 0.1892, Val Loss: 0.5276\n",
      "Epoch 107/500, Train Loss: 0.2170, Val Loss: 0.5022\n",
      "Epoch 108/500, Train Loss: 0.2017, Val Loss: 0.4969\n",
      "Epoch 109/500, Train Loss: 0.1810, Val Loss: 0.4980\n",
      "Epoch 110/500, Train Loss: 0.2126, Val Loss: 0.4908\n",
      "Epoch 111/500, Train Loss: 0.1764, Val Loss: 0.5132\n",
      "Epoch 112/500, Train Loss: 0.2607, Val Loss: 0.4855\n",
      "Epoch 113/500, Train Loss: 0.2072, Val Loss: 0.5116\n",
      "Epoch 114/500, Train Loss: 0.2095, Val Loss: 0.5034\n",
      "Epoch 115/500, Train Loss: 0.1859, Val Loss: 0.4900\n",
      "Epoch 116/500, Train Loss: 0.1968, Val Loss: 0.4897\n",
      "Epoch 117/500, Train Loss: 0.2098, Val Loss: 0.5270\n",
      "Epoch 118/500, Train Loss: 0.1828, Val Loss: 0.4962\n",
      "Epoch 119/500, Train Loss: 0.1855, Val Loss: 0.5223\n",
      "Epoch 120/500, Train Loss: 0.1629, Val Loss: 0.5201\n",
      "Epoch 121/500, Train Loss: 0.1580, Val Loss: 0.5055\n",
      "Epoch 122/500, Train Loss: 0.2167, Val Loss: 0.5084\n",
      "Epoch 123/500, Train Loss: 0.1892, Val Loss: 0.5292\n",
      "Epoch 124/500, Train Loss: 0.1677, Val Loss: 0.4899\n",
      "Epoch 125/500, Train Loss: 0.1636, Val Loss: 0.5217\n",
      "Epoch 126/500, Train Loss: 0.2019, Val Loss: 0.5101\n",
      "Epoch 127/500, Train Loss: 0.1944, Val Loss: 0.5139\n",
      "Epoch 128/500, Train Loss: 0.1571, Val Loss: 0.5515\n",
      "Epoch 129/500, Train Loss: 0.1938, Val Loss: 0.4916\n",
      "Epoch 130/500, Train Loss: 0.1714, Val Loss: 0.5104\n",
      "Epoch 131/500, Train Loss: 0.1564, Val Loss: 0.5244\n",
      "Epoch 132/500, Train Loss: 0.1847, Val Loss: 0.5035\n",
      "Epoch 133/500, Train Loss: 0.1545, Val Loss: 0.4887\n",
      "Epoch 134/500, Train Loss: 0.1816, Val Loss: 0.5119\n",
      "Epoch 135/500, Train Loss: 0.2021, Val Loss: 0.4954\n",
      "Epoch 136/500, Train Loss: 0.1823, Val Loss: 0.4831\n",
      "Epoch 137/500, Train Loss: 0.1604, Val Loss: 0.4924\n",
      "Epoch 138/500, Train Loss: 0.1785, Val Loss: 0.5895\n",
      "Epoch 139/500, Train Loss: 0.1828, Val Loss: 0.5086\n",
      "Epoch 140/500, Train Loss: 0.1701, Val Loss: 0.5212\n",
      "Epoch 141/500, Train Loss: 0.1684, Val Loss: 0.4893\n",
      "Epoch 142/500, Train Loss: 0.1678, Val Loss: 0.4875\n",
      "Epoch 143/500, Train Loss: 0.1361, Val Loss: 0.5103\n",
      "Epoch 144/500, Train Loss: 0.1418, Val Loss: 0.5037\n",
      "Epoch 145/500, Train Loss: 0.1882, Val Loss: 0.5516\n",
      "Epoch 146/500, Train Loss: 0.2063, Val Loss: 0.4791\n",
      "Epoch 147/500, Train Loss: 0.2478, Val Loss: 0.4787\n",
      "Epoch 148/500, Train Loss: 0.1757, Val Loss: 0.4944\n",
      "Epoch 149/500, Train Loss: 0.1554, Val Loss: 0.5077\n",
      "Epoch 150/500, Train Loss: 0.1614, Val Loss: 0.4965\n",
      "Epoch 151/500, Train Loss: 0.1682, Val Loss: 0.4928\n",
      "Epoch 152/500, Train Loss: 0.1436, Val Loss: 0.4888\n",
      "Epoch 153/500, Train Loss: 0.1450, Val Loss: 0.4861\n",
      "Epoch 154/500, Train Loss: 0.1420, Val Loss: 0.4770\n",
      "Epoch 155/500, Train Loss: 0.1665, Val Loss: 0.4893\n",
      "Epoch 156/500, Train Loss: 0.1500, Val Loss: 0.4818\n",
      "Epoch 157/500, Train Loss: 0.1600, Val Loss: 0.4906\n",
      "Epoch 158/500, Train Loss: 0.1634, Val Loss: 0.5280\n",
      "Epoch 159/500, Train Loss: 0.1681, Val Loss: 0.4807\n",
      "Epoch 160/500, Train Loss: 0.1964, Val Loss: 0.4872\n",
      "Epoch 161/500, Train Loss: 0.1421, Val Loss: 0.4829\n",
      "Epoch 162/500, Train Loss: 0.1662, Val Loss: 0.4845\n",
      "Epoch 163/500, Train Loss: 0.1495, Val Loss: 0.5284\n",
      "Epoch 164/500, Train Loss: 0.1672, Val Loss: 0.4771\n",
      "Epoch 165/500, Train Loss: 0.1380, Val Loss: 0.4814\n",
      "Epoch 166/500, Train Loss: 0.1351, Val Loss: 0.5399\n",
      "Epoch 167/500, Train Loss: 0.1326, Val Loss: 0.4883\n",
      "Epoch 168/500, Train Loss: 0.1282, Val Loss: 0.4943\n",
      "Epoch 169/500, Train Loss: 0.1401, Val Loss: 0.5352\n",
      "Epoch 170/500, Train Loss: 0.1511, Val Loss: 0.4964\n",
      "Epoch 171/500, Train Loss: 0.1430, Val Loss: 0.4768\n",
      "Epoch 172/500, Train Loss: 0.1742, Val Loss: 0.4750\n",
      "Epoch 173/500, Train Loss: 0.1756, Val Loss: 0.5464\n",
      "Epoch 174/500, Train Loss: 0.1892, Val Loss: 0.5059\n",
      "Epoch 175/500, Train Loss: 0.1626, Val Loss: 0.4874\n",
      "Epoch 176/500, Train Loss: 0.1277, Val Loss: 0.5004\n",
      "Epoch 177/500, Train Loss: 0.1157, Val Loss: 0.4782\n",
      "Epoch 178/500, Train Loss: 0.1413, Val Loss: 0.4837\n",
      "Epoch 179/500, Train Loss: 0.1363, Val Loss: 0.4994\n",
      "Epoch 180/500, Train Loss: 0.1139, Val Loss: 0.4961\n",
      "Epoch 181/500, Train Loss: 0.1324, Val Loss: 0.4857\n",
      "Epoch 182/500, Train Loss: 0.1195, Val Loss: 0.6061\n",
      "Epoch 183/500, Train Loss: 0.1933, Val Loss: 0.5215\n",
      "Epoch 184/500, Train Loss: 0.1466, Val Loss: 0.4826\n",
      "Epoch 185/500, Train Loss: 0.1256, Val Loss: 0.4967\n",
      "Epoch 186/500, Train Loss: 0.1317, Val Loss: 0.4854\n",
      "Epoch 187/500, Train Loss: 0.1286, Val Loss: 0.4940\n",
      "Epoch 188/500, Train Loss: 0.1439, Val Loss: 0.5030\n",
      "Epoch 189/500, Train Loss: 0.1304, Val Loss: 0.5023\n",
      "Epoch 190/500, Train Loss: 0.1856, Val Loss: 0.4944\n",
      "Epoch 191/500, Train Loss: 0.1735, Val Loss: 0.5521\n",
      "Epoch 192/500, Train Loss: 0.1342, Val Loss: 0.4765\n",
      "Epoch 193/500, Train Loss: 0.1239, Val Loss: 0.5272\n",
      "Epoch 194/500, Train Loss: 0.1387, Val Loss: 0.4831\n",
      "Epoch 195/500, Train Loss: 0.1279, Val Loss: 0.5055\n",
      "Epoch 196/500, Train Loss: 0.1171, Val Loss: 0.5157\n",
      "Epoch 197/500, Train Loss: 0.1119, Val Loss: 0.4864\n",
      "Epoch 198/500, Train Loss: 0.1137, Val Loss: 0.4959\n",
      "Epoch 199/500, Train Loss: 0.1238, Val Loss: 0.5070\n",
      "Epoch 200/500, Train Loss: 0.1133, Val Loss: 0.4852\n",
      "Epoch 201/500, Train Loss: 0.1085, Val Loss: 0.4741\n",
      "Epoch 202/500, Train Loss: 0.1668, Val Loss: 0.4853\n",
      "Epoch 203/500, Train Loss: 0.1213, Val Loss: 0.4897\n",
      "Epoch 204/500, Train Loss: 0.1696, Val Loss: 0.4869\n",
      "Epoch 205/500, Train Loss: 0.1603, Val Loss: 0.4856\n",
      "Epoch 206/500, Train Loss: 0.1267, Val Loss: 0.4868\n",
      "Epoch 207/500, Train Loss: 0.1242, Val Loss: 0.4956\n",
      "Epoch 208/500, Train Loss: 0.1186, Val Loss: 0.4788\n",
      "Epoch 209/500, Train Loss: 0.1399, Val Loss: 0.5862\n",
      "Epoch 210/500, Train Loss: 0.1973, Val Loss: 0.4790\n",
      "Epoch 211/500, Train Loss: 0.1595, Val Loss: 0.4955\n",
      "Epoch 212/500, Train Loss: 0.1643, Val Loss: 0.5194\n",
      "Epoch 213/500, Train Loss: 0.1400, Val Loss: 0.5301\n",
      "Epoch 214/500, Train Loss: 0.1359, Val Loss: 0.4951\n",
      "Epoch 215/500, Train Loss: 0.1342, Val Loss: 0.5135\n",
      "Epoch 216/500, Train Loss: 0.1165, Val Loss: 0.4987\n",
      "Epoch 217/500, Train Loss: 0.1320, Val Loss: 0.5457\n",
      "Epoch 218/500, Train Loss: 0.1147, Val Loss: 0.4893\n",
      "Epoch 219/500, Train Loss: 0.1133, Val Loss: 0.4810\n",
      "Epoch 220/500, Train Loss: 0.1251, Val Loss: 0.5266\n",
      "Epoch 221/500, Train Loss: 0.1615, Val Loss: 0.5032\n",
      "Epoch 222/500, Train Loss: 0.1568, Val Loss: 0.4913\n",
      "Epoch 223/500, Train Loss: 0.1711, Val Loss: 0.4572\n",
      "Epoch 224/500, Train Loss: 0.1522, Val Loss: 0.5343\n",
      "Epoch 225/500, Train Loss: 0.1834, Val Loss: 0.5042\n",
      "Epoch 226/500, Train Loss: 0.1334, Val Loss: 0.4882\n",
      "Epoch 227/500, Train Loss: 0.1343, Val Loss: 0.5054\n",
      "Epoch 228/500, Train Loss: 0.1116, Val Loss: 0.4947\n",
      "Epoch 229/500, Train Loss: 0.1024, Val Loss: 0.4937\n",
      "Epoch 230/500, Train Loss: 0.0958, Val Loss: 0.4983\n",
      "Epoch 231/500, Train Loss: 0.0968, Val Loss: 0.5055\n",
      "Epoch 232/500, Train Loss: 0.0980, Val Loss: 0.4839\n",
      "Epoch 233/500, Train Loss: 0.1187, Val Loss: 0.4859\n",
      "Epoch 234/500, Train Loss: 0.1264, Val Loss: 0.4836\n",
      "Epoch 235/500, Train Loss: 0.1016, Val Loss: 0.5478\n",
      "Epoch 236/500, Train Loss: 0.1515, Val Loss: 0.4880\n",
      "Epoch 237/500, Train Loss: 0.1135, Val Loss: 0.5190\n",
      "Epoch 238/500, Train Loss: 0.1069, Val Loss: 0.5101\n",
      "Epoch 239/500, Train Loss: 0.1026, Val Loss: 0.4817\n",
      "Epoch 240/500, Train Loss: 0.1277, Val Loss: 0.5274\n",
      "Epoch 241/500, Train Loss: 0.1217, Val Loss: 0.5298\n",
      "Epoch 242/500, Train Loss: 0.1248, Val Loss: 0.4935\n",
      "Epoch 243/500, Train Loss: 0.1066, Val Loss: 0.4834\n",
      "Epoch 244/500, Train Loss: 0.1509, Val Loss: 0.4899\n",
      "Epoch 245/500, Train Loss: 0.1224, Val Loss: 0.5312\n",
      "Epoch 246/500, Train Loss: 0.1084, Val Loss: 0.5011\n",
      "Epoch 247/500, Train Loss: 0.1252, Val Loss: 0.4947\n",
      "Epoch 248/500, Train Loss: 0.1367, Val Loss: 0.4994\n",
      "Epoch 249/500, Train Loss: 0.1253, Val Loss: 0.5001\n",
      "Epoch 250/500, Train Loss: 0.1149, Val Loss: 0.4965\n",
      "Epoch 251/500, Train Loss: 0.1256, Val Loss: 0.5062\n",
      "Epoch 252/500, Train Loss: 0.1192, Val Loss: 0.4956\n",
      "Epoch 253/500, Train Loss: 0.1258, Val Loss: 0.5245\n",
      "Epoch 254/500, Train Loss: 0.1378, Val Loss: 0.4817\n",
      "Epoch 255/500, Train Loss: 0.1336, Val Loss: 0.4793\n",
      "Epoch 256/500, Train Loss: 0.1237, Val Loss: 0.5109\n",
      "Epoch 257/500, Train Loss: 0.0988, Val Loss: 0.4895\n",
      "Epoch 258/500, Train Loss: 0.1175, Val Loss: 0.4980\n",
      "Epoch 259/500, Train Loss: 0.1242, Val Loss: 0.5002\n",
      "Epoch 260/500, Train Loss: 0.1046, Val Loss: 0.4837\n",
      "Epoch 261/500, Train Loss: 0.1713, Val Loss: 0.4769\n",
      "Epoch 262/500, Train Loss: 0.1277, Val Loss: 0.4871\n",
      "Epoch 263/500, Train Loss: 0.1001, Val Loss: 0.5076\n",
      "Epoch 264/500, Train Loss: 0.1112, Val Loss: 0.5105\n",
      "Epoch 265/500, Train Loss: 0.1074, Val Loss: 0.5029\n",
      "Epoch 266/500, Train Loss: 0.1060, Val Loss: 0.4910\n",
      "Epoch 267/500, Train Loss: 0.1185, Val Loss: 0.4761\n",
      "Epoch 268/500, Train Loss: 0.1146, Val Loss: 0.4907\n",
      "Epoch 269/500, Train Loss: 0.1027, Val Loss: 0.4862\n",
      "Epoch 270/500, Train Loss: 0.0978, Val Loss: 0.4773\n",
      "Epoch 271/500, Train Loss: 0.1111, Val Loss: 0.4873\n",
      "Epoch 272/500, Train Loss: 0.1224, Val Loss: 0.4854\n",
      "Epoch 273/500, Train Loss: 0.1259, Val Loss: 0.4845\n",
      "Epoch 274/500, Train Loss: 0.1342, Val Loss: 0.5048\n",
      "Epoch 275/500, Train Loss: 0.1390, Val Loss: 0.5396\n",
      "Epoch 276/500, Train Loss: 0.1874, Val Loss: 0.5068\n",
      "Epoch 277/500, Train Loss: 0.1400, Val Loss: 0.5695\n",
      "Epoch 278/500, Train Loss: 0.1869, Val Loss: 0.5058\n",
      "Epoch 279/500, Train Loss: 0.1294, Val Loss: 0.4846\n",
      "Epoch 280/500, Train Loss: 0.1206, Val Loss: 0.4707\n",
      "Epoch 281/500, Train Loss: 0.1342, Val Loss: 0.5381\n",
      "Epoch 282/500, Train Loss: 0.1715, Val Loss: 0.5162\n",
      "Epoch 283/500, Train Loss: 0.1257, Val Loss: 0.5091\n",
      "Epoch 284/500, Train Loss: 0.1196, Val Loss: 0.4896\n",
      "Epoch 285/500, Train Loss: 0.1165, Val Loss: 0.4761\n",
      "Epoch 286/500, Train Loss: 0.1134, Val Loss: 0.5074\n",
      "Epoch 287/500, Train Loss: 0.0948, Val Loss: 0.5070\n",
      "Epoch 288/500, Train Loss: 0.1223, Val Loss: 0.4918\n",
      "Epoch 289/500, Train Loss: 0.1142, Val Loss: 0.5030\n",
      "Epoch 290/500, Train Loss: 0.1142, Val Loss: 0.5041\n",
      "Epoch 291/500, Train Loss: 0.1095, Val Loss: 0.5043\n",
      "Epoch 292/500, Train Loss: 0.0931, Val Loss: 0.4727\n",
      "Epoch 293/500, Train Loss: 0.1111, Val Loss: 0.5379\n",
      "Epoch 294/500, Train Loss: 0.1237, Val Loss: 0.4831\n",
      "Epoch 295/500, Train Loss: 0.1291, Val Loss: 0.4943\n",
      "Epoch 296/500, Train Loss: 0.0953, Val Loss: 0.4902\n",
      "Epoch 297/500, Train Loss: 0.0921, Val Loss: 0.4717\n",
      "Epoch 298/500, Train Loss: 0.1043, Val Loss: 0.5009\n",
      "Epoch 299/500, Train Loss: 0.1091, Val Loss: 0.4662\n",
      "Epoch 300/500, Train Loss: 0.1480, Val Loss: 0.4773\n",
      "Epoch 301/500, Train Loss: 0.1233, Val Loss: 0.4968\n",
      "Epoch 302/500, Train Loss: 0.1048, Val Loss: 0.4986\n",
      "Epoch 303/500, Train Loss: 0.1158, Val Loss: 0.5091\n",
      "Epoch 304/500, Train Loss: 0.1108, Val Loss: 0.4834\n",
      "Epoch 305/500, Train Loss: 0.0969, Val Loss: 0.5054\n",
      "Epoch 306/500, Train Loss: 0.1080, Val Loss: 0.5009\n",
      "Epoch 307/500, Train Loss: 0.1056, Val Loss: 0.4750\n",
      "Epoch 308/500, Train Loss: 0.0935, Val Loss: 0.4938\n",
      "Epoch 309/500, Train Loss: 0.1127, Val Loss: 0.4775\n",
      "Epoch 310/500, Train Loss: 0.1009, Val Loss: 0.4817\n",
      "Epoch 311/500, Train Loss: 0.0936, Val Loss: 0.4758\n",
      "Epoch 312/500, Train Loss: 0.0985, Val Loss: 0.4780\n",
      "Epoch 313/500, Train Loss: 0.0927, Val Loss: 0.4735\n",
      "Epoch 314/500, Train Loss: 0.1006, Val Loss: 0.4786\n",
      "Epoch 315/500, Train Loss: 0.1040, Val Loss: 0.4876\n",
      "Epoch 316/500, Train Loss: 0.1100, Val Loss: 0.4878\n",
      "Epoch 317/500, Train Loss: 0.1030, Val Loss: 0.4981\n",
      "Epoch 318/500, Train Loss: 0.1190, Val Loss: 0.4841\n",
      "Epoch 319/500, Train Loss: 0.1065, Val Loss: 0.4633\n",
      "Epoch 320/500, Train Loss: 0.1054, Val Loss: 0.4830\n",
      "Epoch 321/500, Train Loss: 0.1098, Val Loss: 0.5202\n",
      "Epoch 322/500, Train Loss: 0.1495, Val Loss: 0.4977\n",
      "Epoch 323/500, Train Loss: 0.0990, Val Loss: 0.5003\n",
      "Epoch 324/500, Train Loss: 0.0971, Val Loss: 0.5068\n",
      "Epoch 325/500, Train Loss: 0.1008, Val Loss: 0.4848\n",
      "Epoch 326/500, Train Loss: 0.1125, Val Loss: 0.5170\n",
      "Epoch 327/500, Train Loss: 0.1005, Val Loss: 0.4947\n",
      "Epoch 328/500, Train Loss: 0.0983, Val Loss: 0.4868\n",
      "Epoch 329/500, Train Loss: 0.0944, Val Loss: 0.4846\n",
      "Epoch 330/500, Train Loss: 0.1132, Val Loss: 0.4982\n",
      "Epoch 331/500, Train Loss: 0.0999, Val Loss: 0.4765\n",
      "Epoch 332/500, Train Loss: 0.1123, Val Loss: 0.4781\n",
      "Epoch 333/500, Train Loss: 0.1017, Val Loss: 0.5082\n",
      "Epoch 334/500, Train Loss: 0.1104, Val Loss: 0.4862\n",
      "Epoch 335/500, Train Loss: 0.0922, Val Loss: 0.4917\n",
      "Epoch 336/500, Train Loss: 0.0969, Val Loss: 0.4796\n",
      "Epoch 337/500, Train Loss: 0.1034, Val Loss: 0.4757\n",
      "Epoch 338/500, Train Loss: 0.1418, Val Loss: 0.4752\n",
      "Epoch 339/500, Train Loss: 0.1177, Val Loss: 0.4985\n",
      "Epoch 340/500, Train Loss: 0.1019, Val Loss: 0.4753\n",
      "Epoch 341/500, Train Loss: 0.1002, Val Loss: 0.4622\n",
      "Epoch 342/500, Train Loss: 0.1302, Val Loss: 0.4911\n",
      "Epoch 343/500, Train Loss: 0.0940, Val Loss: 0.5244\n",
      "Epoch 344/500, Train Loss: 0.1352, Val Loss: 0.5071\n",
      "Epoch 345/500, Train Loss: 0.1128, Val Loss: 0.4679\n",
      "Epoch 346/500, Train Loss: 0.1017, Val Loss: 0.4842\n",
      "Epoch 347/500, Train Loss: 0.1098, Val Loss: 0.4875\n",
      "Epoch 348/500, Train Loss: 0.0970, Val Loss: 0.4874\n",
      "Epoch 349/500, Train Loss: 0.1112, Val Loss: 0.4830\n",
      "Epoch 350/500, Train Loss: 0.0953, Val Loss: 0.4782\n",
      "Epoch 351/500, Train Loss: 0.0923, Val Loss: 0.4760\n",
      "Epoch 352/500, Train Loss: 0.0945, Val Loss: 0.4842\n",
      "Epoch 353/500, Train Loss: 0.0977, Val Loss: 0.5134\n",
      "Epoch 354/500, Train Loss: 0.1046, Val Loss: 0.4858\n",
      "Epoch 355/500, Train Loss: 0.1158, Val Loss: 0.4753\n",
      "Epoch 356/500, Train Loss: 0.1129, Val Loss: 0.5212\n",
      "Epoch 357/500, Train Loss: 0.1020, Val Loss: 0.5085\n",
      "Epoch 358/500, Train Loss: 0.0921, Val Loss: 0.4879\n",
      "Epoch 359/500, Train Loss: 0.1014, Val Loss: 0.4750\n",
      "Epoch 360/500, Train Loss: 0.1025, Val Loss: 0.4993\n",
      "Epoch 361/500, Train Loss: 0.1078, Val Loss: 0.4739\n",
      "Epoch 362/500, Train Loss: 0.1164, Val Loss: 0.5151\n",
      "Epoch 363/500, Train Loss: 0.1061, Val Loss: 0.4777\n",
      "Epoch 364/500, Train Loss: 0.1121, Val Loss: 0.4912\n",
      "Epoch 365/500, Train Loss: 0.1309, Val Loss: 0.4965\n",
      "Epoch 366/500, Train Loss: 0.1080, Val Loss: 0.4864\n",
      "Epoch 367/500, Train Loss: 0.1024, Val Loss: 0.4851\n",
      "Epoch 368/500, Train Loss: 0.1261, Val Loss: 0.4774\n",
      "Epoch 369/500, Train Loss: 0.0893, Val Loss: 0.4935\n",
      "Epoch 370/500, Train Loss: 0.1112, Val Loss: 0.5023\n",
      "Epoch 371/500, Train Loss: 0.0995, Val Loss: 0.4605\n",
      "Epoch 372/500, Train Loss: 0.1402, Val Loss: 0.4657\n",
      "Epoch 373/500, Train Loss: 0.1276, Val Loss: 0.4590\n",
      "Epoch 374/500, Train Loss: 0.1375, Val Loss: 0.5076\n",
      "Epoch 375/500, Train Loss: 0.1089, Val Loss: 0.4660\n",
      "Epoch 376/500, Train Loss: 0.1000, Val Loss: 0.4804\n",
      "Epoch 377/500, Train Loss: 0.1356, Val Loss: 0.4742\n",
      "Epoch 378/500, Train Loss: 0.0955, Val Loss: 0.4846\n",
      "Epoch 379/500, Train Loss: 0.0998, Val Loss: 0.5051\n",
      "Epoch 380/500, Train Loss: 0.0826, Val Loss: 0.5012\n",
      "Epoch 381/500, Train Loss: 0.1013, Val Loss: 0.4826\n",
      "Epoch 382/500, Train Loss: 0.1443, Val Loss: 0.4799\n",
      "Epoch 383/500, Train Loss: 0.1548, Val Loss: 0.5026\n",
      "Epoch 384/500, Train Loss: 0.1165, Val Loss: 0.4992\n",
      "Epoch 385/500, Train Loss: 0.1236, Val Loss: 0.4936\n",
      "Epoch 386/500, Train Loss: 0.1325, Val Loss: 0.4856\n",
      "Epoch 387/500, Train Loss: 0.0919, Val Loss: 0.4823\n",
      "Epoch 388/500, Train Loss: 0.1095, Val Loss: 0.4740\n",
      "Epoch 389/500, Train Loss: 0.1032, Val Loss: 0.4772\n",
      "Epoch 390/500, Train Loss: 0.1158, Val Loss: 0.4816\n",
      "Epoch 391/500, Train Loss: 0.1022, Val Loss: 0.4912\n",
      "Epoch 392/500, Train Loss: 0.1023, Val Loss: 0.4719\n",
      "Epoch 393/500, Train Loss: 0.1342, Val Loss: 0.4996\n",
      "Epoch 394/500, Train Loss: 0.1194, Val Loss: 0.5249\n",
      "Epoch 395/500, Train Loss: 0.1027, Val Loss: 0.4791\n",
      "Epoch 396/500, Train Loss: 0.0897, Val Loss: 0.4743\n",
      "Epoch 397/500, Train Loss: 0.1143, Val Loss: 0.4930\n",
      "Epoch 398/500, Train Loss: 0.0935, Val Loss: 0.4883\n",
      "Epoch 399/500, Train Loss: 0.0877, Val Loss: 0.5412\n",
      "Epoch 400/500, Train Loss: 0.1662, Val Loss: 0.5240\n",
      "Epoch 401/500, Train Loss: 0.1690, Val Loss: 0.5379\n",
      "Epoch 402/500, Train Loss: 0.1185, Val Loss: 0.4928\n",
      "Epoch 403/500, Train Loss: 0.1009, Val Loss: 0.4974\n",
      "Epoch 404/500, Train Loss: 0.1035, Val Loss: 0.5139\n",
      "Epoch 405/500, Train Loss: 0.1150, Val Loss: 0.4761\n",
      "Epoch 406/500, Train Loss: 0.1394, Val Loss: 0.4904\n",
      "Epoch 407/500, Train Loss: 0.1308, Val Loss: 0.4905\n",
      "Epoch 408/500, Train Loss: 0.0952, Val Loss: 0.5131\n",
      "Epoch 409/500, Train Loss: 0.1009, Val Loss: 0.4840\n",
      "Epoch 410/500, Train Loss: 0.0943, Val Loss: 0.4936\n",
      "Epoch 411/500, Train Loss: 0.0903, Val Loss: 0.5246\n",
      "Epoch 412/500, Train Loss: 0.1195, Val Loss: 0.4886\n",
      "Epoch 413/500, Train Loss: 0.0957, Val Loss: 0.5058\n",
      "Epoch 414/500, Train Loss: 0.0893, Val Loss: 0.4759\n",
      "Epoch 415/500, Train Loss: 0.1164, Val Loss: 0.4968\n",
      "Epoch 416/500, Train Loss: 0.0838, Val Loss: 0.5286\n",
      "Epoch 417/500, Train Loss: 0.1327, Val Loss: 0.5415\n",
      "Epoch 418/500, Train Loss: 0.1116, Val Loss: 0.4945\n",
      "Epoch 419/500, Train Loss: 0.1164, Val Loss: 0.4927\n",
      "Epoch 420/500, Train Loss: 0.1066, Val Loss: 0.4943\n",
      "Epoch 421/500, Train Loss: 0.1072, Val Loss: 0.4976\n",
      "Epoch 422/500, Train Loss: 0.0825, Val Loss: 0.4894\n",
      "Epoch 423/500, Train Loss: 0.0875, Val Loss: 0.5304\n",
      "Epoch 424/500, Train Loss: 0.1107, Val Loss: 0.4837\n",
      "Epoch 425/500, Train Loss: 0.1182, Val Loss: 0.4835\n",
      "Epoch 426/500, Train Loss: 0.1020, Val Loss: 0.5212\n",
      "Epoch 427/500, Train Loss: 0.1072, Val Loss: 0.5086\n",
      "Epoch 428/500, Train Loss: 0.0976, Val Loss: 0.4879\n",
      "Epoch 429/500, Train Loss: 0.0850, Val Loss: 0.5132\n",
      "Epoch 430/500, Train Loss: 0.1005, Val Loss: 0.4824\n",
      "Epoch 431/500, Train Loss: 0.1216, Val Loss: 0.4860\n",
      "Epoch 432/500, Train Loss: 0.1087, Val Loss: 0.5552\n",
      "Epoch 433/500, Train Loss: 0.1228, Val Loss: 0.4861\n",
      "Epoch 434/500, Train Loss: 0.0988, Val Loss: 0.4942\n",
      "Epoch 435/500, Train Loss: 0.1049, Val Loss: 0.5179\n",
      "Epoch 436/500, Train Loss: 0.0986, Val Loss: 0.5122\n",
      "Epoch 437/500, Train Loss: 0.0931, Val Loss: 0.4934\n",
      "Epoch 438/500, Train Loss: 0.0956, Val Loss: 0.4753\n",
      "Epoch 439/500, Train Loss: 0.0984, Val Loss: 0.5002\n",
      "Epoch 440/500, Train Loss: 0.0941, Val Loss: 0.4690\n",
      "Epoch 441/500, Train Loss: 0.1161, Val Loss: 0.4668\n",
      "Epoch 442/500, Train Loss: 0.1950, Val Loss: 0.4720\n",
      "Epoch 443/500, Train Loss: 0.1224, Val Loss: 0.4675\n",
      "Epoch 444/500, Train Loss: 0.0928, Val Loss: 0.4954\n",
      "Epoch 445/500, Train Loss: 0.0946, Val Loss: 0.4729\n",
      "Epoch 446/500, Train Loss: 0.1196, Val Loss: 0.4590\n",
      "Epoch 447/500, Train Loss: 0.0968, Val Loss: 0.4684\n",
      "Epoch 448/500, Train Loss: 0.0880, Val Loss: 0.4790\n",
      "Epoch 449/500, Train Loss: 0.0824, Val Loss: 0.4790\n",
      "Epoch 450/500, Train Loss: 0.1016, Val Loss: 0.5044\n",
      "Epoch 451/500, Train Loss: 0.1026, Val Loss: 0.4705\n",
      "Epoch 452/500, Train Loss: 0.1307, Val Loss: 0.4680\n",
      "Epoch 453/500, Train Loss: 0.0982, Val Loss: 0.4701\n",
      "Epoch 454/500, Train Loss: 0.0937, Val Loss: 0.4731\n",
      "Epoch 455/500, Train Loss: 0.1002, Val Loss: 0.4931\n",
      "Epoch 456/500, Train Loss: 0.1040, Val Loss: 0.4827\n",
      "Epoch 457/500, Train Loss: 0.0895, Val Loss: 0.5401\n",
      "Epoch 458/500, Train Loss: 0.1310, Val Loss: 0.5273\n",
      "Epoch 459/500, Train Loss: 0.1339, Val Loss: 0.4987\n",
      "Epoch 460/500, Train Loss: 0.1225, Val Loss: 0.4896\n",
      "Epoch 461/500, Train Loss: 0.0928, Val Loss: 0.4733\n",
      "Epoch 462/500, Train Loss: 0.1070, Val Loss: 0.4868\n",
      "Epoch 463/500, Train Loss: 0.1149, Val Loss: 0.4955\n",
      "Epoch 464/500, Train Loss: 0.0782, Val Loss: 0.4936\n",
      "Epoch 465/500, Train Loss: 0.0922, Val Loss: 0.4845\n",
      "Epoch 466/500, Train Loss: 0.1181, Val Loss: 0.4782\n",
      "Epoch 467/500, Train Loss: 0.1160, Val Loss: 0.5019\n",
      "Epoch 468/500, Train Loss: 0.1053, Val Loss: 0.4880\n",
      "Epoch 469/500, Train Loss: 0.1050, Val Loss: 0.4805\n",
      "Epoch 470/500, Train Loss: 0.1357, Val Loss: 0.4834\n",
      "Epoch 471/500, Train Loss: 0.1049, Val Loss: 0.5174\n",
      "Epoch 472/500, Train Loss: 0.1108, Val Loss: 0.5114\n",
      "Epoch 473/500, Train Loss: 0.0967, Val Loss: 0.4891\n",
      "Epoch 474/500, Train Loss: 0.0942, Val Loss: 0.5342\n",
      "Epoch 475/500, Train Loss: 0.1033, Val Loss: 0.4868\n",
      "Epoch 476/500, Train Loss: 0.0903, Val Loss: 0.4924\n",
      "Epoch 477/500, Train Loss: 0.0948, Val Loss: 0.4755\n",
      "Epoch 478/500, Train Loss: 0.0985, Val Loss: 0.5131\n",
      "Epoch 479/500, Train Loss: 0.1101, Val Loss: 0.5153\n",
      "Epoch 480/500, Train Loss: 0.0962, Val Loss: 0.5032\n",
      "Epoch 481/500, Train Loss: 0.0958, Val Loss: 0.4756\n",
      "Epoch 482/500, Train Loss: 0.1257, Val Loss: 0.4923\n",
      "Epoch 483/500, Train Loss: 0.0969, Val Loss: 0.4901\n",
      "Epoch 484/500, Train Loss: 0.1226, Val Loss: 0.5303\n",
      "Epoch 485/500, Train Loss: 0.1280, Val Loss: 0.4784\n",
      "Epoch 486/500, Train Loss: 0.1132, Val Loss: 0.4750\n",
      "Epoch 487/500, Train Loss: 0.1041, Val Loss: 0.4753\n",
      "Epoch 488/500, Train Loss: 0.1030, Val Loss: 0.5053\n",
      "Epoch 489/500, Train Loss: 0.1186, Val Loss: 0.5038\n",
      "Epoch 490/500, Train Loss: 0.0789, Val Loss: 0.4876\n",
      "Epoch 491/500, Train Loss: 0.0829, Val Loss: 0.4800\n",
      "Epoch 492/500, Train Loss: 0.0869, Val Loss: 0.5051\n",
      "Epoch 493/500, Train Loss: 0.0945, Val Loss: 0.4944\n",
      "Epoch 494/500, Train Loss: 0.0861, Val Loss: 0.4848\n",
      "Epoch 495/500, Train Loss: 0.0977, Val Loss: 0.4864\n",
      "Epoch 496/500, Train Loss: 0.1011, Val Loss: 0.4845\n",
      "Epoch 497/500, Train Loss: 0.1123, Val Loss: 0.4835\n",
      "Epoch 498/500, Train Loss: 0.1044, Val Loss: 0.5093\n",
      "Epoch 499/500, Train Loss: 0.0891, Val Loss: 0.4768\n",
      "Epoch 500/500, Train Loss: 0.0915, Val Loss: 0.4865\n",
      "Training with hidden_dim=128, lr=0.0005\n",
      "Epoch 1/500, Train Loss: 0.4635, Val Loss: 0.5856\n",
      "Epoch 2/500, Train Loss: 0.4772, Val Loss: 0.4730\n",
      "Epoch 3/500, Train Loss: 0.3584, Val Loss: 0.4477\n",
      "Epoch 4/500, Train Loss: 0.3369, Val Loss: 0.4590\n",
      "Epoch 5/500, Train Loss: 0.3222, Val Loss: 0.4664\n",
      "Epoch 6/500, Train Loss: 0.3426, Val Loss: 0.4489\n",
      "Epoch 7/500, Train Loss: 0.3231, Val Loss: 0.4579\n",
      "Epoch 8/500, Train Loss: 0.3320, Val Loss: 0.4574\n",
      "Epoch 9/500, Train Loss: 0.3317, Val Loss: 0.4582\n",
      "Epoch 10/500, Train Loss: 0.3301, Val Loss: 0.4686\n",
      "Epoch 11/500, Train Loss: 0.3524, Val Loss: 0.4613\n",
      "Epoch 12/500, Train Loss: 0.3385, Val Loss: 0.4587\n",
      "Epoch 13/500, Train Loss: 0.3315, Val Loss: 0.4626\n",
      "Epoch 14/500, Train Loss: 0.2967, Val Loss: 0.4668\n",
      "Epoch 15/500, Train Loss: 0.3134, Val Loss: 0.4718\n",
      "Epoch 16/500, Train Loss: 0.3068, Val Loss: 0.5019\n",
      "Epoch 17/500, Train Loss: 0.3145, Val Loss: 0.4795\n",
      "Epoch 18/500, Train Loss: 0.2974, Val Loss: 0.4933\n",
      "Epoch 19/500, Train Loss: 0.3272, Val Loss: 0.5134\n",
      "Epoch 20/500, Train Loss: 0.3277, Val Loss: 0.5136\n",
      "Epoch 21/500, Train Loss: 0.3237, Val Loss: 0.5071\n",
      "Epoch 22/500, Train Loss: 0.3146, Val Loss: 0.4761\n",
      "Epoch 23/500, Train Loss: 0.3018, Val Loss: 0.4745\n",
      "Epoch 24/500, Train Loss: 0.3155, Val Loss: 0.4723\n",
      "Epoch 25/500, Train Loss: 0.3118, Val Loss: 0.4824\n",
      "Epoch 26/500, Train Loss: 0.2993, Val Loss: 0.4823\n",
      "Epoch 27/500, Train Loss: 0.2894, Val Loss: 0.4922\n",
      "Epoch 28/500, Train Loss: 0.2933, Val Loss: 0.4923\n",
      "Epoch 29/500, Train Loss: 0.3070, Val Loss: 0.4702\n",
      "Epoch 30/500, Train Loss: 0.2792, Val Loss: 0.4703\n",
      "Epoch 31/500, Train Loss: 0.3032, Val Loss: 0.4912\n",
      "Epoch 32/500, Train Loss: 0.2588, Val Loss: 0.5225\n",
      "Epoch 33/500, Train Loss: 0.3138, Val Loss: 0.4895\n",
      "Epoch 34/500, Train Loss: 0.2828, Val Loss: 0.5000\n",
      "Epoch 35/500, Train Loss: 0.2953, Val Loss: 0.4903\n",
      "Epoch 36/500, Train Loss: 0.2971, Val Loss: 0.4787\n",
      "Epoch 37/500, Train Loss: 0.2991, Val Loss: 0.4777\n",
      "Epoch 38/500, Train Loss: 0.2602, Val Loss: 0.5898\n",
      "Epoch 39/500, Train Loss: 0.3585, Val Loss: 0.5514\n",
      "Epoch 40/500, Train Loss: 0.4299, Val Loss: 0.4731\n",
      "Epoch 41/500, Train Loss: 0.3340, Val Loss: 0.4904\n",
      "Epoch 42/500, Train Loss: 0.3270, Val Loss: 0.4671\n",
      "Epoch 43/500, Train Loss: 0.2773, Val Loss: 0.4622\n",
      "Epoch 44/500, Train Loss: 0.2925, Val Loss: 0.4764\n",
      "Epoch 45/500, Train Loss: 0.3022, Val Loss: 0.4707\n",
      "Epoch 46/500, Train Loss: 0.2943, Val Loss: 0.4705\n",
      "Epoch 47/500, Train Loss: 0.2719, Val Loss: 0.4792\n",
      "Epoch 48/500, Train Loss: 0.2921, Val Loss: 0.4752\n",
      "Epoch 49/500, Train Loss: 0.2844, Val Loss: 0.4947\n",
      "Epoch 50/500, Train Loss: 0.3116, Val Loss: 0.4754\n",
      "Epoch 51/500, Train Loss: 0.2981, Val Loss: 0.4775\n",
      "Epoch 52/500, Train Loss: 0.2611, Val Loss: 0.4886\n",
      "Epoch 53/500, Train Loss: 0.3052, Val Loss: 0.5049\n",
      "Epoch 54/500, Train Loss: 0.3238, Val Loss: 0.4785\n",
      "Epoch 55/500, Train Loss: 0.2871, Val Loss: 0.4886\n",
      "Epoch 56/500, Train Loss: 0.2666, Val Loss: 0.5025\n",
      "Epoch 57/500, Train Loss: 0.2670, Val Loss: 0.4866\n",
      "Epoch 58/500, Train Loss: 0.2632, Val Loss: 0.4915\n",
      "Epoch 59/500, Train Loss: 0.2574, Val Loss: 0.5095\n",
      "Epoch 60/500, Train Loss: 0.2826, Val Loss: 0.4886\n",
      "Epoch 61/500, Train Loss: 0.2495, Val Loss: 0.4836\n",
      "Epoch 62/500, Train Loss: 0.2473, Val Loss: 0.4855\n",
      "Epoch 63/500, Train Loss: 0.2342, Val Loss: 0.5294\n",
      "Epoch 64/500, Train Loss: 0.3021, Val Loss: 0.4984\n",
      "Epoch 65/500, Train Loss: 0.2603, Val Loss: 0.4990\n",
      "Epoch 66/500, Train Loss: 0.2241, Val Loss: 0.4966\n",
      "Epoch 67/500, Train Loss: 0.2368, Val Loss: 0.4936\n",
      "Epoch 68/500, Train Loss: 0.2232, Val Loss: 0.5148\n",
      "Epoch 69/500, Train Loss: 0.2871, Val Loss: 0.5036\n",
      "Epoch 70/500, Train Loss: 0.2365, Val Loss: 0.4937\n",
      "Epoch 71/500, Train Loss: 0.2373, Val Loss: 0.4961\n",
      "Epoch 72/500, Train Loss: 0.2744, Val Loss: 0.4777\n",
      "Epoch 73/500, Train Loss: 0.2046, Val Loss: 0.4855\n",
      "Epoch 74/500, Train Loss: 0.2237, Val Loss: 0.5056\n",
      "Epoch 75/500, Train Loss: 0.2855, Val Loss: 0.4965\n",
      "Epoch 76/500, Train Loss: 0.2581, Val Loss: 0.4808\n",
      "Epoch 77/500, Train Loss: 0.2393, Val Loss: 0.4850\n",
      "Epoch 78/500, Train Loss: 0.2220, Val Loss: 0.5148\n",
      "Epoch 79/500, Train Loss: 0.2859, Val Loss: 0.4666\n",
      "Epoch 80/500, Train Loss: 0.2397, Val Loss: 0.4715\n",
      "Epoch 81/500, Train Loss: 0.2223, Val Loss: 0.4789\n",
      "Epoch 82/500, Train Loss: 0.2180, Val Loss: 0.4938\n",
      "Epoch 83/500, Train Loss: 0.2157, Val Loss: 0.5020\n",
      "Epoch 84/500, Train Loss: 0.2262, Val Loss: 0.4835\n",
      "Epoch 85/500, Train Loss: 0.2158, Val Loss: 0.4883\n",
      "Epoch 86/500, Train Loss: 0.2031, Val Loss: 0.4995\n",
      "Epoch 87/500, Train Loss: 0.1950, Val Loss: 0.5045\n",
      "Epoch 88/500, Train Loss: 0.2082, Val Loss: 0.4859\n",
      "Epoch 89/500, Train Loss: 0.2024, Val Loss: 0.4963\n",
      "Epoch 90/500, Train Loss: 0.2068, Val Loss: 0.4913\n",
      "Epoch 91/500, Train Loss: 0.1830, Val Loss: 0.5100\n",
      "Epoch 92/500, Train Loss: 0.2828, Val Loss: 0.4803\n",
      "Epoch 93/500, Train Loss: 0.2223, Val Loss: 0.5081\n",
      "Epoch 94/500, Train Loss: 0.2284, Val Loss: 0.4992\n",
      "Epoch 95/500, Train Loss: 0.2372, Val Loss: 0.4991\n",
      "Epoch 96/500, Train Loss: 0.2284, Val Loss: 0.4955\n",
      "Epoch 97/500, Train Loss: 0.2054, Val Loss: 0.4950\n",
      "Epoch 98/500, Train Loss: 0.2124, Val Loss: 0.5184\n",
      "Epoch 99/500, Train Loss: 0.1972, Val Loss: 0.5084\n",
      "Epoch 100/500, Train Loss: 0.1765, Val Loss: 0.5009\n",
      "Epoch 101/500, Train Loss: 0.1796, Val Loss: 0.5226\n",
      "Epoch 102/500, Train Loss: 0.2504, Val Loss: 0.5056\n",
      "Epoch 103/500, Train Loss: 0.1982, Val Loss: 0.5034\n",
      "Epoch 104/500, Train Loss: 0.2173, Val Loss: 0.4843\n",
      "Epoch 105/500, Train Loss: 0.1693, Val Loss: 0.4924\n",
      "Epoch 106/500, Train Loss: 0.1846, Val Loss: 0.4992\n",
      "Epoch 107/500, Train Loss: 0.1714, Val Loss: 0.5015\n",
      "Epoch 108/500, Train Loss: 0.1647, Val Loss: 0.5022\n",
      "Epoch 109/500, Train Loss: 0.1775, Val Loss: 0.4942\n",
      "Epoch 110/500, Train Loss: 0.1934, Val Loss: 0.4787\n",
      "Epoch 111/500, Train Loss: 0.1546, Val Loss: 0.4900\n",
      "Epoch 112/500, Train Loss: 0.1981, Val Loss: 0.4968\n",
      "Epoch 113/500, Train Loss: 0.1967, Val Loss: 0.4886\n",
      "Epoch 114/500, Train Loss: 0.1806, Val Loss: 0.4812\n",
      "Epoch 115/500, Train Loss: 0.1632, Val Loss: 0.4887\n",
      "Epoch 116/500, Train Loss: 0.1707, Val Loss: 0.4928\n",
      "Epoch 117/500, Train Loss: 0.1838, Val Loss: 0.4713\n",
      "Epoch 118/500, Train Loss: 0.1808, Val Loss: 0.4740\n",
      "Epoch 119/500, Train Loss: 0.1889, Val Loss: 0.5452\n",
      "Epoch 120/500, Train Loss: 0.1994, Val Loss: 0.4979\n",
      "Epoch 121/500, Train Loss: 0.1729, Val Loss: 0.4863\n",
      "Epoch 122/500, Train Loss: 0.1493, Val Loss: 0.4782\n",
      "Epoch 123/500, Train Loss: 0.1852, Val Loss: 0.4680\n",
      "Epoch 124/500, Train Loss: 0.1694, Val Loss: 0.4834\n",
      "Epoch 125/500, Train Loss: 0.1635, Val Loss: 0.5329\n",
      "Epoch 126/500, Train Loss: 0.1776, Val Loss: 0.4917\n",
      "Epoch 127/500, Train Loss: 0.1798, Val Loss: 0.5031\n",
      "Epoch 128/500, Train Loss: 0.1660, Val Loss: 0.4983\n",
      "Epoch 129/500, Train Loss: 0.1842, Val Loss: 0.4889\n",
      "Epoch 130/500, Train Loss: 0.1769, Val Loss: 0.4973\n",
      "Epoch 131/500, Train Loss: 0.1897, Val Loss: 0.5577\n",
      "Epoch 132/500, Train Loss: 0.2259, Val Loss: 0.5265\n",
      "Epoch 133/500, Train Loss: 0.1765, Val Loss: 0.5144\n",
      "Epoch 134/500, Train Loss: 0.1852, Val Loss: 0.5119\n",
      "Epoch 135/500, Train Loss: 0.2791, Val Loss: 0.4800\n",
      "Epoch 136/500, Train Loss: 0.1750, Val Loss: 0.5476\n",
      "Epoch 137/500, Train Loss: 0.2214, Val Loss: 0.5294\n",
      "Epoch 138/500, Train Loss: 0.2106, Val Loss: 0.4771\n",
      "Epoch 139/500, Train Loss: 0.2025, Val Loss: 0.4937\n",
      "Epoch 140/500, Train Loss: 0.2145, Val Loss: 0.4796\n",
      "Epoch 141/500, Train Loss: 0.1848, Val Loss: 0.4888\n",
      "Epoch 142/500, Train Loss: 0.1665, Val Loss: 0.5276\n",
      "Epoch 143/500, Train Loss: 0.1628, Val Loss: 0.5033\n",
      "Epoch 144/500, Train Loss: 0.1628, Val Loss: 0.4953\n",
      "Epoch 145/500, Train Loss: 0.1656, Val Loss: 0.5493\n",
      "Epoch 146/500, Train Loss: 0.1917, Val Loss: 0.4892\n",
      "Epoch 147/500, Train Loss: 0.1497, Val Loss: 0.5431\n",
      "Epoch 148/500, Train Loss: 0.1804, Val Loss: 0.4912\n",
      "Epoch 149/500, Train Loss: 0.1980, Val Loss: 0.4848\n",
      "Epoch 150/500, Train Loss: 0.1662, Val Loss: 0.4892\n",
      "Epoch 151/500, Train Loss: 0.1433, Val Loss: 0.5001\n",
      "Epoch 152/500, Train Loss: 0.1349, Val Loss: 0.5152\n",
      "Epoch 153/500, Train Loss: 0.1405, Val Loss: 0.4800\n",
      "Epoch 154/500, Train Loss: 0.1341, Val Loss: 0.4945\n",
      "Epoch 155/500, Train Loss: 0.1400, Val Loss: 0.4914\n",
      "Epoch 156/500, Train Loss: 0.1675, Val Loss: 0.5100\n",
      "Epoch 157/500, Train Loss: 0.1676, Val Loss: 0.4898\n",
      "Epoch 158/500, Train Loss: 0.1968, Val Loss: 0.4795\n",
      "Epoch 159/500, Train Loss: 0.1655, Val Loss: 0.5043\n",
      "Epoch 160/500, Train Loss: 0.1345, Val Loss: 0.4847\n",
      "Epoch 161/500, Train Loss: 0.1812, Val Loss: 0.4830\n",
      "Epoch 162/500, Train Loss: 0.1485, Val Loss: 0.5119\n",
      "Epoch 163/500, Train Loss: 0.1551, Val Loss: 0.5048\n",
      "Epoch 164/500, Train Loss: 0.1299, Val Loss: 0.5022\n",
      "Epoch 165/500, Train Loss: 0.1267, Val Loss: 0.4896\n",
      "Epoch 166/500, Train Loss: 0.1722, Val Loss: 0.5174\n",
      "Epoch 167/500, Train Loss: 0.1668, Val Loss: 0.4907\n",
      "Epoch 168/500, Train Loss: 0.1527, Val Loss: 0.5123\n",
      "Epoch 169/500, Train Loss: 0.2170, Val Loss: 0.4874\n",
      "Epoch 170/500, Train Loss: 0.1956, Val Loss: 0.5101\n",
      "Epoch 171/500, Train Loss: 0.1676, Val Loss: 0.4876\n",
      "Epoch 172/500, Train Loss: 0.1522, Val Loss: 0.4846\n",
      "Epoch 173/500, Train Loss: 0.1358, Val Loss: 0.4791\n",
      "Epoch 174/500, Train Loss: 0.1405, Val Loss: 0.5148\n",
      "Epoch 175/500, Train Loss: 0.1392, Val Loss: 0.4853\n",
      "Epoch 176/500, Train Loss: 0.1566, Val Loss: 0.5483\n",
      "Epoch 177/500, Train Loss: 0.1636, Val Loss: 0.4811\n",
      "Epoch 178/500, Train Loss: 0.2147, Val Loss: 0.4800\n",
      "Epoch 179/500, Train Loss: 0.2662, Val Loss: 0.4727\n",
      "Epoch 180/500, Train Loss: 0.2037, Val Loss: 0.4975\n",
      "Epoch 181/500, Train Loss: 0.2001, Val Loss: 0.4674\n",
      "Epoch 182/500, Train Loss: 0.2152, Val Loss: 0.4841\n",
      "Epoch 183/500, Train Loss: 0.1546, Val Loss: 0.4794\n",
      "Epoch 184/500, Train Loss: 0.1384, Val Loss: 0.4865\n",
      "Epoch 185/500, Train Loss: 0.1381, Val Loss: 0.5115\n",
      "Epoch 186/500, Train Loss: 0.1172, Val Loss: 0.4865\n",
      "Epoch 187/500, Train Loss: 0.1348, Val Loss: 0.4750\n",
      "Epoch 188/500, Train Loss: 0.1416, Val Loss: 0.5010\n",
      "Epoch 189/500, Train Loss: 0.1185, Val Loss: 0.4706\n",
      "Epoch 190/500, Train Loss: 0.2128, Val Loss: 0.4684\n",
      "Epoch 191/500, Train Loss: 0.1623, Val Loss: 0.4700\n",
      "Epoch 192/500, Train Loss: 0.1272, Val Loss: 0.4683\n",
      "Epoch 193/500, Train Loss: 0.1273, Val Loss: 0.4776\n",
      "Epoch 194/500, Train Loss: 0.1112, Val Loss: 0.4880\n",
      "Epoch 195/500, Train Loss: 0.1194, Val Loss: 0.4736\n",
      "Epoch 196/500, Train Loss: 0.1176, Val Loss: 0.4642\n",
      "Epoch 197/500, Train Loss: 0.1560, Val Loss: 0.4973\n",
      "Epoch 198/500, Train Loss: 0.1318, Val Loss: 0.4808\n",
      "Epoch 199/500, Train Loss: 0.1307, Val Loss: 0.4890\n",
      "Epoch 200/500, Train Loss: 0.1091, Val Loss: 0.4793\n",
      "Epoch 201/500, Train Loss: 0.1279, Val Loss: 0.4949\n",
      "Epoch 202/500, Train Loss: 0.1247, Val Loss: 0.5310\n",
      "Epoch 203/500, Train Loss: 0.1519, Val Loss: 0.4454\n",
      "Epoch 204/500, Train Loss: 0.1335, Val Loss: 0.5003\n",
      "Epoch 205/500, Train Loss: 0.1221, Val Loss: 0.4669\n",
      "Epoch 206/500, Train Loss: 0.1612, Val Loss: 0.4718\n",
      "Epoch 207/500, Train Loss: 0.1250, Val Loss: 0.4620\n",
      "Epoch 208/500, Train Loss: 0.1490, Val Loss: 0.4577\n",
      "Epoch 209/500, Train Loss: 0.1212, Val Loss: 0.4697\n",
      "Epoch 210/500, Train Loss: 0.1129, Val Loss: 0.5223\n",
      "Epoch 211/500, Train Loss: 0.1112, Val Loss: 0.5106\n",
      "Epoch 212/500, Train Loss: 0.1388, Val Loss: 0.4837\n",
      "Epoch 213/500, Train Loss: 0.1094, Val Loss: 0.4864\n",
      "Epoch 214/500, Train Loss: 0.1095, Val Loss: 0.4809\n",
      "Epoch 215/500, Train Loss: 0.1080, Val Loss: 0.4717\n",
      "Epoch 216/500, Train Loss: 0.1174, Val Loss: 0.4785\n",
      "Epoch 217/500, Train Loss: 0.1122, Val Loss: 0.4798\n",
      "Epoch 218/500, Train Loss: 0.1160, Val Loss: 0.4571\n",
      "Epoch 219/500, Train Loss: 0.1399, Val Loss: 0.4599\n",
      "Epoch 220/500, Train Loss: 0.1815, Val Loss: 0.4419\n",
      "Epoch 221/500, Train Loss: 0.1468, Val Loss: 0.5033\n",
      "Epoch 222/500, Train Loss: 0.1338, Val Loss: 0.4587\n",
      "Epoch 223/500, Train Loss: 0.1410, Val Loss: 0.4802\n",
      "Epoch 224/500, Train Loss: 0.1223, Val Loss: 0.5561\n",
      "Epoch 225/500, Train Loss: 0.1520, Val Loss: 0.4839\n",
      "Epoch 226/500, Train Loss: 0.1233, Val Loss: 0.5231\n",
      "Epoch 227/500, Train Loss: 0.1725, Val Loss: 0.4713\n",
      "Epoch 228/500, Train Loss: 0.1142, Val Loss: 0.5054\n",
      "Epoch 229/500, Train Loss: 0.1143, Val Loss: 0.4834\n",
      "Epoch 230/500, Train Loss: 0.1298, Val Loss: 0.4727\n",
      "Epoch 231/500, Train Loss: 0.1151, Val Loss: 0.4878\n",
      "Epoch 232/500, Train Loss: 0.1061, Val Loss: 0.4752\n",
      "Epoch 233/500, Train Loss: 0.1132, Val Loss: 0.4895\n",
      "Epoch 234/500, Train Loss: 0.1180, Val Loss: 0.5133\n",
      "Epoch 235/500, Train Loss: 0.1141, Val Loss: 0.4749\n",
      "Epoch 236/500, Train Loss: 0.1154, Val Loss: 0.4718\n",
      "Epoch 237/500, Train Loss: 0.1009, Val Loss: 0.4656\n",
      "Epoch 238/500, Train Loss: 0.1009, Val Loss: 0.4857\n",
      "Epoch 239/500, Train Loss: 0.1519, Val Loss: 0.4674\n",
      "Epoch 240/500, Train Loss: 0.1026, Val Loss: 0.4665\n",
      "Epoch 241/500, Train Loss: 0.1579, Val Loss: 0.5228\n",
      "Epoch 242/500, Train Loss: 0.1496, Val Loss: 0.4641\n",
      "Epoch 243/500, Train Loss: 0.1208, Val Loss: 0.4823\n",
      "Epoch 244/500, Train Loss: 0.1245, Val Loss: 0.4946\n",
      "Epoch 245/500, Train Loss: 0.1088, Val Loss: 0.4675\n",
      "Epoch 246/500, Train Loss: 0.1014, Val Loss: 0.4879\n",
      "Epoch 247/500, Train Loss: 0.0991, Val Loss: 0.4668\n",
      "Epoch 248/500, Train Loss: 0.1061, Val Loss: 0.5409\n",
      "Epoch 249/500, Train Loss: 0.1363, Val Loss: 0.4645\n",
      "Epoch 250/500, Train Loss: 0.1087, Val Loss: 0.4926\n",
      "Epoch 251/500, Train Loss: 0.1064, Val Loss: 0.5407\n",
      "Epoch 252/500, Train Loss: 0.1572, Val Loss: 0.5225\n",
      "Epoch 253/500, Train Loss: 0.1853, Val Loss: 0.4888\n",
      "Epoch 254/500, Train Loss: 0.1203, Val Loss: 0.4537\n",
      "Epoch 255/500, Train Loss: 0.0968, Val Loss: 0.4853\n",
      "Epoch 256/500, Train Loss: 0.0971, Val Loss: 0.4814\n",
      "Epoch 257/500, Train Loss: 0.0893, Val Loss: 0.4703\n",
      "Epoch 258/500, Train Loss: 0.0950, Val Loss: 0.5304\n",
      "Epoch 259/500, Train Loss: 0.1269, Val Loss: 0.4727\n",
      "Epoch 260/500, Train Loss: 0.1039, Val Loss: 0.4843\n",
      "Epoch 261/500, Train Loss: 0.1332, Val Loss: 0.4597\n",
      "Epoch 262/500, Train Loss: 0.1142, Val Loss: 0.4770\n",
      "Epoch 263/500, Train Loss: 0.1238, Val Loss: 0.5319\n",
      "Epoch 264/500, Train Loss: 0.1562, Val Loss: 0.5259\n",
      "Epoch 265/500, Train Loss: 0.1222, Val Loss: 0.4775\n",
      "Epoch 266/500, Train Loss: 0.1149, Val Loss: 0.4902\n",
      "Epoch 267/500, Train Loss: 0.1049, Val Loss: 0.4703\n",
      "Epoch 268/500, Train Loss: 0.1122, Val Loss: 0.5140\n",
      "Epoch 269/500, Train Loss: 0.0973, Val Loss: 0.4727\n",
      "Epoch 270/500, Train Loss: 0.0967, Val Loss: 0.4669\n",
      "Epoch 271/500, Train Loss: 0.1332, Val Loss: 0.4586\n",
      "Epoch 272/500, Train Loss: 0.1151, Val Loss: 0.4944\n",
      "Epoch 273/500, Train Loss: 0.0975, Val Loss: 0.4976\n",
      "Epoch 274/500, Train Loss: 0.1069, Val Loss: 0.5106\n",
      "Epoch 275/500, Train Loss: 0.1056, Val Loss: 0.4852\n",
      "Epoch 276/500, Train Loss: 0.1092, Val Loss: 0.4862\n",
      "Epoch 277/500, Train Loss: 0.0998, Val Loss: 0.4820\n",
      "Epoch 278/500, Train Loss: 0.1386, Val Loss: 0.4822\n",
      "Epoch 279/500, Train Loss: 0.1455, Val Loss: 0.5177\n",
      "Epoch 280/500, Train Loss: 0.1067, Val Loss: 0.4749\n",
      "Epoch 281/500, Train Loss: 0.0966, Val Loss: 0.5340\n",
      "Epoch 282/500, Train Loss: 0.1228, Val Loss: 0.4575\n",
      "Epoch 283/500, Train Loss: 0.0995, Val Loss: 0.4943\n",
      "Epoch 284/500, Train Loss: 0.1021, Val Loss: 0.4594\n",
      "Epoch 285/500, Train Loss: 0.1048, Val Loss: 0.4464\n",
      "Epoch 286/500, Train Loss: 0.1000, Val Loss: 0.4758\n",
      "Epoch 287/500, Train Loss: 0.1324, Val Loss: 0.5044\n",
      "Epoch 288/500, Train Loss: 0.1358, Val Loss: 0.4752\n",
      "Epoch 289/500, Train Loss: 0.0939, Val Loss: 0.4806\n",
      "Epoch 290/500, Train Loss: 0.1381, Val Loss: 0.5006\n",
      "Epoch 291/500, Train Loss: 0.1561, Val Loss: 0.4624\n",
      "Epoch 292/500, Train Loss: 0.1307, Val Loss: 0.4692\n",
      "Epoch 293/500, Train Loss: 0.1031, Val Loss: 0.4579\n",
      "Epoch 294/500, Train Loss: 0.1109, Val Loss: 0.4930\n",
      "Epoch 295/500, Train Loss: 0.1225, Val Loss: 0.4551\n",
      "Epoch 296/500, Train Loss: 0.1770, Val Loss: 0.4634\n",
      "Epoch 297/500, Train Loss: 0.0971, Val Loss: 0.4962\n",
      "Epoch 298/500, Train Loss: 0.1114, Val Loss: 0.4634\n",
      "Epoch 299/500, Train Loss: 0.1078, Val Loss: 0.5246\n",
      "Epoch 300/500, Train Loss: 0.1420, Val Loss: 0.4894\n",
      "Epoch 301/500, Train Loss: 0.0998, Val Loss: 0.4914\n",
      "Epoch 302/500, Train Loss: 0.1000, Val Loss: 0.4948\n",
      "Epoch 303/500, Train Loss: 0.0986, Val Loss: 0.4577\n",
      "Epoch 304/500, Train Loss: 0.1082, Val Loss: 0.5306\n",
      "Epoch 305/500, Train Loss: 0.1085, Val Loss: 0.4683\n",
      "Epoch 306/500, Train Loss: 0.0949, Val Loss: 0.4663\n",
      "Epoch 307/500, Train Loss: 0.1144, Val Loss: 0.4555\n",
      "Epoch 308/500, Train Loss: 0.0985, Val Loss: 0.4438\n",
      "Epoch 309/500, Train Loss: 0.1506, Val Loss: 0.4714\n",
      "Epoch 310/500, Train Loss: 0.1218, Val Loss: 0.4734\n",
      "Epoch 311/500, Train Loss: 0.1019, Val Loss: 0.4614\n",
      "Epoch 312/500, Train Loss: 0.0934, Val Loss: 0.4617\n",
      "Epoch 313/500, Train Loss: 0.1005, Val Loss: 0.4658\n",
      "Epoch 314/500, Train Loss: 0.0999, Val Loss: 0.4851\n",
      "Epoch 315/500, Train Loss: 0.1043, Val Loss: 0.4756\n",
      "Epoch 316/500, Train Loss: 0.0887, Val Loss: 0.4807\n",
      "Epoch 317/500, Train Loss: 0.1240, Val Loss: 0.4644\n",
      "Epoch 318/500, Train Loss: 0.0925, Val Loss: 0.4737\n",
      "Epoch 319/500, Train Loss: 0.0865, Val Loss: 0.4956\n",
      "Epoch 320/500, Train Loss: 0.1007, Val Loss: 0.4653\n",
      "Epoch 321/500, Train Loss: 0.0994, Val Loss: 0.4621\n",
      "Epoch 322/500, Train Loss: 0.0987, Val Loss: 0.4754\n",
      "Epoch 323/500, Train Loss: 0.0836, Val Loss: 0.4525\n",
      "Epoch 324/500, Train Loss: 0.1140, Val Loss: 0.4885\n",
      "Epoch 325/500, Train Loss: 0.1309, Val Loss: 0.4910\n",
      "Epoch 326/500, Train Loss: 0.0919, Val Loss: 0.4688\n",
      "Epoch 327/500, Train Loss: 0.1088, Val Loss: 0.4772\n",
      "Epoch 328/500, Train Loss: 0.1232, Val Loss: 0.4588\n",
      "Epoch 329/500, Train Loss: 0.1291, Val Loss: 0.4863\n",
      "Epoch 330/500, Train Loss: 0.1127, Val Loss: 0.4578\n",
      "Epoch 331/500, Train Loss: 0.0929, Val Loss: 0.4832\n",
      "Epoch 332/500, Train Loss: 0.0866, Val Loss: 0.4922\n",
      "Epoch 333/500, Train Loss: 0.1043, Val Loss: 0.4725\n",
      "Epoch 334/500, Train Loss: 0.0898, Val Loss: 0.4669\n",
      "Epoch 335/500, Train Loss: 0.1172, Val Loss: 0.4870\n",
      "Epoch 336/500, Train Loss: 0.1422, Val Loss: 0.4663\n",
      "Epoch 337/500, Train Loss: 0.1552, Val Loss: 0.4727\n",
      "Epoch 338/500, Train Loss: 0.1062, Val Loss: 0.4953\n",
      "Epoch 339/500, Train Loss: 0.0877, Val Loss: 0.4697\n",
      "Epoch 340/500, Train Loss: 0.0931, Val Loss: 0.4740\n",
      "Epoch 341/500, Train Loss: 0.0992, Val Loss: 0.4619\n",
      "Epoch 342/500, Train Loss: 0.1161, Val Loss: 0.4648\n",
      "Epoch 343/500, Train Loss: 0.1251, Val Loss: 0.5192\n",
      "Epoch 344/500, Train Loss: 0.1108, Val Loss: 0.4731\n",
      "Epoch 345/500, Train Loss: 0.0897, Val Loss: 0.4578\n",
      "Epoch 346/500, Train Loss: 0.1156, Val Loss: 0.5058\n",
      "Epoch 347/500, Train Loss: 0.1141, Val Loss: 0.4670\n",
      "Epoch 348/500, Train Loss: 0.1081, Val Loss: 0.4951\n",
      "Epoch 349/500, Train Loss: 0.0899, Val Loss: 0.4722\n",
      "Epoch 350/500, Train Loss: 0.1061, Val Loss: 0.5072\n",
      "Epoch 351/500, Train Loss: 0.1096, Val Loss: 0.4723\n",
      "Epoch 352/500, Train Loss: 0.0995, Val Loss: 0.4777\n",
      "Epoch 353/500, Train Loss: 0.0874, Val Loss: 0.4720\n",
      "Epoch 354/500, Train Loss: 0.0939, Val Loss: 0.4681\n",
      "Epoch 355/500, Train Loss: 0.1036, Val Loss: 0.4948\n",
      "Epoch 356/500, Train Loss: 0.1010, Val Loss: 0.4638\n",
      "Epoch 357/500, Train Loss: 0.0994, Val Loss: 0.5040\n",
      "Epoch 358/500, Train Loss: 0.0955, Val Loss: 0.5036\n",
      "Epoch 359/500, Train Loss: 0.1065, Val Loss: 0.4711\n",
      "Epoch 360/500, Train Loss: 0.1399, Val Loss: 0.4787\n",
      "Epoch 361/500, Train Loss: 0.1198, Val Loss: 0.4800\n",
      "Epoch 362/500, Train Loss: 0.1005, Val Loss: 0.4596\n",
      "Epoch 363/500, Train Loss: 0.0988, Val Loss: 0.5219\n",
      "Epoch 364/500, Train Loss: 0.1550, Val Loss: 0.4717\n",
      "Epoch 365/500, Train Loss: 0.1034, Val Loss: 0.4904\n",
      "Epoch 366/500, Train Loss: 0.1134, Val Loss: 0.4453\n",
      "Epoch 367/500, Train Loss: 0.1246, Val Loss: 0.4864\n",
      "Epoch 368/500, Train Loss: 0.1042, Val Loss: 0.4676\n",
      "Epoch 369/500, Train Loss: 0.1141, Val Loss: 0.4902\n",
      "Epoch 370/500, Train Loss: 0.1000, Val Loss: 0.4745\n",
      "Epoch 371/500, Train Loss: 0.1247, Val Loss: 0.5075\n",
      "Epoch 372/500, Train Loss: 0.0950, Val Loss: 0.4737\n",
      "Epoch 373/500, Train Loss: 0.1830, Val Loss: 0.4668\n",
      "Epoch 374/500, Train Loss: 0.1403, Val Loss: 0.4574\n",
      "Epoch 375/500, Train Loss: 0.0999, Val Loss: 0.5104\n",
      "Epoch 376/500, Train Loss: 0.0954, Val Loss: 0.4683\n",
      "Epoch 377/500, Train Loss: 0.0841, Val Loss: 0.4896\n",
      "Epoch 378/500, Train Loss: 0.0892, Val Loss: 0.4605\n",
      "Epoch 379/500, Train Loss: 0.1397, Val Loss: 0.4748\n",
      "Epoch 380/500, Train Loss: 0.1137, Val Loss: 0.4639\n",
      "Epoch 381/500, Train Loss: 0.1994, Val Loss: 0.4870\n",
      "Epoch 382/500, Train Loss: 0.1224, Val Loss: 0.4878\n",
      "Epoch 383/500, Train Loss: 0.1300, Val Loss: 0.4995\n",
      "Epoch 384/500, Train Loss: 0.1196, Val Loss: 0.4686\n",
      "Epoch 385/500, Train Loss: 0.1049, Val Loss: 0.4736\n",
      "Epoch 386/500, Train Loss: 0.0959, Val Loss: 0.4720\n",
      "Epoch 387/500, Train Loss: 0.1147, Val Loss: 0.4809\n",
      "Epoch 388/500, Train Loss: 0.1279, Val Loss: 0.5075\n",
      "Epoch 389/500, Train Loss: 0.1681, Val Loss: 0.4533\n",
      "Epoch 390/500, Train Loss: 0.1082, Val Loss: 0.5170\n",
      "Epoch 391/500, Train Loss: 0.1077, Val Loss: 0.4744\n",
      "Epoch 392/500, Train Loss: 0.0918, Val Loss: 0.4991\n",
      "Epoch 393/500, Train Loss: 0.0932, Val Loss: 0.4957\n",
      "Epoch 394/500, Train Loss: 0.0930, Val Loss: 0.4726\n",
      "Epoch 395/500, Train Loss: 0.1073, Val Loss: 0.5054\n",
      "Epoch 396/500, Train Loss: 0.1209, Val Loss: 0.4593\n",
      "Epoch 397/500, Train Loss: 0.1189, Val Loss: 0.4839\n",
      "Epoch 398/500, Train Loss: 0.0974, Val Loss: 0.4696\n",
      "Epoch 399/500, Train Loss: 0.1250, Val Loss: 0.4881\n",
      "Epoch 400/500, Train Loss: 0.0916, Val Loss: 0.4807\n",
      "Epoch 401/500, Train Loss: 0.1005, Val Loss: 0.4857\n",
      "Epoch 402/500, Train Loss: 0.0862, Val Loss: 0.4883\n",
      "Epoch 403/500, Train Loss: 0.0900, Val Loss: 0.4951\n",
      "Epoch 404/500, Train Loss: 0.0976, Val Loss: 0.5054\n",
      "Epoch 405/500, Train Loss: 0.1091, Val Loss: 0.4749\n",
      "Epoch 406/500, Train Loss: 0.1013, Val Loss: 0.4683\n",
      "Epoch 407/500, Train Loss: 0.0979, Val Loss: 0.4742\n",
      "Epoch 408/500, Train Loss: 0.0922, Val Loss: 0.4805\n",
      "Epoch 409/500, Train Loss: 0.0913, Val Loss: 0.4756\n",
      "Epoch 410/500, Train Loss: 0.1026, Val Loss: 0.5346\n",
      "Epoch 411/500, Train Loss: 0.1509, Val Loss: 0.4880\n",
      "Epoch 412/500, Train Loss: 0.1126, Val Loss: 0.4839\n",
      "Epoch 413/500, Train Loss: 0.1042, Val Loss: 0.4569\n",
      "Epoch 414/500, Train Loss: 0.0935, Val Loss: 0.4764\n",
      "Epoch 415/500, Train Loss: 0.0830, Val Loss: 0.5097\n",
      "Epoch 416/500, Train Loss: 0.1110, Val Loss: 0.4728\n",
      "Epoch 417/500, Train Loss: 0.0873, Val Loss: 0.4664\n",
      "Epoch 418/500, Train Loss: 0.1233, Val Loss: 0.5153\n",
      "Epoch 419/500, Train Loss: 0.1295, Val Loss: 0.4648\n",
      "Epoch 420/500, Train Loss: 0.1031, Val Loss: 0.5352\n",
      "Epoch 421/500, Train Loss: 0.1444, Val Loss: 0.4793\n",
      "Epoch 422/500, Train Loss: 0.1059, Val Loss: 0.4508\n",
      "Epoch 423/500, Train Loss: 0.1642, Val Loss: 0.4642\n",
      "Epoch 424/500, Train Loss: 0.1326, Val Loss: 0.5061\n",
      "Epoch 425/500, Train Loss: 0.1108, Val Loss: 0.4791\n",
      "Epoch 426/500, Train Loss: 0.1396, Val Loss: 0.4663\n",
      "Epoch 427/500, Train Loss: 0.1213, Val Loss: 0.4770\n",
      "Epoch 428/500, Train Loss: 0.1309, Val Loss: 0.4925\n",
      "Epoch 429/500, Train Loss: 0.1132, Val Loss: 0.4560\n",
      "Epoch 430/500, Train Loss: 0.2094, Val Loss: 0.4638\n",
      "Epoch 431/500, Train Loss: 0.1196, Val Loss: 0.4598\n",
      "Epoch 432/500, Train Loss: 0.1157, Val Loss: 0.4641\n",
      "Epoch 433/500, Train Loss: 0.1058, Val Loss: 0.4964\n",
      "Epoch 434/500, Train Loss: 0.1024, Val Loss: 0.4598\n",
      "Epoch 435/500, Train Loss: 0.0864, Val Loss: 0.4758\n",
      "Epoch 436/500, Train Loss: 0.0947, Val Loss: 0.4683\n",
      "Epoch 437/500, Train Loss: 0.0897, Val Loss: 0.4730\n",
      "Epoch 438/500, Train Loss: 0.0910, Val Loss: 0.4986\n",
      "Epoch 439/500, Train Loss: 0.1009, Val Loss: 0.4681\n",
      "Epoch 440/500, Train Loss: 0.1071, Val Loss: 0.4629\n",
      "Epoch 441/500, Train Loss: 0.0817, Val Loss: 0.4866\n",
      "Epoch 442/500, Train Loss: 0.0866, Val Loss: 0.4719\n",
      "Epoch 443/500, Train Loss: 0.0975, Val Loss: 0.4796\n",
      "Epoch 444/500, Train Loss: 0.1093, Val Loss: 0.4552\n",
      "Epoch 445/500, Train Loss: 0.1101, Val Loss: 0.5044\n",
      "Epoch 446/500, Train Loss: 0.1128, Val Loss: 0.4565\n",
      "Epoch 447/500, Train Loss: 0.1449, Val Loss: 0.4578\n",
      "Epoch 448/500, Train Loss: 0.1183, Val Loss: 0.4520\n",
      "Epoch 449/500, Train Loss: 0.1159, Val Loss: 0.4850\n",
      "Epoch 450/500, Train Loss: 0.1044, Val Loss: 0.4725\n",
      "Epoch 451/500, Train Loss: 0.0914, Val Loss: 0.4656\n",
      "Epoch 452/500, Train Loss: 0.0919, Val Loss: 0.4717\n",
      "Epoch 453/500, Train Loss: 0.0948, Val Loss: 0.4842\n",
      "Epoch 454/500, Train Loss: 0.1127, Val Loss: 0.4773\n",
      "Epoch 455/500, Train Loss: 0.0884, Val Loss: 0.4524\n",
      "Epoch 456/500, Train Loss: 0.1169, Val Loss: 0.4839\n",
      "Epoch 457/500, Train Loss: 0.0886, Val Loss: 0.4478\n",
      "Epoch 458/500, Train Loss: 0.1020, Val Loss: 0.4824\n",
      "Epoch 459/500, Train Loss: 0.1159, Val Loss: 0.4840\n",
      "Epoch 460/500, Train Loss: 0.0919, Val Loss: 0.5056\n",
      "Epoch 461/500, Train Loss: 0.1034, Val Loss: 0.4731\n",
      "Epoch 462/500, Train Loss: 0.0802, Val Loss: 0.4564\n",
      "Epoch 463/500, Train Loss: 0.0857, Val Loss: 0.5266\n",
      "Epoch 464/500, Train Loss: 0.1214, Val Loss: 0.4717\n",
      "Epoch 465/500, Train Loss: 0.0858, Val Loss: 0.4726\n",
      "Epoch 466/500, Train Loss: 0.0984, Val Loss: 0.4548\n",
      "Epoch 467/500, Train Loss: 0.1032, Val Loss: 0.4710\n",
      "Epoch 468/500, Train Loss: 0.0939, Val Loss: 0.4787\n",
      "Epoch 469/500, Train Loss: 0.1064, Val Loss: 0.5032\n",
      "Epoch 470/500, Train Loss: 0.0919, Val Loss: 0.4692\n",
      "Epoch 471/500, Train Loss: 0.0893, Val Loss: 0.4964\n",
      "Epoch 472/500, Train Loss: 0.1151, Val Loss: 0.4572\n",
      "Epoch 473/500, Train Loss: 0.1171, Val Loss: 0.4555\n",
      "Epoch 474/500, Train Loss: 0.0856, Val Loss: 0.4602\n",
      "Epoch 475/500, Train Loss: 0.0936, Val Loss: 0.4372\n",
      "Epoch 476/500, Train Loss: 0.1217, Val Loss: 0.4500\n",
      "Epoch 477/500, Train Loss: 0.0900, Val Loss: 0.4570\n",
      "Epoch 478/500, Train Loss: 0.0836, Val Loss: 0.4677\n",
      "Epoch 479/500, Train Loss: 0.0910, Val Loss: 0.4581\n",
      "Epoch 480/500, Train Loss: 0.1157, Val Loss: 0.4748\n",
      "Epoch 481/500, Train Loss: 0.1129, Val Loss: 0.4659\n",
      "Epoch 482/500, Train Loss: 0.0930, Val Loss: 0.4560\n",
      "Epoch 483/500, Train Loss: 0.1093, Val Loss: 0.4754\n",
      "Epoch 484/500, Train Loss: 0.0823, Val Loss: 0.4732\n",
      "Epoch 485/500, Train Loss: 0.1111, Val Loss: 0.4610\n",
      "Epoch 486/500, Train Loss: 0.1483, Val Loss: 0.4890\n",
      "Epoch 487/500, Train Loss: 0.1033, Val Loss: 0.4676\n",
      "Epoch 488/500, Train Loss: 0.1002, Val Loss: 0.4655\n",
      "Epoch 489/500, Train Loss: 0.0945, Val Loss: 0.4693\n",
      "Epoch 490/500, Train Loss: 0.1440, Val Loss: 0.4643\n",
      "Epoch 491/500, Train Loss: 0.1632, Val Loss: 0.4848\n",
      "Epoch 492/500, Train Loss: 0.1121, Val Loss: 0.5192\n",
      "Epoch 493/500, Train Loss: 0.1123, Val Loss: 0.4812\n",
      "Epoch 494/500, Train Loss: 0.1009, Val Loss: 0.4571\n",
      "Epoch 495/500, Train Loss: 0.1186, Val Loss: 0.5524\n",
      "Epoch 496/500, Train Loss: 0.1307, Val Loss: 0.4758\n",
      "Epoch 497/500, Train Loss: 0.0945, Val Loss: 0.4656\n",
      "Epoch 498/500, Train Loss: 0.0923, Val Loss: 0.5565\n",
      "Epoch 499/500, Train Loss: 0.1174, Val Loss: 0.4679\n",
      "Epoch 500/500, Train Loss: 0.1400, Val Loss: 0.4716\n",
      "Training with hidden_dim=256, lr=0.001\n",
      "Epoch 1/500, Train Loss: 0.5541, Val Loss: 0.5526\n",
      "Epoch 2/500, Train Loss: 0.4901, Val Loss: 0.5303\n",
      "Epoch 3/500, Train Loss: 0.4237, Val Loss: 0.5246\n",
      "Epoch 4/500, Train Loss: 0.3863, Val Loss: 0.4500\n",
      "Epoch 5/500, Train Loss: 0.3472, Val Loss: 0.4779\n",
      "Epoch 6/500, Train Loss: 0.3611, Val Loss: 0.4604\n",
      "Epoch 7/500, Train Loss: 0.3362, Val Loss: 0.4519\n",
      "Epoch 8/500, Train Loss: 0.3156, Val Loss: 0.4587\n",
      "Epoch 9/500, Train Loss: 0.3317, Val Loss: 0.4624\n",
      "Epoch 10/500, Train Loss: 0.3064, Val Loss: 0.4658\n",
      "Epoch 11/500, Train Loss: 0.3098, Val Loss: 0.5426\n",
      "Epoch 12/500, Train Loss: 0.3555, Val Loss: 0.4900\n",
      "Epoch 13/500, Train Loss: 0.3048, Val Loss: 0.5322\n",
      "Epoch 14/500, Train Loss: 0.3293, Val Loss: 0.5205\n",
      "Epoch 15/500, Train Loss: 0.3334, Val Loss: 0.4786\n",
      "Epoch 16/500, Train Loss: 0.3011, Val Loss: 0.4843\n",
      "Epoch 17/500, Train Loss: 0.3011, Val Loss: 0.4802\n",
      "Epoch 18/500, Train Loss: 0.3112, Val Loss: 0.4730\n",
      "Epoch 19/500, Train Loss: 0.2926, Val Loss: 0.4724\n",
      "Epoch 20/500, Train Loss: 0.2891, Val Loss: 0.4810\n",
      "Epoch 21/500, Train Loss: 0.2873, Val Loss: 0.5456\n",
      "Epoch 22/500, Train Loss: 0.4145, Val Loss: 0.4737\n",
      "Epoch 23/500, Train Loss: 0.3245, Val Loss: 0.4576\n",
      "Epoch 24/500, Train Loss: 0.2902, Val Loss: 0.4635\n",
      "Epoch 25/500, Train Loss: 0.2991, Val Loss: 0.4731\n",
      "Epoch 26/500, Train Loss: 0.2984, Val Loss: 0.5010\n",
      "Epoch 27/500, Train Loss: 0.2879, Val Loss: 0.4853\n",
      "Epoch 28/500, Train Loss: 0.2710, Val Loss: 0.6379\n",
      "Epoch 29/500, Train Loss: 0.4218, Val Loss: 0.5447\n",
      "Epoch 30/500, Train Loss: 0.3540, Val Loss: 0.4603\n",
      "Epoch 31/500, Train Loss: 0.3074, Val Loss: 0.4655\n",
      "Epoch 32/500, Train Loss: 0.3096, Val Loss: 0.4604\n",
      "Epoch 33/500, Train Loss: 0.3037, Val Loss: 0.4634\n",
      "Epoch 34/500, Train Loss: 0.2942, Val Loss: 0.5247\n",
      "Epoch 35/500, Train Loss: 0.3299, Val Loss: 0.4593\n",
      "Epoch 36/500, Train Loss: 0.2771, Val Loss: 0.4811\n",
      "Epoch 37/500, Train Loss: 0.2850, Val Loss: 0.4835\n",
      "Epoch 38/500, Train Loss: 0.2934, Val Loss: 0.5119\n",
      "Epoch 39/500, Train Loss: 0.2748, Val Loss: 0.4972\n",
      "Epoch 40/500, Train Loss: 0.2745, Val Loss: 0.5131\n",
      "Epoch 41/500, Train Loss: 0.2562, Val Loss: 0.5368\n",
      "Epoch 42/500, Train Loss: 0.2901, Val Loss: 0.4858\n",
      "Epoch 43/500, Train Loss: 0.3000, Val Loss: 0.4729\n",
      "Epoch 44/500, Train Loss: 0.2755, Val Loss: 0.4940\n",
      "Epoch 45/500, Train Loss: 0.2488, Val Loss: 0.5109\n",
      "Epoch 46/500, Train Loss: 0.3054, Val Loss: 0.4910\n",
      "Epoch 47/500, Train Loss: 0.2820, Val Loss: 0.4785\n",
      "Epoch 48/500, Train Loss: 0.2707, Val Loss: 0.4883\n",
      "Epoch 49/500, Train Loss: 0.2665, Val Loss: 0.4991\n",
      "Epoch 50/500, Train Loss: 0.2692, Val Loss: 0.4880\n",
      "Epoch 51/500, Train Loss: 0.2451, Val Loss: 0.5059\n",
      "Epoch 52/500, Train Loss: 0.2451, Val Loss: 0.4970\n",
      "Epoch 53/500, Train Loss: 0.2912, Val Loss: 0.4932\n",
      "Epoch 54/500, Train Loss: 0.2573, Val Loss: 0.5022\n",
      "Epoch 55/500, Train Loss: 0.2795, Val Loss: 0.5197\n",
      "Epoch 56/500, Train Loss: 0.2420, Val Loss: 0.5070\n",
      "Epoch 57/500, Train Loss: 0.2368, Val Loss: 0.5189\n",
      "Epoch 58/500, Train Loss: 0.3200, Val Loss: 0.4801\n",
      "Epoch 59/500, Train Loss: 0.2852, Val Loss: 0.4892\n",
      "Epoch 60/500, Train Loss: 0.2470, Val Loss: 0.4951\n",
      "Epoch 61/500, Train Loss: 0.2672, Val Loss: 0.4718\n",
      "Epoch 62/500, Train Loss: 0.2500, Val Loss: 0.4626\n",
      "Epoch 63/500, Train Loss: 0.2508, Val Loss: 0.4910\n",
      "Epoch 64/500, Train Loss: 0.2330, Val Loss: 0.4934\n",
      "Epoch 65/500, Train Loss: 0.2068, Val Loss: 0.4906\n",
      "Epoch 66/500, Train Loss: 0.2169, Val Loss: 0.6094\n",
      "Epoch 67/500, Train Loss: 0.3500, Val Loss: 0.4843\n",
      "Epoch 68/500, Train Loss: 0.2668, Val Loss: 0.4884\n",
      "Epoch 69/500, Train Loss: 0.2924, Val Loss: 0.4845\n",
      "Epoch 70/500, Train Loss: 0.2201, Val Loss: 0.4918\n",
      "Epoch 71/500, Train Loss: 0.2204, Val Loss: 0.4878\n",
      "Epoch 72/500, Train Loss: 0.2098, Val Loss: 0.5612\n",
      "Epoch 73/500, Train Loss: 0.3540, Val Loss: 0.4878\n",
      "Epoch 74/500, Train Loss: 0.3155, Val Loss: 0.4776\n",
      "Epoch 75/500, Train Loss: 0.2318, Val Loss: 0.5405\n",
      "Epoch 76/500, Train Loss: 0.2765, Val Loss: 0.4800\n",
      "Epoch 77/500, Train Loss: 0.2715, Val Loss: 0.4880\n",
      "Epoch 78/500, Train Loss: 0.2347, Val Loss: 0.4956\n",
      "Epoch 79/500, Train Loss: 0.2312, Val Loss: 0.5283\n",
      "Epoch 80/500, Train Loss: 0.2237, Val Loss: 0.5105\n",
      "Epoch 81/500, Train Loss: 0.2346, Val Loss: 0.4920\n",
      "Epoch 82/500, Train Loss: 0.2527, Val Loss: 0.5006\n",
      "Epoch 83/500, Train Loss: 0.3075, Val Loss: 0.4857\n",
      "Epoch 84/500, Train Loss: 0.2324, Val Loss: 0.4717\n",
      "Epoch 85/500, Train Loss: 0.2273, Val Loss: 0.5347\n",
      "Epoch 86/500, Train Loss: 0.2183, Val Loss: 0.5276\n",
      "Epoch 87/500, Train Loss: 0.3736, Val Loss: 0.4675\n",
      "Epoch 88/500, Train Loss: 0.2832, Val Loss: 0.4700\n",
      "Epoch 89/500, Train Loss: 0.3026, Val Loss: 0.4624\n",
      "Epoch 90/500, Train Loss: 0.2525, Val Loss: 0.4728\n",
      "Epoch 91/500, Train Loss: 0.2333, Val Loss: 0.4894\n",
      "Epoch 92/500, Train Loss: 0.2142, Val Loss: 0.5182\n",
      "Epoch 93/500, Train Loss: 0.2264, Val Loss: 0.4981\n",
      "Epoch 94/500, Train Loss: 0.2014, Val Loss: 0.5291\n",
      "Epoch 95/500, Train Loss: 0.2223, Val Loss: 0.5016\n",
      "Epoch 96/500, Train Loss: 0.2098, Val Loss: 0.5023\n",
      "Epoch 97/500, Train Loss: 0.2867, Val Loss: 0.5465\n",
      "Epoch 98/500, Train Loss: 0.2417, Val Loss: 0.5024\n",
      "Epoch 99/500, Train Loss: 0.1667, Val Loss: 0.5237\n",
      "Epoch 100/500, Train Loss: 0.1790, Val Loss: 0.5019\n",
      "Epoch 101/500, Train Loss: 0.1875, Val Loss: 0.5227\n",
      "Epoch 102/500, Train Loss: 0.1831, Val Loss: 0.5022\n",
      "Epoch 103/500, Train Loss: 0.1899, Val Loss: 0.5007\n",
      "Epoch 104/500, Train Loss: 0.1869, Val Loss: 0.4953\n",
      "Epoch 105/500, Train Loss: 0.1972, Val Loss: 0.5098\n",
      "Epoch 106/500, Train Loss: 0.1668, Val Loss: 0.5015\n",
      "Epoch 107/500, Train Loss: 0.1747, Val Loss: 0.5407\n",
      "Epoch 108/500, Train Loss: 0.2461, Val Loss: 0.4796\n",
      "Epoch 109/500, Train Loss: 0.2141, Val Loss: 0.5349\n",
      "Epoch 110/500, Train Loss: 0.2204, Val Loss: 0.4939\n",
      "Epoch 111/500, Train Loss: 0.2122, Val Loss: 0.5228\n",
      "Epoch 112/500, Train Loss: 0.1914, Val Loss: 0.4879\n",
      "Epoch 113/500, Train Loss: 0.1727, Val Loss: 0.4952\n",
      "Epoch 114/500, Train Loss: 0.1564, Val Loss: 0.4882\n",
      "Epoch 115/500, Train Loss: 0.2198, Val Loss: 0.5543\n",
      "Epoch 116/500, Train Loss: 0.2458, Val Loss: 0.4901\n",
      "Epoch 117/500, Train Loss: 0.1673, Val Loss: 0.5037\n",
      "Epoch 118/500, Train Loss: 0.2183, Val Loss: 0.5209\n",
      "Epoch 119/500, Train Loss: 0.1695, Val Loss: 0.5253\n",
      "Epoch 120/500, Train Loss: 0.1741, Val Loss: 0.4819\n",
      "Epoch 121/500, Train Loss: 0.1758, Val Loss: 0.5070\n",
      "Epoch 122/500, Train Loss: 0.1820, Val Loss: 0.4969\n",
      "Epoch 123/500, Train Loss: 0.1901, Val Loss: 0.5633\n",
      "Epoch 124/500, Train Loss: 0.2034, Val Loss: 0.4823\n",
      "Epoch 125/500, Train Loss: 0.1604, Val Loss: 0.5275\n",
      "Epoch 126/500, Train Loss: 0.1742, Val Loss: 0.4982\n",
      "Epoch 127/500, Train Loss: 0.1809, Val Loss: 0.5021\n",
      "Epoch 128/500, Train Loss: 0.1676, Val Loss: 0.4783\n",
      "Epoch 129/500, Train Loss: 0.1658, Val Loss: 0.5542\n",
      "Epoch 130/500, Train Loss: 0.1703, Val Loss: 0.4623\n",
      "Epoch 131/500, Train Loss: 0.1736, Val Loss: 0.4915\n",
      "Epoch 132/500, Train Loss: 0.1667, Val Loss: 0.5192\n",
      "Epoch 133/500, Train Loss: 0.1945, Val Loss: 0.4875\n",
      "Epoch 134/500, Train Loss: 0.1943, Val Loss: 0.5417\n",
      "Epoch 135/500, Train Loss: 0.1544, Val Loss: 0.5124\n",
      "Epoch 136/500, Train Loss: 0.1326, Val Loss: 0.4969\n",
      "Epoch 137/500, Train Loss: 0.2342, Val Loss: 0.4644\n",
      "Epoch 138/500, Train Loss: 0.1890, Val Loss: 0.5218\n",
      "Epoch 139/500, Train Loss: 0.1652, Val Loss: 0.5052\n",
      "Epoch 140/500, Train Loss: 0.1536, Val Loss: 0.4949\n",
      "Epoch 141/500, Train Loss: 0.1550, Val Loss: 0.5206\n",
      "Epoch 142/500, Train Loss: 0.1665, Val Loss: 0.4847\n",
      "Epoch 143/500, Train Loss: 0.1558, Val Loss: 0.5840\n",
      "Epoch 144/500, Train Loss: 0.2096, Val Loss: 0.5050\n",
      "Epoch 145/500, Train Loss: 0.1660, Val Loss: 0.4951\n",
      "Epoch 146/500, Train Loss: 0.1480, Val Loss: 0.6376\n",
      "Epoch 147/500, Train Loss: 0.2388, Val Loss: 0.5007\n",
      "Epoch 148/500, Train Loss: 0.1854, Val Loss: 0.4705\n",
      "Epoch 149/500, Train Loss: 0.1484, Val Loss: 0.5131\n",
      "Epoch 150/500, Train Loss: 0.1568, Val Loss: 0.5730\n",
      "Epoch 151/500, Train Loss: 0.1574, Val Loss: 0.4852\n",
      "Epoch 152/500, Train Loss: 0.1409, Val Loss: 0.4788\n",
      "Epoch 153/500, Train Loss: 0.1278, Val Loss: 0.5419\n",
      "Epoch 154/500, Train Loss: 0.1499, Val Loss: 0.4870\n",
      "Epoch 155/500, Train Loss: 0.1410, Val Loss: 0.5048\n",
      "Epoch 156/500, Train Loss: 0.1550, Val Loss: 0.4775\n",
      "Epoch 157/500, Train Loss: 0.1348, Val Loss: 0.4883\n",
      "Epoch 158/500, Train Loss: 0.1506, Val Loss: 0.4648\n",
      "Epoch 159/500, Train Loss: 0.2136, Val Loss: 0.4857\n",
      "Epoch 160/500, Train Loss: 0.1499, Val Loss: 0.4906\n",
      "Epoch 161/500, Train Loss: 0.1233, Val Loss: 0.5092\n",
      "Epoch 162/500, Train Loss: 0.1322, Val Loss: 0.5317\n",
      "Epoch 163/500, Train Loss: 0.1583, Val Loss: 0.4805\n",
      "Epoch 164/500, Train Loss: 0.1215, Val Loss: 0.4739\n",
      "Epoch 165/500, Train Loss: 0.1867, Val Loss: 0.4780\n",
      "Epoch 166/500, Train Loss: 0.1556, Val Loss: 0.4644\n",
      "Epoch 167/500, Train Loss: 0.1461, Val Loss: 0.5480\n",
      "Epoch 168/500, Train Loss: 0.1507, Val Loss: 0.4616\n",
      "Epoch 169/500, Train Loss: 0.1371, Val Loss: 0.4904\n",
      "Epoch 170/500, Train Loss: 0.1285, Val Loss: 0.4756\n",
      "Epoch 171/500, Train Loss: 0.1318, Val Loss: 0.4735\n",
      "Epoch 172/500, Train Loss: 0.1795, Val Loss: 0.4855\n",
      "Epoch 173/500, Train Loss: 0.1199, Val Loss: 0.4720\n",
      "Epoch 174/500, Train Loss: 0.1352, Val Loss: 0.5207\n",
      "Epoch 175/500, Train Loss: 0.1531, Val Loss: 0.5383\n",
      "Epoch 176/500, Train Loss: 0.1250, Val Loss: 0.5132\n",
      "Epoch 177/500, Train Loss: 0.1368, Val Loss: 0.4886\n",
      "Epoch 178/500, Train Loss: 0.1153, Val Loss: 0.4791\n",
      "Epoch 179/500, Train Loss: 0.1896, Val Loss: 0.4644\n",
      "Epoch 180/500, Train Loss: 0.1491, Val Loss: 0.4775\n",
      "Epoch 181/500, Train Loss: 0.1669, Val Loss: 0.5929\n",
      "Epoch 182/500, Train Loss: 0.1546, Val Loss: 0.4869\n",
      "Epoch 183/500, Train Loss: 0.1980, Val Loss: 0.4669\n",
      "Epoch 184/500, Train Loss: 0.1614, Val Loss: 0.5135\n",
      "Epoch 185/500, Train Loss: 0.1878, Val Loss: 0.5328\n",
      "Epoch 186/500, Train Loss: 0.1528, Val Loss: 0.4979\n",
      "Epoch 187/500, Train Loss: 0.1238, Val Loss: 0.5103\n",
      "Epoch 188/500, Train Loss: 0.1109, Val Loss: 0.4631\n",
      "Epoch 189/500, Train Loss: 0.1140, Val Loss: 0.4915\n",
      "Epoch 190/500, Train Loss: 0.1150, Val Loss: 0.4771\n",
      "Epoch 191/500, Train Loss: 0.1265, Val Loss: 0.4944\n",
      "Epoch 192/500, Train Loss: 0.1264, Val Loss: 0.4997\n",
      "Epoch 193/500, Train Loss: 0.1269, Val Loss: 0.4975\n",
      "Epoch 194/500, Train Loss: 0.1221, Val Loss: 0.4678\n",
      "Epoch 195/500, Train Loss: 0.1283, Val Loss: 0.5466\n",
      "Epoch 196/500, Train Loss: 0.1436, Val Loss: 0.4667\n",
      "Epoch 197/500, Train Loss: 0.1198, Val Loss: 0.4746\n",
      "Epoch 198/500, Train Loss: 0.1263, Val Loss: 0.4870\n",
      "Epoch 199/500, Train Loss: 0.1096, Val Loss: 0.4741\n",
      "Epoch 200/500, Train Loss: 0.1382, Val Loss: 0.5132\n",
      "Epoch 201/500, Train Loss: 0.1412, Val Loss: 0.4753\n",
      "Epoch 202/500, Train Loss: 0.1376, Val Loss: 0.4793\n",
      "Epoch 203/500, Train Loss: 0.2561, Val Loss: 0.4370\n",
      "Epoch 204/500, Train Loss: 0.1566, Val Loss: 0.5723\n",
      "Epoch 205/500, Train Loss: 0.1822, Val Loss: 0.4660\n",
      "Epoch 206/500, Train Loss: 0.1384, Val Loss: 0.5034\n",
      "Epoch 207/500, Train Loss: 0.1174, Val Loss: 0.4909\n",
      "Epoch 208/500, Train Loss: 0.1094, Val Loss: 0.5398\n",
      "Epoch 209/500, Train Loss: 0.1207, Val Loss: 0.5081\n",
      "Epoch 210/500, Train Loss: 0.1041, Val Loss: 0.4956\n",
      "Epoch 211/500, Train Loss: 0.1237, Val Loss: 0.4868\n",
      "Epoch 212/500, Train Loss: 0.1126, Val Loss: 0.5032\n",
      "Epoch 213/500, Train Loss: 0.1180, Val Loss: 0.4802\n",
      "Epoch 214/500, Train Loss: 0.1142, Val Loss: 0.4729\n",
      "Epoch 215/500, Train Loss: 0.1910, Val Loss: 0.4645\n",
      "Epoch 216/500, Train Loss: 0.2701, Val Loss: 0.4480\n",
      "Epoch 217/500, Train Loss: 0.1990, Val Loss: 0.5483\n",
      "Epoch 218/500, Train Loss: 0.1603, Val Loss: 0.4898\n",
      "Epoch 219/500, Train Loss: 0.1008, Val Loss: 0.4678\n",
      "Epoch 220/500, Train Loss: 0.1247, Val Loss: 0.6030\n",
      "Epoch 221/500, Train Loss: 0.1614, Val Loss: 0.4799\n",
      "Epoch 222/500, Train Loss: 0.1158, Val Loss: 0.4920\n",
      "Epoch 223/500, Train Loss: 0.1008, Val Loss: 0.5286\n",
      "Epoch 224/500, Train Loss: 0.1160, Val Loss: 0.5109\n",
      "Epoch 225/500, Train Loss: 0.1048, Val Loss: 0.4718\n",
      "Epoch 226/500, Train Loss: 0.1122, Val Loss: 0.4707\n",
      "Epoch 227/500, Train Loss: 0.1166, Val Loss: 0.6362\n",
      "Epoch 228/500, Train Loss: 0.2280, Val Loss: 0.5000\n",
      "Epoch 229/500, Train Loss: 0.1719, Val Loss: 0.4992\n",
      "Epoch 230/500, Train Loss: 0.1418, Val Loss: 0.5133\n",
      "Epoch 231/500, Train Loss: 0.1314, Val Loss: 0.4889\n",
      "Epoch 232/500, Train Loss: 0.1023, Val Loss: 0.4802\n",
      "Epoch 233/500, Train Loss: 0.1279, Val Loss: 0.5494\n",
      "Epoch 234/500, Train Loss: 0.1835, Val Loss: 0.4703\n",
      "Epoch 235/500, Train Loss: 0.1411, Val Loss: 0.4884\n",
      "Epoch 236/500, Train Loss: 0.1427, Val Loss: 0.5134\n",
      "Epoch 237/500, Train Loss: 0.1081, Val Loss: 0.4941\n",
      "Epoch 238/500, Train Loss: 0.0931, Val Loss: 0.4637\n",
      "Epoch 239/500, Train Loss: 0.1579, Val Loss: 0.4699\n",
      "Epoch 240/500, Train Loss: 0.1494, Val Loss: 0.5523\n",
      "Epoch 241/500, Train Loss: 0.1717, Val Loss: 0.5345\n",
      "Epoch 242/500, Train Loss: 0.1721, Val Loss: 0.4633\n",
      "Epoch 243/500, Train Loss: 0.1404, Val Loss: 0.5141\n",
      "Epoch 244/500, Train Loss: 0.1373, Val Loss: 0.4666\n",
      "Epoch 245/500, Train Loss: 0.1086, Val Loss: 0.5402\n",
      "Epoch 246/500, Train Loss: 0.1104, Val Loss: 0.4806\n",
      "Epoch 247/500, Train Loss: 0.1029, Val Loss: 0.4926\n",
      "Epoch 248/500, Train Loss: 0.0946, Val Loss: 0.4681\n",
      "Epoch 249/500, Train Loss: 0.1126, Val Loss: 0.4987\n",
      "Epoch 250/500, Train Loss: 0.1489, Val Loss: 0.4771\n",
      "Epoch 251/500, Train Loss: 0.1321, Val Loss: 0.4899\n",
      "Epoch 252/500, Train Loss: 0.1261, Val Loss: 0.4768\n",
      "Epoch 253/500, Train Loss: 0.1264, Val Loss: 0.5052\n",
      "Epoch 254/500, Train Loss: 0.1211, Val Loss: 0.4759\n",
      "Epoch 255/500, Train Loss: 0.0956, Val Loss: 0.5154\n",
      "Epoch 256/500, Train Loss: 0.1241, Val Loss: 0.5092\n",
      "Epoch 257/500, Train Loss: 0.1478, Val Loss: 0.4804\n",
      "Epoch 258/500, Train Loss: 0.1116, Val Loss: 0.4839\n",
      "Epoch 259/500, Train Loss: 0.1001, Val Loss: 0.4836\n",
      "Epoch 260/500, Train Loss: 0.0964, Val Loss: 0.5294\n",
      "Epoch 261/500, Train Loss: 0.1343, Val Loss: 0.4772\n",
      "Epoch 262/500, Train Loss: 0.1016, Val Loss: 0.4797\n",
      "Epoch 263/500, Train Loss: 0.1027, Val Loss: 0.4849\n",
      "Epoch 264/500, Train Loss: 0.1290, Val Loss: 0.4925\n",
      "Epoch 265/500, Train Loss: 0.0967, Val Loss: 0.4984\n",
      "Epoch 266/500, Train Loss: 0.1045, Val Loss: 0.4754\n",
      "Epoch 267/500, Train Loss: 0.1429, Val Loss: 0.5015\n",
      "Epoch 268/500, Train Loss: 0.0966, Val Loss: 0.4610\n",
      "Epoch 269/500, Train Loss: 0.1093, Val Loss: 0.4869\n",
      "Epoch 270/500, Train Loss: 0.1143, Val Loss: 0.4902\n",
      "Epoch 271/500, Train Loss: 0.2035, Val Loss: 0.4908\n",
      "Epoch 272/500, Train Loss: 0.2612, Val Loss: 0.4648\n",
      "Epoch 273/500, Train Loss: 0.2897, Val Loss: 0.4423\n",
      "Epoch 274/500, Train Loss: 0.2662, Val Loss: 0.4348\n",
      "Epoch 275/500, Train Loss: 0.2499, Val Loss: 0.4429\n",
      "Epoch 276/500, Train Loss: 0.1992, Val Loss: 0.4564\n",
      "Epoch 277/500, Train Loss: 0.1805, Val Loss: 0.4807\n",
      "Epoch 278/500, Train Loss: 0.1995, Val Loss: 0.5094\n",
      "Epoch 279/500, Train Loss: 0.1922, Val Loss: 0.5130\n",
      "Epoch 280/500, Train Loss: 0.1023, Val Loss: 0.5025\n",
      "Epoch 281/500, Train Loss: 0.1875, Val Loss: 0.4719\n",
      "Epoch 282/500, Train Loss: 0.1657, Val Loss: 0.5078\n",
      "Epoch 283/500, Train Loss: 0.1014, Val Loss: 0.4825\n",
      "Epoch 284/500, Train Loss: 0.1196, Val Loss: 0.4772\n",
      "Epoch 285/500, Train Loss: 0.1093, Val Loss: 0.5122\n",
      "Epoch 286/500, Train Loss: 0.1472, Val Loss: 0.5036\n",
      "Epoch 287/500, Train Loss: 0.1198, Val Loss: 0.5223\n",
      "Epoch 288/500, Train Loss: 0.1499, Val Loss: 0.5200\n",
      "Epoch 289/500, Train Loss: 0.1205, Val Loss: 0.4816\n",
      "Epoch 290/500, Train Loss: 0.1202, Val Loss: 0.5048\n",
      "Epoch 291/500, Train Loss: 0.0926, Val Loss: 0.5125\n",
      "Epoch 292/500, Train Loss: 0.1273, Val Loss: 0.4760\n",
      "Epoch 293/500, Train Loss: 0.0957, Val Loss: 0.4849\n",
      "Epoch 294/500, Train Loss: 0.1064, Val Loss: 0.5029\n",
      "Epoch 295/500, Train Loss: 0.1100, Val Loss: 0.5050\n",
      "Epoch 296/500, Train Loss: 0.1154, Val Loss: 0.4760\n",
      "Epoch 297/500, Train Loss: 0.0962, Val Loss: 0.5104\n",
      "Epoch 298/500, Train Loss: 0.1451, Val Loss: 0.4961\n",
      "Epoch 299/500, Train Loss: 0.1587, Val Loss: 0.5016\n",
      "Epoch 300/500, Train Loss: 0.1350, Val Loss: 0.5335\n",
      "Epoch 301/500, Train Loss: 0.1277, Val Loss: 0.4852\n",
      "Epoch 302/500, Train Loss: 0.1105, Val Loss: 0.5005\n",
      "Epoch 303/500, Train Loss: 0.1259, Val Loss: 0.5285\n",
      "Epoch 304/500, Train Loss: 0.1276, Val Loss: 0.4679\n",
      "Epoch 305/500, Train Loss: 0.1196, Val Loss: 0.5285\n",
      "Epoch 306/500, Train Loss: 0.1200, Val Loss: 0.4702\n",
      "Epoch 307/500, Train Loss: 0.0959, Val Loss: 0.5117\n",
      "Epoch 308/500, Train Loss: 0.1103, Val Loss: 0.4795\n",
      "Epoch 309/500, Train Loss: 0.1174, Val Loss: 0.4869\n",
      "Epoch 310/500, Train Loss: 0.1001, Val Loss: 0.4980\n",
      "Epoch 311/500, Train Loss: 0.0934, Val Loss: 0.4857\n",
      "Epoch 312/500, Train Loss: 0.1197, Val Loss: 0.4787\n",
      "Epoch 313/500, Train Loss: 0.1248, Val Loss: 0.4970\n",
      "Epoch 314/500, Train Loss: 0.0928, Val Loss: 0.4825\n",
      "Epoch 315/500, Train Loss: 0.0964, Val Loss: 0.4797\n",
      "Epoch 316/500, Train Loss: 0.1167, Val Loss: 0.6225\n",
      "Epoch 317/500, Train Loss: 0.1947, Val Loss: 0.5262\n",
      "Epoch 318/500, Train Loss: 0.1670, Val Loss: 0.4857\n",
      "Epoch 319/500, Train Loss: 0.1305, Val Loss: 0.4563\n",
      "Epoch 320/500, Train Loss: 0.1196, Val Loss: 0.5306\n",
      "Epoch 321/500, Train Loss: 0.1057, Val Loss: 0.4910\n",
      "Epoch 322/500, Train Loss: 0.1258, Val Loss: 0.5129\n",
      "Epoch 323/500, Train Loss: 0.0927, Val Loss: 0.5522\n",
      "Epoch 324/500, Train Loss: 0.1393, Val Loss: 0.4785\n",
      "Epoch 325/500, Train Loss: 0.0961, Val Loss: 0.4770\n",
      "Epoch 326/500, Train Loss: 0.1114, Val Loss: 0.5004\n",
      "Epoch 327/500, Train Loss: 0.1033, Val Loss: 0.4804\n",
      "Epoch 328/500, Train Loss: 0.1296, Val Loss: 0.5092\n",
      "Epoch 329/500, Train Loss: 0.0856, Val Loss: 0.5042\n",
      "Epoch 330/500, Train Loss: 0.0993, Val Loss: 0.4770\n",
      "Epoch 331/500, Train Loss: 0.1327, Val Loss: 0.4863\n",
      "Epoch 332/500, Train Loss: 0.1157, Val Loss: 0.5066\n",
      "Epoch 333/500, Train Loss: 0.0971, Val Loss: 0.4960\n",
      "Epoch 334/500, Train Loss: 0.0930, Val Loss: 0.4816\n",
      "Epoch 335/500, Train Loss: 0.1421, Val Loss: 0.4922\n",
      "Epoch 336/500, Train Loss: 0.1143, Val Loss: 0.4983\n",
      "Epoch 337/500, Train Loss: 0.1078, Val Loss: 0.4865\n",
      "Epoch 338/500, Train Loss: 0.1215, Val Loss: 0.4904\n",
      "Epoch 339/500, Train Loss: 0.1386, Val Loss: 0.4744\n",
      "Epoch 340/500, Train Loss: 0.1403, Val Loss: 0.5277\n",
      "Epoch 341/500, Train Loss: 0.1378, Val Loss: 0.5088\n",
      "Epoch 342/500, Train Loss: 0.0929, Val Loss: 0.5391\n",
      "Epoch 343/500, Train Loss: 0.1061, Val Loss: 0.4957\n",
      "Epoch 344/500, Train Loss: 0.1100, Val Loss: 0.4901\n",
      "Epoch 345/500, Train Loss: 0.1071, Val Loss: 0.5305\n",
      "Epoch 346/500, Train Loss: 0.1132, Val Loss: 0.5163\n",
      "Epoch 347/500, Train Loss: 0.1019, Val Loss: 0.5006\n",
      "Epoch 348/500, Train Loss: 0.0917, Val Loss: 0.4934\n",
      "Epoch 349/500, Train Loss: 0.0991, Val Loss: 0.4910\n",
      "Epoch 350/500, Train Loss: 0.0916, Val Loss: 0.4912\n",
      "Epoch 351/500, Train Loss: 0.1250, Val Loss: 0.4982\n",
      "Epoch 352/500, Train Loss: 0.0869, Val Loss: 0.4850\n",
      "Epoch 353/500, Train Loss: 0.0871, Val Loss: 0.4820\n",
      "Epoch 354/500, Train Loss: 0.0866, Val Loss: 0.5124\n",
      "Epoch 355/500, Train Loss: 0.0875, Val Loss: 0.5099\n",
      "Epoch 356/500, Train Loss: 0.0919, Val Loss: 0.5000\n",
      "Epoch 357/500, Train Loss: 0.0914, Val Loss: 0.4808\n",
      "Epoch 358/500, Train Loss: 0.1165, Val Loss: 0.4649\n",
      "Epoch 359/500, Train Loss: 0.1388, Val Loss: 0.5002\n",
      "Epoch 360/500, Train Loss: 0.1293, Val Loss: 0.5287\n",
      "Epoch 361/500, Train Loss: 0.1185, Val Loss: 0.4871\n",
      "Epoch 362/500, Train Loss: 0.0937, Val Loss: 0.4758\n",
      "Epoch 363/500, Train Loss: 0.1094, Val Loss: 0.4891\n",
      "Epoch 364/500, Train Loss: 0.0900, Val Loss: 0.4988\n",
      "Epoch 365/500, Train Loss: 0.1147, Val Loss: 0.4760\n",
      "Epoch 366/500, Train Loss: 0.1225, Val Loss: 0.4964\n",
      "Epoch 367/500, Train Loss: 0.1201, Val Loss: 0.5243\n",
      "Epoch 368/500, Train Loss: 0.1382, Val Loss: 0.5115\n",
      "Epoch 369/500, Train Loss: 0.0888, Val Loss: 0.4877\n",
      "Epoch 370/500, Train Loss: 0.0864, Val Loss: 0.5160\n",
      "Epoch 371/500, Train Loss: 0.0996, Val Loss: 0.4904\n",
      "Epoch 372/500, Train Loss: 0.1235, Val Loss: 0.5268\n",
      "Epoch 373/500, Train Loss: 0.1266, Val Loss: 0.4822\n",
      "Epoch 374/500, Train Loss: 0.2675, Val Loss: 0.4489\n",
      "Epoch 375/500, Train Loss: 0.1728, Val Loss: 0.4840\n",
      "Epoch 376/500, Train Loss: 0.1463, Val Loss: 0.4601\n",
      "Epoch 377/500, Train Loss: 0.1360, Val Loss: 0.4953\n",
      "Epoch 378/500, Train Loss: 0.1026, Val Loss: 0.5480\n",
      "Epoch 379/500, Train Loss: 0.1492, Val Loss: 0.5621\n",
      "Epoch 380/500, Train Loss: 0.1278, Val Loss: 0.4884\n",
      "Epoch 381/500, Train Loss: 0.1161, Val Loss: 0.5030\n",
      "Epoch 382/500, Train Loss: 0.0975, Val Loss: 0.5067\n",
      "Epoch 383/500, Train Loss: 0.1022, Val Loss: 0.5020\n",
      "Epoch 384/500, Train Loss: 0.0972, Val Loss: 0.4900\n",
      "Epoch 385/500, Train Loss: 0.0841, Val Loss: 0.4984\n",
      "Epoch 386/500, Train Loss: 0.0952, Val Loss: 0.5194\n",
      "Epoch 387/500, Train Loss: 0.1034, Val Loss: 0.4808\n",
      "Epoch 388/500, Train Loss: 0.1252, Val Loss: 0.4832\n",
      "Epoch 389/500, Train Loss: 0.1042, Val Loss: 0.4861\n",
      "Epoch 390/500, Train Loss: 0.0927, Val Loss: 0.4727\n",
      "Epoch 391/500, Train Loss: 0.0927, Val Loss: 0.5190\n",
      "Epoch 392/500, Train Loss: 0.0814, Val Loss: 0.5072\n",
      "Epoch 393/500, Train Loss: 0.0884, Val Loss: 0.4898\n",
      "Epoch 394/500, Train Loss: 0.1042, Val Loss: 0.4681\n",
      "Epoch 395/500, Train Loss: 0.0888, Val Loss: 0.4727\n",
      "Epoch 396/500, Train Loss: 0.1113, Val Loss: 0.4667\n",
      "Epoch 397/500, Train Loss: 0.0858, Val Loss: 0.4786\n",
      "Epoch 398/500, Train Loss: 0.0851, Val Loss: 0.4930\n",
      "Epoch 399/500, Train Loss: 0.1150, Val Loss: 0.4992\n",
      "Epoch 400/500, Train Loss: 0.1138, Val Loss: 0.4861\n",
      "Epoch 401/500, Train Loss: 0.1082, Val Loss: 0.4795\n",
      "Epoch 402/500, Train Loss: 0.1309, Val Loss: 0.5012\n",
      "Epoch 403/500, Train Loss: 0.1348, Val Loss: 0.4834\n",
      "Epoch 404/500, Train Loss: 0.1044, Val Loss: 0.5001\n",
      "Epoch 405/500, Train Loss: 0.1008, Val Loss: 0.4801\n",
      "Epoch 406/500, Train Loss: 0.0926, Val Loss: 0.5172\n",
      "Epoch 407/500, Train Loss: 0.1060, Val Loss: 0.4750\n",
      "Epoch 408/500, Train Loss: 0.1997, Val Loss: 0.4892\n",
      "Epoch 409/500, Train Loss: 0.1310, Val Loss: 0.4723\n",
      "Epoch 410/500, Train Loss: 0.1322, Val Loss: 0.5435\n",
      "Epoch 411/500, Train Loss: 0.1625, Val Loss: 0.5309\n",
      "Epoch 412/500, Train Loss: 0.1268, Val Loss: 0.4685\n",
      "Epoch 413/500, Train Loss: 0.1345, Val Loss: 0.4704\n",
      "Epoch 414/500, Train Loss: 0.0952, Val Loss: 0.4861\n",
      "Epoch 415/500, Train Loss: 0.0928, Val Loss: 0.4905\n",
      "Epoch 416/500, Train Loss: 0.1041, Val Loss: 0.4869\n",
      "Epoch 417/500, Train Loss: 0.0971, Val Loss: 0.4863\n",
      "Epoch 418/500, Train Loss: 0.0848, Val Loss: 0.4787\n",
      "Epoch 419/500, Train Loss: 0.0906, Val Loss: 0.4855\n",
      "Epoch 420/500, Train Loss: 0.0904, Val Loss: 0.4738\n",
      "Epoch 421/500, Train Loss: 0.1271, Val Loss: 0.4796\n",
      "Epoch 422/500, Train Loss: 0.1190, Val Loss: 0.5277\n",
      "Epoch 423/500, Train Loss: 0.1462, Val Loss: 0.5479\n",
      "Epoch 424/500, Train Loss: 0.1078, Val Loss: 0.4709\n",
      "Epoch 425/500, Train Loss: 0.1056, Val Loss: 0.4923\n",
      "Epoch 426/500, Train Loss: 0.0944, Val Loss: 0.5006\n",
      "Epoch 427/500, Train Loss: 0.1095, Val Loss: 0.4798\n",
      "Epoch 428/500, Train Loss: 0.0975, Val Loss: 0.4634\n",
      "Epoch 429/500, Train Loss: 0.1023, Val Loss: 0.4999\n",
      "Epoch 430/500, Train Loss: 0.0912, Val Loss: 0.4678\n",
      "Epoch 431/500, Train Loss: 0.0980, Val Loss: 0.4831\n",
      "Epoch 432/500, Train Loss: 0.0978, Val Loss: 0.4751\n",
      "Epoch 433/500, Train Loss: 0.1297, Val Loss: 0.4871\n",
      "Epoch 434/500, Train Loss: 0.0887, Val Loss: 0.5022\n",
      "Epoch 435/500, Train Loss: 0.0903, Val Loss: 0.4673\n",
      "Epoch 436/500, Train Loss: 0.1053, Val Loss: 0.4664\n",
      "Epoch 437/500, Train Loss: 0.1169, Val Loss: 0.4905\n",
      "Epoch 438/500, Train Loss: 0.0997, Val Loss: 0.4691\n",
      "Epoch 439/500, Train Loss: 0.1033, Val Loss: 0.4964\n",
      "Epoch 440/500, Train Loss: 0.0938, Val Loss: 0.4889\n",
      "Epoch 441/500, Train Loss: 0.1069, Val Loss: 0.4900\n",
      "Epoch 442/500, Train Loss: 0.0843, Val Loss: 0.4899\n",
      "Epoch 443/500, Train Loss: 0.1017, Val Loss: 0.4792\n",
      "Epoch 444/500, Train Loss: 0.1213, Val Loss: 0.4689\n",
      "Epoch 445/500, Train Loss: 0.1112, Val Loss: 0.5561\n",
      "Epoch 446/500, Train Loss: 0.1405, Val Loss: 0.4737\n",
      "Epoch 447/500, Train Loss: 0.1005, Val Loss: 0.4550\n",
      "Epoch 448/500, Train Loss: 0.1018, Val Loss: 0.4956\n",
      "Epoch 449/500, Train Loss: 0.0836, Val Loss: 0.4737\n",
      "Epoch 450/500, Train Loss: 0.1039, Val Loss: 0.4800\n",
      "Epoch 451/500, Train Loss: 0.1107, Val Loss: 0.4974\n",
      "Epoch 452/500, Train Loss: 0.0953, Val Loss: 0.5006\n",
      "Epoch 453/500, Train Loss: 0.0905, Val Loss: 0.4841\n",
      "Epoch 454/500, Train Loss: 0.0975, Val Loss: 0.4747\n",
      "Epoch 455/500, Train Loss: 0.0846, Val Loss: 0.4861\n",
      "Epoch 456/500, Train Loss: 0.0992, Val Loss: 0.4788\n",
      "Epoch 457/500, Train Loss: 0.0997, Val Loss: 0.5000\n",
      "Epoch 458/500, Train Loss: 0.1070, Val Loss: 0.4781\n",
      "Epoch 459/500, Train Loss: 0.0883, Val Loss: 0.5108\n",
      "Epoch 460/500, Train Loss: 0.0895, Val Loss: 0.4974\n",
      "Epoch 461/500, Train Loss: 0.0985, Val Loss: 0.4742\n",
      "Epoch 462/500, Train Loss: 0.1174, Val Loss: 0.4623\n",
      "Epoch 463/500, Train Loss: 0.1257, Val Loss: 0.4797\n",
      "Epoch 464/500, Train Loss: 0.0994, Val Loss: 0.4985\n",
      "Epoch 465/500, Train Loss: 0.1058, Val Loss: 0.4603\n",
      "Epoch 466/500, Train Loss: 0.1061, Val Loss: 0.4572\n",
      "Epoch 467/500, Train Loss: 0.1190, Val Loss: 0.4866\n",
      "Epoch 468/500, Train Loss: 0.1074, Val Loss: 0.5091\n",
      "Epoch 469/500, Train Loss: 0.1232, Val Loss: 0.4765\n",
      "Epoch 470/500, Train Loss: 0.1075, Val Loss: 0.4855\n",
      "Epoch 471/500, Train Loss: 0.1095, Val Loss: 0.4662\n",
      "Epoch 472/500, Train Loss: 0.0962, Val Loss: 0.5245\n",
      "Epoch 473/500, Train Loss: 0.1010, Val Loss: 0.4695\n",
      "Epoch 474/500, Train Loss: 0.1287, Val Loss: 0.4633\n",
      "Epoch 475/500, Train Loss: 0.1069, Val Loss: 0.5017\n",
      "Epoch 476/500, Train Loss: 0.1224, Val Loss: 0.4708\n",
      "Epoch 477/500, Train Loss: 0.1131, Val Loss: 0.4631\n",
      "Epoch 478/500, Train Loss: 0.0983, Val Loss: 0.5389\n",
      "Epoch 479/500, Train Loss: 0.1008, Val Loss: 0.4616\n",
      "Epoch 480/500, Train Loss: 0.1018, Val Loss: 0.4778\n",
      "Epoch 481/500, Train Loss: 0.0866, Val Loss: 0.4598\n",
      "Epoch 482/500, Train Loss: 0.1155, Val Loss: 0.4823\n",
      "Epoch 483/500, Train Loss: 0.1371, Val Loss: 0.4838\n",
      "Epoch 484/500, Train Loss: 0.1211, Val Loss: 0.4779\n",
      "Epoch 485/500, Train Loss: 0.1939, Val Loss: 0.4878\n",
      "Epoch 486/500, Train Loss: 0.1363, Val Loss: 0.4836\n",
      "Epoch 487/500, Train Loss: 0.0941, Val Loss: 0.4773\n",
      "Epoch 488/500, Train Loss: 0.1021, Val Loss: 0.4819\n",
      "Epoch 489/500, Train Loss: 0.1032, Val Loss: 0.4962\n",
      "Epoch 490/500, Train Loss: 0.0942, Val Loss: 0.4661\n",
      "Epoch 491/500, Train Loss: 0.1023, Val Loss: 0.4911\n",
      "Epoch 492/500, Train Loss: 0.0833, Val Loss: 0.4769\n",
      "Epoch 493/500, Train Loss: 0.0846, Val Loss: 0.5096\n",
      "Epoch 494/500, Train Loss: 0.0893, Val Loss: 0.4784\n",
      "Epoch 495/500, Train Loss: 0.1025, Val Loss: 0.5231\n",
      "Epoch 496/500, Train Loss: 0.0941, Val Loss: 0.4701\n",
      "Epoch 497/500, Train Loss: 0.0855, Val Loss: 0.4887\n",
      "Epoch 498/500, Train Loss: 0.0975, Val Loss: 0.4903\n",
      "Epoch 499/500, Train Loss: 0.1083, Val Loss: 0.4736\n",
      "Epoch 500/500, Train Loss: 0.1018, Val Loss: 0.5259\n",
      "Training with hidden_dim=256, lr=0.0005\n",
      "Epoch 1/500, Train Loss: 0.5247, Val Loss: 0.4538\n",
      "Epoch 2/500, Train Loss: 0.3478, Val Loss: 0.4487\n",
      "Epoch 3/500, Train Loss: 0.3529, Val Loss: 0.4623\n",
      "Epoch 4/500, Train Loss: 0.3311, Val Loss: 0.4694\n",
      "Epoch 5/500, Train Loss: 0.3509, Val Loss: 0.4758\n",
      "Epoch 6/500, Train Loss: 0.3733, Val Loss: 0.4826\n",
      "Epoch 7/500, Train Loss: 0.3625, Val Loss: 0.4749\n",
      "Epoch 8/500, Train Loss: 0.3496, Val Loss: 0.4695\n",
      "Epoch 9/500, Train Loss: 0.3117, Val Loss: 0.5113\n",
      "Epoch 10/500, Train Loss: 0.3400, Val Loss: 0.4666\n",
      "Epoch 11/500, Train Loss: 0.3110, Val Loss: 0.4688\n",
      "Epoch 12/500, Train Loss: 0.3193, Val Loss: 0.4834\n",
      "Epoch 13/500, Train Loss: 0.3183, Val Loss: 0.4823\n",
      "Epoch 14/500, Train Loss: 0.3315, Val Loss: 0.4893\n",
      "Epoch 15/500, Train Loss: 0.3328, Val Loss: 0.4888\n",
      "Epoch 16/500, Train Loss: 0.2937, Val Loss: 0.4888\n",
      "Epoch 17/500, Train Loss: 0.3391, Val Loss: 0.4827\n",
      "Epoch 18/500, Train Loss: 0.3128, Val Loss: 0.4873\n",
      "Epoch 19/500, Train Loss: 0.3231, Val Loss: 0.5169\n",
      "Epoch 20/500, Train Loss: 0.3065, Val Loss: 0.4662\n",
      "Epoch 21/500, Train Loss: 0.3111, Val Loss: 0.4745\n",
      "Epoch 22/500, Train Loss: 0.3173, Val Loss: 0.4828\n",
      "Epoch 23/500, Train Loss: 0.3136, Val Loss: 0.5628\n",
      "Epoch 24/500, Train Loss: 0.4367, Val Loss: 0.4674\n",
      "Epoch 25/500, Train Loss: 0.4097, Val Loss: 0.4770\n",
      "Epoch 26/500, Train Loss: 0.3246, Val Loss: 0.4576\n",
      "Epoch 27/500, Train Loss: 0.2841, Val Loss: 0.5461\n",
      "Epoch 28/500, Train Loss: 0.3917, Val Loss: 0.4930\n",
      "Epoch 29/500, Train Loss: 0.3018, Val Loss: 0.4574\n",
      "Epoch 30/500, Train Loss: 0.3151, Val Loss: 0.5202\n",
      "Epoch 31/500, Train Loss: 0.3612, Val Loss: 0.4554\n",
      "Epoch 32/500, Train Loss: 0.2927, Val Loss: 0.4576\n",
      "Epoch 33/500, Train Loss: 0.3203, Val Loss: 0.4660\n",
      "Epoch 34/500, Train Loss: 0.2953, Val Loss: 0.4725\n",
      "Epoch 35/500, Train Loss: 0.2993, Val Loss: 0.4736\n",
      "Epoch 36/500, Train Loss: 0.2822, Val Loss: 0.4616\n",
      "Epoch 37/500, Train Loss: 0.2740, Val Loss: 0.4718\n",
      "Epoch 38/500, Train Loss: 0.2937, Val Loss: 0.4793\n",
      "Epoch 39/500, Train Loss: 0.2818, Val Loss: 0.4870\n",
      "Epoch 40/500, Train Loss: 0.2641, Val Loss: 0.4812\n",
      "Epoch 41/500, Train Loss: 0.2771, Val Loss: 0.4809\n",
      "Epoch 42/500, Train Loss: 0.2753, Val Loss: 0.4852\n",
      "Epoch 43/500, Train Loss: 0.2792, Val Loss: 0.4936\n",
      "Epoch 44/500, Train Loss: 0.2821, Val Loss: 0.4874\n",
      "Epoch 45/500, Train Loss: 0.3086, Val Loss: 0.5223\n",
      "Epoch 46/500, Train Loss: 0.3573, Val Loss: 0.4596\n",
      "Epoch 47/500, Train Loss: 0.2989, Val Loss: 0.4625\n",
      "Epoch 48/500, Train Loss: 0.2841, Val Loss: 0.4702\n",
      "Epoch 49/500, Train Loss: 0.2965, Val Loss: 0.4693\n",
      "Epoch 50/500, Train Loss: 0.2577, Val Loss: 0.4969\n",
      "Epoch 51/500, Train Loss: 0.2830, Val Loss: 0.4807\n",
      "Epoch 52/500, Train Loss: 0.2763, Val Loss: 0.4816\n",
      "Epoch 53/500, Train Loss: 0.2776, Val Loss: 0.4821\n",
      "Epoch 54/500, Train Loss: 0.2527, Val Loss: 0.4829\n",
      "Epoch 55/500, Train Loss: 0.2680, Val Loss: 0.4842\n",
      "Epoch 56/500, Train Loss: 0.2841, Val Loss: 0.4844\n",
      "Epoch 57/500, Train Loss: 0.2541, Val Loss: 0.5255\n",
      "Epoch 58/500, Train Loss: 0.3072, Val Loss: 0.4913\n",
      "Epoch 59/500, Train Loss: 0.2621, Val Loss: 0.4904\n",
      "Epoch 60/500, Train Loss: 0.2719, Val Loss: 0.4965\n",
      "Epoch 61/500, Train Loss: 0.2515, Val Loss: 0.4896\n",
      "Epoch 62/500, Train Loss: 0.2299, Val Loss: 0.4979\n",
      "Epoch 63/500, Train Loss: 0.2480, Val Loss: 0.5008\n",
      "Epoch 64/500, Train Loss: 0.2729, Val Loss: 0.4970\n",
      "Epoch 65/500, Train Loss: 0.2806, Val Loss: 0.4936\n",
      "Epoch 66/500, Train Loss: 0.2502, Val Loss: 0.4823\n",
      "Epoch 67/500, Train Loss: 0.2199, Val Loss: 0.5221\n",
      "Epoch 68/500, Train Loss: 0.2888, Val Loss: 0.4862\n",
      "Epoch 69/500, Train Loss: 0.2534, Val Loss: 0.4919\n",
      "Epoch 70/500, Train Loss: 0.2526, Val Loss: 0.5062\n",
      "Epoch 71/500, Train Loss: 0.2865, Val Loss: 0.4802\n",
      "Epoch 72/500, Train Loss: 0.2649, Val Loss: 0.4922\n",
      "Epoch 73/500, Train Loss: 0.2315, Val Loss: 0.4891\n",
      "Epoch 74/500, Train Loss: 0.2422, Val Loss: 0.5048\n",
      "Epoch 75/500, Train Loss: 0.1982, Val Loss: 0.5164\n",
      "Epoch 76/500, Train Loss: 0.2119, Val Loss: 0.5438\n",
      "Epoch 77/500, Train Loss: 0.2420, Val Loss: 0.4887\n",
      "Epoch 78/500, Train Loss: 0.2301, Val Loss: 0.5072\n",
      "Epoch 79/500, Train Loss: 0.2057, Val Loss: 0.5017\n",
      "Epoch 80/500, Train Loss: 0.1906, Val Loss: 0.5476\n",
      "Epoch 81/500, Train Loss: 0.2729, Val Loss: 0.5030\n",
      "Epoch 82/500, Train Loss: 0.2714, Val Loss: 0.5383\n",
      "Epoch 83/500, Train Loss: 0.2587, Val Loss: 0.5017\n",
      "Epoch 84/500, Train Loss: 0.2216, Val Loss: 0.5108\n",
      "Epoch 85/500, Train Loss: 0.2080, Val Loss: 0.4967\n",
      "Epoch 86/500, Train Loss: 0.2199, Val Loss: 0.5010\n",
      "Epoch 87/500, Train Loss: 0.1905, Val Loss: 0.5082\n",
      "Epoch 88/500, Train Loss: 0.2359, Val Loss: 0.4889\n",
      "Epoch 89/500, Train Loss: 0.2064, Val Loss: 0.5125\n",
      "Epoch 90/500, Train Loss: 0.1960, Val Loss: 0.4920\n",
      "Epoch 91/500, Train Loss: 0.2139, Val Loss: 0.5074\n",
      "Epoch 92/500, Train Loss: 0.2143, Val Loss: 0.4954\n",
      "Epoch 93/500, Train Loss: 0.1943, Val Loss: 0.4920\n",
      "Epoch 94/500, Train Loss: 0.1669, Val Loss: 0.4912\n",
      "Epoch 95/500, Train Loss: 0.2442, Val Loss: 0.4915\n",
      "Epoch 96/500, Train Loss: 0.2493, Val Loss: 0.5286\n",
      "Epoch 97/500, Train Loss: 0.2513, Val Loss: 0.5003\n",
      "Epoch 98/500, Train Loss: 0.1930, Val Loss: 0.5124\n",
      "Epoch 99/500, Train Loss: 0.2097, Val Loss: 0.5317\n",
      "Epoch 100/500, Train Loss: 0.2043, Val Loss: 0.4918\n",
      "Epoch 101/500, Train Loss: 0.1967, Val Loss: 0.4890\n",
      "Epoch 102/500, Train Loss: 0.2345, Val Loss: 0.5139\n",
      "Epoch 103/500, Train Loss: 0.2413, Val Loss: 0.5093\n",
      "Epoch 104/500, Train Loss: 0.2045, Val Loss: 0.4839\n",
      "Epoch 105/500, Train Loss: 0.1858, Val Loss: 0.4980\n",
      "Epoch 106/500, Train Loss: 0.1864, Val Loss: 0.5036\n",
      "Epoch 107/500, Train Loss: 0.1573, Val Loss: 0.5021\n",
      "Epoch 108/500, Train Loss: 0.1544, Val Loss: 0.5687\n",
      "Epoch 109/500, Train Loss: 0.2367, Val Loss: 0.4955\n",
      "Epoch 110/500, Train Loss: 0.1837, Val Loss: 0.5252\n",
      "Epoch 111/500, Train Loss: 0.1730, Val Loss: 0.5278\n",
      "Epoch 112/500, Train Loss: 0.1964, Val Loss: 0.5017\n",
      "Epoch 113/500, Train Loss: 0.1646, Val Loss: 0.5180\n",
      "Epoch 114/500, Train Loss: 0.1903, Val Loss: 0.5401\n",
      "Epoch 115/500, Train Loss: 0.2149, Val Loss: 0.5517\n",
      "Epoch 116/500, Train Loss: 0.2143, Val Loss: 0.5135\n",
      "Epoch 117/500, Train Loss: 0.1903, Val Loss: 0.5201\n",
      "Epoch 118/500, Train Loss: 0.1604, Val Loss: 0.5256\n",
      "Epoch 119/500, Train Loss: 0.1755, Val Loss: 0.5109\n",
      "Epoch 120/500, Train Loss: 0.1752, Val Loss: 0.5170\n",
      "Epoch 121/500, Train Loss: 0.1359, Val Loss: 0.5090\n",
      "Epoch 122/500, Train Loss: 0.1765, Val Loss: 0.5449\n",
      "Epoch 123/500, Train Loss: 0.1721, Val Loss: 0.5041\n",
      "Epoch 124/500, Train Loss: 0.1552, Val Loss: 0.5066\n",
      "Epoch 125/500, Train Loss: 0.2610, Val Loss: 0.4930\n",
      "Epoch 126/500, Train Loss: 0.2023, Val Loss: 0.5114\n",
      "Epoch 127/500, Train Loss: 0.1893, Val Loss: 0.5071\n",
      "Epoch 128/500, Train Loss: 0.1520, Val Loss: 0.4898\n",
      "Epoch 129/500, Train Loss: 0.1680, Val Loss: 0.5154\n",
      "Epoch 130/500, Train Loss: 0.1690, Val Loss: 0.4915\n",
      "Epoch 131/500, Train Loss: 0.1592, Val Loss: 0.5086\n",
      "Epoch 132/500, Train Loss: 0.1625, Val Loss: 0.5051\n",
      "Epoch 133/500, Train Loss: 0.1438, Val Loss: 0.5097\n",
      "Epoch 134/500, Train Loss: 0.1429, Val Loss: 0.4849\n",
      "Epoch 135/500, Train Loss: 0.1527, Val Loss: 0.5405\n",
      "Epoch 136/500, Train Loss: 0.1559, Val Loss: 0.4830\n",
      "Epoch 137/500, Train Loss: 0.1582, Val Loss: 0.4968\n",
      "Epoch 138/500, Train Loss: 0.1775, Val Loss: 0.4899\n",
      "Epoch 139/500, Train Loss: 0.1815, Val Loss: 0.5220\n",
      "Epoch 140/500, Train Loss: 0.1471, Val Loss: 0.5465\n",
      "Epoch 141/500, Train Loss: 0.1637, Val Loss: 0.4848\n",
      "Epoch 142/500, Train Loss: 0.1399, Val Loss: 0.5501\n",
      "Epoch 143/500, Train Loss: 0.2106, Val Loss: 0.4826\n",
      "Epoch 144/500, Train Loss: 0.1827, Val Loss: 0.4965\n",
      "Epoch 145/500, Train Loss: 0.1332, Val Loss: 0.4947\n",
      "Epoch 146/500, Train Loss: 0.1265, Val Loss: 0.4986\n",
      "Epoch 147/500, Train Loss: 0.1181, Val Loss: 0.4992\n",
      "Epoch 148/500, Train Loss: 0.1322, Val Loss: 0.4962\n",
      "Epoch 149/500, Train Loss: 0.1431, Val Loss: 0.5164\n",
      "Epoch 150/500, Train Loss: 0.1685, Val Loss: 0.4991\n",
      "Epoch 151/500, Train Loss: 0.1334, Val Loss: 0.4991\n",
      "Epoch 152/500, Train Loss: 0.1548, Val Loss: 0.5030\n",
      "Epoch 153/500, Train Loss: 0.1387, Val Loss: 0.4949\n",
      "Epoch 154/500, Train Loss: 0.1548, Val Loss: 0.5432\n",
      "Epoch 155/500, Train Loss: 0.1471, Val Loss: 0.4960\n",
      "Epoch 156/500, Train Loss: 0.1339, Val Loss: 0.5166\n",
      "Epoch 157/500, Train Loss: 0.1308, Val Loss: 0.5281\n",
      "Epoch 158/500, Train Loss: 0.1465, Val Loss: 0.5086\n",
      "Epoch 159/500, Train Loss: 0.1253, Val Loss: 0.4993\n",
      "Epoch 160/500, Train Loss: 0.1789, Val Loss: 0.4948\n",
      "Epoch 161/500, Train Loss: 0.1532, Val Loss: 0.4810\n",
      "Epoch 162/500, Train Loss: 0.1615, Val Loss: 0.4689\n",
      "Epoch 163/500, Train Loss: 0.1640, Val Loss: 0.5323\n",
      "Epoch 164/500, Train Loss: 0.1510, Val Loss: 0.5085\n",
      "Epoch 165/500, Train Loss: 0.1739, Val Loss: 0.4907\n",
      "Epoch 166/500, Train Loss: 0.1689, Val Loss: 0.4783\n",
      "Epoch 167/500, Train Loss: 0.1207, Val Loss: 0.4986\n",
      "Epoch 168/500, Train Loss: 0.1966, Val Loss: 0.4751\n",
      "Epoch 169/500, Train Loss: 0.1832, Val Loss: 0.6119\n",
      "Epoch 170/500, Train Loss: 0.2106, Val Loss: 0.4844\n",
      "Epoch 171/500, Train Loss: 0.1652, Val Loss: 0.4950\n",
      "Epoch 172/500, Train Loss: 0.1528, Val Loss: 0.5124\n",
      "Epoch 173/500, Train Loss: 0.1468, Val Loss: 0.5038\n",
      "Epoch 174/500, Train Loss: 0.1395, Val Loss: 0.4975\n",
      "Epoch 175/500, Train Loss: 0.1838, Val Loss: 0.4913\n",
      "Epoch 176/500, Train Loss: 0.1361, Val Loss: 0.4920\n",
      "Epoch 177/500, Train Loss: 0.1272, Val Loss: 0.4928\n",
      "Epoch 178/500, Train Loss: 0.1651, Val Loss: 0.5495\n",
      "Epoch 179/500, Train Loss: 0.2201, Val Loss: 0.5630\n",
      "Epoch 180/500, Train Loss: 0.2114, Val Loss: 0.5571\n",
      "Epoch 181/500, Train Loss: 0.1589, Val Loss: 0.4974\n",
      "Epoch 182/500, Train Loss: 0.2056, Val Loss: 0.4803\n",
      "Epoch 183/500, Train Loss: 0.1631, Val Loss: 0.4792\n",
      "Epoch 184/500, Train Loss: 0.1362, Val Loss: 0.5089\n",
      "Epoch 185/500, Train Loss: 0.1548, Val Loss: 0.5000\n",
      "Epoch 186/500, Train Loss: 0.1429, Val Loss: 0.5167\n",
      "Epoch 187/500, Train Loss: 0.1484, Val Loss: 0.4896\n",
      "Epoch 188/500, Train Loss: 0.1254, Val Loss: 0.4887\n",
      "Epoch 189/500, Train Loss: 0.1551, Val Loss: 0.4944\n",
      "Epoch 190/500, Train Loss: 0.1191, Val Loss: 0.5293\n",
      "Epoch 191/500, Train Loss: 0.1321, Val Loss: 0.4916\n",
      "Epoch 192/500, Train Loss: 0.1118, Val Loss: 0.4901\n",
      "Epoch 193/500, Train Loss: 0.1911, Val Loss: 0.4662\n",
      "Epoch 194/500, Train Loss: 0.1425, Val Loss: 0.5237\n",
      "Epoch 195/500, Train Loss: 0.1373, Val Loss: 0.4936\n",
      "Epoch 196/500, Train Loss: 0.1543, Val Loss: 0.5275\n",
      "Epoch 197/500, Train Loss: 0.1742, Val Loss: 0.5013\n",
      "Epoch 198/500, Train Loss: 0.1297, Val Loss: 0.4862\n",
      "Epoch 199/500, Train Loss: 0.1362, Val Loss: 0.4849\n",
      "Epoch 200/500, Train Loss: 0.1177, Val Loss: 0.4835\n",
      "Epoch 201/500, Train Loss: 0.1169, Val Loss: 0.4756\n",
      "Epoch 202/500, Train Loss: 0.1184, Val Loss: 0.5365\n",
      "Epoch 203/500, Train Loss: 0.1838, Val Loss: 0.5748\n",
      "Epoch 204/500, Train Loss: 0.1886, Val Loss: 0.5100\n",
      "Epoch 205/500, Train Loss: 0.1134, Val Loss: 0.4826\n",
      "Epoch 206/500, Train Loss: 0.1127, Val Loss: 0.4851\n",
      "Epoch 207/500, Train Loss: 0.1081, Val Loss: 0.4955\n",
      "Epoch 208/500, Train Loss: 0.1462, Val Loss: 0.4727\n",
      "Epoch 209/500, Train Loss: 0.1635, Val Loss: 0.5898\n",
      "Epoch 210/500, Train Loss: 0.2134, Val Loss: 0.5121\n",
      "Epoch 211/500, Train Loss: 0.1319, Val Loss: 0.4831\n",
      "Epoch 212/500, Train Loss: 0.1108, Val Loss: 0.4848\n",
      "Epoch 213/500, Train Loss: 0.1089, Val Loss: 0.4871\n",
      "Epoch 214/500, Train Loss: 0.1099, Val Loss: 0.4780\n",
      "Epoch 215/500, Train Loss: 0.1089, Val Loss: 0.4986\n",
      "Epoch 216/500, Train Loss: 0.1141, Val Loss: 0.4970\n",
      "Epoch 217/500, Train Loss: 0.1736, Val Loss: 0.4793\n",
      "Epoch 218/500, Train Loss: 0.1603, Val Loss: 0.5247\n",
      "Epoch 219/500, Train Loss: 0.1055, Val Loss: 0.4777\n",
      "Epoch 220/500, Train Loss: 0.1077, Val Loss: 0.4834\n",
      "Epoch 221/500, Train Loss: 0.1302, Val Loss: 0.4963\n",
      "Epoch 222/500, Train Loss: 0.1350, Val Loss: 0.4827\n",
      "Epoch 223/500, Train Loss: 0.1829, Val Loss: 0.4896\n",
      "Epoch 224/500, Train Loss: 0.1256, Val Loss: 0.4972\n",
      "Epoch 225/500, Train Loss: 0.1246, Val Loss: 0.4711\n",
      "Epoch 226/500, Train Loss: 0.1746, Val Loss: 0.4719\n",
      "Epoch 227/500, Train Loss: 0.1413, Val Loss: 0.5511\n",
      "Epoch 228/500, Train Loss: 0.1776, Val Loss: 0.4822\n",
      "Epoch 229/500, Train Loss: 0.1366, Val Loss: 0.4997\n",
      "Epoch 230/500, Train Loss: 0.1226, Val Loss: 0.4863\n",
      "Epoch 231/500, Train Loss: 0.1569, Val Loss: 0.4688\n",
      "Epoch 232/500, Train Loss: 0.1301, Val Loss: 0.4874\n",
      "Epoch 233/500, Train Loss: 0.1060, Val Loss: 0.4867\n",
      "Epoch 234/500, Train Loss: 0.1013, Val Loss: 0.4884\n",
      "Epoch 235/500, Train Loss: 0.1268, Val Loss: 0.4880\n",
      "Epoch 236/500, Train Loss: 0.1193, Val Loss: 0.4865\n",
      "Epoch 237/500, Train Loss: 0.1390, Val Loss: 0.4840\n",
      "Epoch 238/500, Train Loss: 0.1213, Val Loss: 0.4842\n",
      "Epoch 239/500, Train Loss: 0.1547, Val Loss: 0.4869\n",
      "Epoch 240/500, Train Loss: 0.1213, Val Loss: 0.5174\n",
      "Epoch 241/500, Train Loss: 0.1229, Val Loss: 0.5019\n",
      "Epoch 242/500, Train Loss: 0.0994, Val Loss: 0.5462\n",
      "Epoch 243/500, Train Loss: 0.1541, Val Loss: 0.4624\n",
      "Epoch 244/500, Train Loss: 0.1222, Val Loss: 0.5105\n",
      "Epoch 245/500, Train Loss: 0.1405, Val Loss: 0.4863\n",
      "Epoch 246/500, Train Loss: 0.1085, Val Loss: 0.4782\n",
      "Epoch 247/500, Train Loss: 0.1383, Val Loss: 0.4833\n",
      "Epoch 248/500, Train Loss: 0.1341, Val Loss: 0.4830\n",
      "Epoch 249/500, Train Loss: 0.1270, Val Loss: 0.5018\n",
      "Epoch 250/500, Train Loss: 0.1431, Val Loss: 0.4917\n",
      "Epoch 251/500, Train Loss: 0.1259, Val Loss: 0.4933\n",
      "Epoch 252/500, Train Loss: 0.1356, Val Loss: 0.4893\n",
      "Epoch 253/500, Train Loss: 0.1409, Val Loss: 0.5440\n",
      "Epoch 254/500, Train Loss: 0.1064, Val Loss: 0.4735\n",
      "Epoch 255/500, Train Loss: 0.1190, Val Loss: 0.5177\n",
      "Epoch 256/500, Train Loss: 0.1163, Val Loss: 0.4723\n",
      "Epoch 257/500, Train Loss: 0.1130, Val Loss: 0.4630\n",
      "Epoch 258/500, Train Loss: 0.1474, Val Loss: 0.4528\n",
      "Epoch 259/500, Train Loss: 0.1175, Val Loss: 0.4894\n",
      "Epoch 260/500, Train Loss: 0.1182, Val Loss: 0.5179\n",
      "Epoch 261/500, Train Loss: 0.1369, Val Loss: 0.5046\n",
      "Epoch 262/500, Train Loss: 0.1123, Val Loss: 0.4764\n",
      "Epoch 263/500, Train Loss: 0.1276, Val Loss: 0.5234\n",
      "Epoch 264/500, Train Loss: 0.1154, Val Loss: 0.4865\n",
      "Epoch 265/500, Train Loss: 0.0917, Val Loss: 0.4951\n",
      "Epoch 266/500, Train Loss: 0.1964, Val Loss: 0.4715\n",
      "Epoch 267/500, Train Loss: 0.1540, Val Loss: 0.4718\n",
      "Epoch 268/500, Train Loss: 0.1061, Val Loss: 0.4937\n",
      "Epoch 269/500, Train Loss: 0.1084, Val Loss: 0.4747\n",
      "Epoch 270/500, Train Loss: 0.1267, Val Loss: 0.4820\n",
      "Epoch 271/500, Train Loss: 0.1043, Val Loss: 0.4930\n",
      "Epoch 272/500, Train Loss: 0.1032, Val Loss: 0.4903\n",
      "Epoch 273/500, Train Loss: 0.0967, Val Loss: 0.4733\n",
      "Epoch 274/500, Train Loss: 0.1426, Val Loss: 0.4990\n",
      "Epoch 275/500, Train Loss: 0.1071, Val Loss: 0.4795\n",
      "Epoch 276/500, Train Loss: 0.1074, Val Loss: 0.4856\n",
      "Epoch 277/500, Train Loss: 0.1284, Val Loss: 0.4693\n",
      "Epoch 278/500, Train Loss: 0.1208, Val Loss: 0.5286\n",
      "Epoch 279/500, Train Loss: 0.1454, Val Loss: 0.5352\n",
      "Epoch 280/500, Train Loss: 0.1161, Val Loss: 0.5059\n",
      "Epoch 281/500, Train Loss: 0.0942, Val Loss: 0.5105\n",
      "Epoch 282/500, Train Loss: 0.0985, Val Loss: 0.4931\n",
      "Epoch 283/500, Train Loss: 0.1086, Val Loss: 0.4996\n",
      "Epoch 284/500, Train Loss: 0.0952, Val Loss: 0.4696\n",
      "Epoch 285/500, Train Loss: 0.0990, Val Loss: 0.4623\n",
      "Epoch 286/500, Train Loss: 0.1314, Val Loss: 0.4699\n",
      "Epoch 287/500, Train Loss: 0.1205, Val Loss: 0.5003\n",
      "Epoch 288/500, Train Loss: 0.1149, Val Loss: 0.4885\n",
      "Epoch 289/500, Train Loss: 0.0991, Val Loss: 0.4942\n",
      "Epoch 290/500, Train Loss: 0.0940, Val Loss: 0.4778\n",
      "Epoch 291/500, Train Loss: 0.1318, Val Loss: 0.4850\n",
      "Epoch 292/500, Train Loss: 0.1255, Val Loss: 0.4736\n",
      "Epoch 293/500, Train Loss: 0.1205, Val Loss: 0.4726\n",
      "Epoch 294/500, Train Loss: 0.1475, Val Loss: 0.4564\n",
      "Epoch 295/500, Train Loss: 0.1211, Val Loss: 0.5883\n",
      "Epoch 296/500, Train Loss: 0.1821, Val Loss: 0.5007\n",
      "Epoch 297/500, Train Loss: 0.1083, Val Loss: 0.5065\n",
      "Epoch 298/500, Train Loss: 0.0961, Val Loss: 0.4732\n",
      "Epoch 299/500, Train Loss: 0.1041, Val Loss: 0.4832\n",
      "Epoch 300/500, Train Loss: 0.0962, Val Loss: 0.4741\n",
      "Epoch 301/500, Train Loss: 0.1067, Val Loss: 0.4772\n",
      "Epoch 302/500, Train Loss: 0.1112, Val Loss: 0.4692\n",
      "Epoch 303/500, Train Loss: 0.1108, Val Loss: 0.4717\n",
      "Epoch 304/500, Train Loss: 0.1072, Val Loss: 0.4782\n",
      "Epoch 305/500, Train Loss: 0.1136, Val Loss: 0.4640\n",
      "Epoch 306/500, Train Loss: 0.1503, Val Loss: 0.4686\n",
      "Epoch 307/500, Train Loss: 0.1223, Val Loss: 0.4899\n",
      "Epoch 308/500, Train Loss: 0.0944, Val Loss: 0.4856\n",
      "Epoch 309/500, Train Loss: 0.1031, Val Loss: 0.4925\n",
      "Epoch 310/500, Train Loss: 0.0975, Val Loss: 0.4602\n",
      "Epoch 311/500, Train Loss: 0.1459, Val Loss: 0.4811\n",
      "Epoch 312/500, Train Loss: 0.1137, Val Loss: 0.4737\n",
      "Epoch 313/500, Train Loss: 0.1104, Val Loss: 0.4768\n",
      "Epoch 314/500, Train Loss: 0.1123, Val Loss: 0.5084\n",
      "Epoch 315/500, Train Loss: 0.1038, Val Loss: 0.4804\n",
      "Epoch 316/500, Train Loss: 0.1055, Val Loss: 0.4694\n",
      "Epoch 317/500, Train Loss: 0.0907, Val Loss: 0.5375\n",
      "Epoch 318/500, Train Loss: 0.1595, Val Loss: 0.4689\n",
      "Epoch 319/500, Train Loss: 0.0996, Val Loss: 0.5275\n",
      "Epoch 320/500, Train Loss: 0.1235, Val Loss: 0.4769\n",
      "Epoch 321/500, Train Loss: 0.0991, Val Loss: 0.5101\n",
      "Epoch 322/500, Train Loss: 0.1030, Val Loss: 0.4863\n",
      "Epoch 323/500, Train Loss: 0.1193, Val Loss: 0.5011\n",
      "Epoch 324/500, Train Loss: 0.0861, Val Loss: 0.5044\n",
      "Epoch 325/500, Train Loss: 0.0867, Val Loss: 0.4886\n",
      "Epoch 326/500, Train Loss: 0.1032, Val Loss: 0.4970\n",
      "Epoch 327/500, Train Loss: 0.0972, Val Loss: 0.4996\n",
      "Epoch 328/500, Train Loss: 0.0967, Val Loss: 0.4785\n",
      "Epoch 329/500, Train Loss: 0.1142, Val Loss: 0.4941\n",
      "Epoch 330/500, Train Loss: 0.1086, Val Loss: 0.5048\n",
      "Epoch 331/500, Train Loss: 0.0961, Val Loss: 0.4980\n",
      "Epoch 332/500, Train Loss: 0.0919, Val Loss: 0.4959\n",
      "Epoch 333/500, Train Loss: 0.1286, Val Loss: 0.5278\n",
      "Epoch 334/500, Train Loss: 0.1342, Val Loss: 0.4647\n",
      "Epoch 335/500, Train Loss: 0.0958, Val Loss: 0.4801\n",
      "Epoch 336/500, Train Loss: 0.0859, Val Loss: 0.4945\n",
      "Epoch 337/500, Train Loss: 0.1025, Val Loss: 0.5360\n",
      "Epoch 338/500, Train Loss: 0.1536, Val Loss: 0.4953\n",
      "Epoch 339/500, Train Loss: 0.1114, Val Loss: 0.4907\n",
      "Epoch 340/500, Train Loss: 0.0895, Val Loss: 0.4695\n",
      "Epoch 341/500, Train Loss: 0.1100, Val Loss: 0.4828\n",
      "Epoch 342/500, Train Loss: 0.1018, Val Loss: 0.4795\n",
      "Epoch 343/500, Train Loss: 0.0907, Val Loss: 0.4697\n",
      "Epoch 344/500, Train Loss: 0.0899, Val Loss: 0.4926\n",
      "Epoch 345/500, Train Loss: 0.0892, Val Loss: 0.4836\n",
      "Epoch 346/500, Train Loss: 0.1076, Val Loss: 0.4902\n",
      "Epoch 347/500, Train Loss: 0.1167, Val Loss: 0.4742\n",
      "Epoch 348/500, Train Loss: 0.1298, Val Loss: 0.4952\n",
      "Epoch 349/500, Train Loss: 0.0967, Val Loss: 0.4922\n",
      "Epoch 350/500, Train Loss: 0.1146, Val Loss: 0.5048\n",
      "Epoch 351/500, Train Loss: 0.0984, Val Loss: 0.4766\n",
      "Epoch 352/500, Train Loss: 0.1192, Val Loss: 0.5382\n",
      "Epoch 353/500, Train Loss: 0.1964, Val Loss: 0.4788\n",
      "Epoch 354/500, Train Loss: 0.1274, Val Loss: 0.5032\n",
      "Epoch 355/500, Train Loss: 0.1089, Val Loss: 0.4807\n",
      "Epoch 356/500, Train Loss: 0.1013, Val Loss: 0.4856\n",
      "Epoch 357/500, Train Loss: 0.1082, Val Loss: 0.5143\n",
      "Epoch 358/500, Train Loss: 0.0934, Val Loss: 0.5071\n",
      "Epoch 359/500, Train Loss: 0.0952, Val Loss: 0.4818\n",
      "Epoch 360/500, Train Loss: 0.0945, Val Loss: 0.4929\n",
      "Epoch 361/500, Train Loss: 0.0958, Val Loss: 0.4763\n",
      "Epoch 362/500, Train Loss: 0.1306, Val Loss: 0.5001\n",
      "Epoch 363/500, Train Loss: 0.1469, Val Loss: 0.4894\n",
      "Epoch 364/500, Train Loss: 0.1391, Val Loss: 0.4843\n",
      "Epoch 365/500, Train Loss: 0.1099, Val Loss: 0.5762\n",
      "Epoch 366/500, Train Loss: 0.1800, Val Loss: 0.4983\n",
      "Epoch 367/500, Train Loss: 0.1289, Val Loss: 0.4927\n",
      "Epoch 368/500, Train Loss: 0.1038, Val Loss: 0.4736\n",
      "Epoch 369/500, Train Loss: 0.1246, Val Loss: 0.4960\n",
      "Epoch 370/500, Train Loss: 0.0975, Val Loss: 0.5130\n",
      "Epoch 371/500, Train Loss: 0.1079, Val Loss: 0.4771\n",
      "Epoch 372/500, Train Loss: 0.1077, Val Loss: 0.4837\n",
      "Epoch 373/500, Train Loss: 0.1181, Val Loss: 0.4619\n",
      "Epoch 374/500, Train Loss: 0.1156, Val Loss: 0.4626\n",
      "Epoch 375/500, Train Loss: 0.1597, Val Loss: 0.4570\n",
      "Epoch 376/500, Train Loss: 0.1276, Val Loss: 0.5011\n",
      "Epoch 377/500, Train Loss: 0.0957, Val Loss: 0.4924\n",
      "Epoch 378/500, Train Loss: 0.1131, Val Loss: 0.5199\n",
      "Epoch 379/500, Train Loss: 0.0980, Val Loss: 0.5169\n",
      "Epoch 380/500, Train Loss: 0.1227, Val Loss: 0.4707\n",
      "Epoch 381/500, Train Loss: 0.1198, Val Loss: 0.4900\n",
      "Epoch 382/500, Train Loss: 0.1294, Val Loss: 0.4958\n",
      "Epoch 383/500, Train Loss: 0.0965, Val Loss: 0.4816\n",
      "Epoch 384/500, Train Loss: 0.1055, Val Loss: 0.4931\n",
      "Epoch 385/500, Train Loss: 0.1191, Val Loss: 0.5045\n",
      "Epoch 386/500, Train Loss: 0.1022, Val Loss: 0.4737\n",
      "Epoch 387/500, Train Loss: 0.1343, Val Loss: 0.4723\n",
      "Epoch 388/500, Train Loss: 0.1321, Val Loss: 0.5112\n",
      "Epoch 389/500, Train Loss: 0.1218, Val Loss: 0.4778\n",
      "Epoch 390/500, Train Loss: 0.1234, Val Loss: 0.4893\n",
      "Epoch 391/500, Train Loss: 0.1411, Val Loss: 0.4658\n",
      "Epoch 392/500, Train Loss: 0.1296, Val Loss: 0.4624\n",
      "Epoch 393/500, Train Loss: 0.1374, Val Loss: 0.5147\n",
      "Epoch 394/500, Train Loss: 0.1158, Val Loss: 0.4905\n",
      "Epoch 395/500, Train Loss: 0.0949, Val Loss: 0.5128\n",
      "Epoch 396/500, Train Loss: 0.1196, Val Loss: 0.4882\n",
      "Epoch 397/500, Train Loss: 0.1033, Val Loss: 0.5126\n",
      "Epoch 398/500, Train Loss: 0.1144, Val Loss: 0.4817\n",
      "Epoch 399/500, Train Loss: 0.1207, Val Loss: 0.4935\n",
      "Epoch 400/500, Train Loss: 0.0882, Val Loss: 0.5130\n",
      "Epoch 401/500, Train Loss: 0.0952, Val Loss: 0.4694\n",
      "Epoch 402/500, Train Loss: 0.0987, Val Loss: 0.4809\n",
      "Epoch 403/500, Train Loss: 0.0952, Val Loss: 0.4973\n",
      "Epoch 404/500, Train Loss: 0.1118, Val Loss: 0.4851\n",
      "Epoch 405/500, Train Loss: 0.1277, Val Loss: 0.5162\n",
      "Epoch 406/500, Train Loss: 0.1223, Val Loss: 0.4931\n",
      "Epoch 407/500, Train Loss: 0.1192, Val Loss: 0.4725\n",
      "Epoch 408/500, Train Loss: 0.1228, Val Loss: 0.4879\n",
      "Epoch 409/500, Train Loss: 0.1236, Val Loss: 0.4900\n",
      "Epoch 410/500, Train Loss: 0.0896, Val Loss: 0.4773\n",
      "Epoch 411/500, Train Loss: 0.0957, Val Loss: 0.5080\n",
      "Epoch 412/500, Train Loss: 0.1091, Val Loss: 0.4912\n",
      "Epoch 413/500, Train Loss: 0.1004, Val Loss: 0.4905\n",
      "Epoch 414/500, Train Loss: 0.1067, Val Loss: 0.4962\n",
      "Epoch 415/500, Train Loss: 0.0980, Val Loss: 0.4669\n",
      "Epoch 416/500, Train Loss: 0.1229, Val Loss: 0.4810\n",
      "Epoch 417/500, Train Loss: 0.1054, Val Loss: 0.4624\n",
      "Epoch 418/500, Train Loss: 0.1627, Val Loss: 0.4779\n",
      "Epoch 419/500, Train Loss: 0.1312, Val Loss: 0.4984\n",
      "Epoch 420/500, Train Loss: 0.1350, Val Loss: 0.5183\n",
      "Epoch 421/500, Train Loss: 0.1502, Val Loss: 0.5086\n",
      "Epoch 422/500, Train Loss: 0.1220, Val Loss: 0.4632\n",
      "Epoch 423/500, Train Loss: 0.1566, Val Loss: 0.4721\n",
      "Epoch 424/500, Train Loss: 0.1645, Val Loss: 0.4696\n",
      "Epoch 425/500, Train Loss: 0.0954, Val Loss: 0.5116\n",
      "Epoch 426/500, Train Loss: 0.1106, Val Loss: 0.5006\n",
      "Epoch 427/500, Train Loss: 0.1029, Val Loss: 0.4787\n",
      "Epoch 428/500, Train Loss: 0.0874, Val Loss: 0.4958\n",
      "Epoch 429/500, Train Loss: 0.0887, Val Loss: 0.5117\n",
      "Epoch 430/500, Train Loss: 0.0960, Val Loss: 0.4866\n",
      "Epoch 431/500, Train Loss: 0.1013, Val Loss: 0.5185\n",
      "Epoch 432/500, Train Loss: 0.1372, Val Loss: 0.5022\n",
      "Epoch 433/500, Train Loss: 0.0999, Val Loss: 0.4795\n",
      "Epoch 434/500, Train Loss: 0.1252, Val Loss: 0.5282\n",
      "Epoch 435/500, Train Loss: 0.1215, Val Loss: 0.4960\n",
      "Epoch 436/500, Train Loss: 0.0877, Val Loss: 0.4895\n",
      "Epoch 437/500, Train Loss: 0.0873, Val Loss: 0.5083\n",
      "Epoch 438/500, Train Loss: 0.0973, Val Loss: 0.4854\n",
      "Epoch 439/500, Train Loss: 0.0912, Val Loss: 0.5019\n",
      "Epoch 440/500, Train Loss: 0.1095, Val Loss: 0.4803\n",
      "Epoch 441/500, Train Loss: 0.1088, Val Loss: 0.4791\n",
      "Epoch 442/500, Train Loss: 0.1024, Val Loss: 0.5194\n",
      "Epoch 443/500, Train Loss: 0.0959, Val Loss: 0.4808\n",
      "Epoch 444/500, Train Loss: 0.1129, Val Loss: 0.4828\n",
      "Epoch 445/500, Train Loss: 0.0900, Val Loss: 0.4912\n",
      "Epoch 446/500, Train Loss: 0.1033, Val Loss: 0.4738\n",
      "Epoch 447/500, Train Loss: 0.0920, Val Loss: 0.5053\n",
      "Epoch 448/500, Train Loss: 0.0979, Val Loss: 0.4792\n",
      "Epoch 449/500, Train Loss: 0.0909, Val Loss: 0.5148\n",
      "Epoch 450/500, Train Loss: 0.1223, Val Loss: 0.4815\n",
      "Epoch 451/500, Train Loss: 0.0894, Val Loss: 0.4824\n",
      "Epoch 452/500, Train Loss: 0.0879, Val Loss: 0.4746\n",
      "Epoch 453/500, Train Loss: 0.1113, Val Loss: 0.4911\n",
      "Epoch 454/500, Train Loss: 0.0940, Val Loss: 0.5132\n",
      "Epoch 455/500, Train Loss: 0.0972, Val Loss: 0.4845\n",
      "Epoch 456/500, Train Loss: 0.1039, Val Loss: 0.4819\n",
      "Epoch 457/500, Train Loss: 0.0859, Val Loss: 0.5211\n",
      "Epoch 458/500, Train Loss: 0.1080, Val Loss: 0.4964\n",
      "Epoch 459/500, Train Loss: 0.0902, Val Loss: 0.4754\n",
      "Epoch 460/500, Train Loss: 0.0946, Val Loss: 0.5287\n",
      "Epoch 461/500, Train Loss: 0.1256, Val Loss: 0.5140\n",
      "Epoch 462/500, Train Loss: 0.1296, Val Loss: 0.4618\n",
      "Epoch 463/500, Train Loss: 0.0822, Val Loss: 0.4779\n",
      "Epoch 464/500, Train Loss: 0.0823, Val Loss: 0.4806\n",
      "Epoch 465/500, Train Loss: 0.0936, Val Loss: 0.5126\n",
      "Epoch 466/500, Train Loss: 0.1290, Val Loss: 0.4891\n",
      "Epoch 467/500, Train Loss: 0.0983, Val Loss: 0.4762\n",
      "Epoch 468/500, Train Loss: 0.1142, Val Loss: 0.4763\n",
      "Epoch 469/500, Train Loss: 0.0924, Val Loss: 0.5100\n",
      "Epoch 470/500, Train Loss: 0.1013, Val Loss: 0.4726\n",
      "Epoch 471/500, Train Loss: 0.0874, Val Loss: 0.5292\n",
      "Epoch 472/500, Train Loss: 0.1211, Val Loss: 0.5117\n",
      "Epoch 473/500, Train Loss: 0.1176, Val Loss: 0.4627\n",
      "Epoch 474/500, Train Loss: 0.1222, Val Loss: 0.4843\n",
      "Epoch 475/500, Train Loss: 0.0942, Val Loss: 0.4759\n",
      "Epoch 476/500, Train Loss: 0.0933, Val Loss: 0.4929\n",
      "Epoch 477/500, Train Loss: 0.0850, Val Loss: 0.4728\n",
      "Epoch 478/500, Train Loss: 0.0961, Val Loss: 0.4792\n",
      "Epoch 479/500, Train Loss: 0.0853, Val Loss: 0.5002\n",
      "Epoch 480/500, Train Loss: 0.0787, Val Loss: 0.4880\n",
      "Epoch 481/500, Train Loss: 0.1117, Val Loss: 0.5165\n",
      "Epoch 482/500, Train Loss: 0.0863, Val Loss: 0.4772\n",
      "Epoch 483/500, Train Loss: 0.0904, Val Loss: 0.5121\n",
      "Epoch 484/500, Train Loss: 0.1055, Val Loss: 0.4727\n",
      "Epoch 485/500, Train Loss: 0.1391, Val Loss: 0.4613\n",
      "Epoch 486/500, Train Loss: 0.1413, Val Loss: 0.4613\n",
      "Epoch 487/500, Train Loss: 0.0947, Val Loss: 0.4881\n",
      "Epoch 488/500, Train Loss: 0.0903, Val Loss: 0.5098\n",
      "Epoch 489/500, Train Loss: 0.1115, Val Loss: 0.5195\n",
      "Epoch 490/500, Train Loss: 0.0973, Val Loss: 0.4931\n",
      "Epoch 491/500, Train Loss: 0.0816, Val Loss: 0.4746\n",
      "Epoch 492/500, Train Loss: 0.1047, Val Loss: 0.4609\n",
      "Epoch 493/500, Train Loss: 0.1025, Val Loss: 0.5087\n",
      "Epoch 494/500, Train Loss: 0.0925, Val Loss: 0.5051\n",
      "Epoch 495/500, Train Loss: 0.1110, Val Loss: 0.4935\n",
      "Epoch 496/500, Train Loss: 0.0794, Val Loss: 0.4745\n",
      "Epoch 497/500, Train Loss: 0.1105, Val Loss: 0.4750\n",
      "Epoch 498/500, Train Loss: 0.0928, Val Loss: 0.4783\n",
      "Epoch 499/500, Train Loss: 0.0947, Val Loss: 0.4685\n",
      "Epoch 500/500, Train Loss: 0.1118, Val Loss: 0.4873\n",
      "Best Model Loss: 0.43344298005104065\n"
     ]
    }
   ],
   "source": [
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Split the training data into train and validation sets\n",
    "train_features_split, val_features_split, train_labels_split, val_labels_split = train_test_split(\n",
    "    train_features, train_labels, test_size=0.2, random_state=42\n",
    ")\n",
    "\n",
    "# Convert to PyTorch Datasets and DataLoaders\n",
    "train_dataset_split = MultiModalDataset(train_features_split, train_labels_split)\n",
    "val_dataset_split = MultiModalDataset(val_features_split, val_labels_split)\n",
    "\n",
    "train_loader_split = DataLoader(train_dataset_split, batch_size=32, shuffle=True)\n",
    "val_loader_split = DataLoader(val_dataset_split, batch_size=32, shuffle=False)\n",
    "\n",
    "# Define a function to train and evaluate the model\n",
    "def train_and_evaluate_model(hidden_dim, learning_rate, epochs):\n",
    "    model = MultiModalModel(input_dim, hidden_dim, embedding_dim)\n",
    "    optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
    "    loss_fn = rmsre\n",
    "\n",
    "    best_val_loss = float(\"inf\")\n",
    "    best_model = None\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        # Training phase\n",
    "        model.train()\n",
    "        train_loss = 0\n",
    "        for features, labels in train_loader_split:\n",
    "            optimizer.zero_grad()\n",
    "            predictions = model(features).squeeze()\n",
    "            loss = loss_fn(predictions, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            train_loss += loss.item()\n",
    "        \n",
    "        # Validation phase\n",
    "        model.eval()\n",
    "        val_loss = 0\n",
    "        with torch.no_grad():\n",
    "            for features, labels in val_loader_split:\n",
    "                predictions = model(features).squeeze()\n",
    "                loss = loss_fn(predictions, labels)\n",
    "                val_loss += loss.item()\n",
    "\n",
    "        avg_val_loss = val_loss / len(val_loader_split)\n",
    "        if avg_val_loss < best_val_loss:\n",
    "            best_val_loss = avg_val_loss\n",
    "            best_model = model\n",
    "        \n",
    "        print(f\"Epoch {epoch+1}/{epochs}, Train Loss: {train_loss/len(train_loader_split):.4f}, Val Loss: {avg_val_loss:.4f}\")\n",
    "    \n",
    "    return best_model, best_val_loss\n",
    "\n",
    "# Hyperparameter grid\n",
    "hidden_dims = [64, 128, 256]\n",
    "learning_rates = [0.001, 0.0005]\n",
    "epochs = 500\n",
    "\n",
    "# Fine-tune hyperparameters\n",
    "best_model = None\n",
    "best_loss = float(\"inf\")\n",
    "for hidden_dim in hidden_dims:\n",
    "    for lr in learning_rates:\n",
    "        print(f\"Training with hidden_dim={hidden_dim}, lr={lr}\")\n",
    "        model, val_loss = train_and_evaluate_model(hidden_dim, lr, epochs)\n",
    "        if val_loss < best_loss:\n",
    "            best_loss = val_loss\n",
    "            best_model = model\n",
    "\n",
    "print(f\"Best Model Loss: {best_loss}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Kaggle submission file saved as kaggle_submission.csv\n"
     ]
    }
   ],
   "source": [
    "# Generate predictions on the test set\n",
    "best_model.eval()\n",
    "predictions = []\n",
    "\n",
    "with torch.no_grad():\n",
    "    for features in test_loader:\n",
    "        preds = best_model(features).squeeze()\n",
    "        predictions.extend(preds.numpy())\n",
    "\n",
    "# Ensure predictions are positive (e.g., clip negative values to 0)\n",
    "predictions = np.maximum(predictions, 0)\n",
    "\n",
    "# Prepare Kaggle submission file\n",
    "submission = pd.DataFrame({\n",
    "    \"row_id\": np.arange(len(predictions)),  # row_id starts from 0\n",
    "    \"label\": predictions  # Predictions as the label column\n",
    "})\n",
    "\n",
    "# Save to CSV\n",
    "submission_file = \"kaggle_submission.csv\"\n",
    "submission.to_csv(submission_file, index=False)\n",
    "\n",
    "print(f\"Kaggle submission file saved as {submission_file}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Features Shape: (324, 12353)\n",
      "Test Features Shape: (73, 12350)\n"
     ]
    }
   ],
   "source": [
    "print(\"Train Features Shape:\", train_features.shape)\n",
    "print(\"Test Features Shape:\", test_features.shape)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
